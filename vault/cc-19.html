<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content=",,," />





  <link rel="alternate" href="/atom.xml" title="小土刀" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="从这节课开始我们进入了一个新的阶段，开始具体来应用 MapReduce 编程模型，这次主要是计算文本的 N-Gram 及语言模型并连接到 web 服务中。">
<meta name="keywords">
<meta property="og:type" content="website">
<meta property="og:title" content="云计算 第 19 课 用 MapReduce 进行批处理">
<meta property="og:url" content="http://wdxtub.com/vault/cc-19.html">
<meta property="og:site_name" content="小土刀">
<meta property="og:description" content="从这节课开始我们进入了一个新的阶段，开始具体来应用 MapReduce 编程模型，这次主要是计算文本的 N-Gram 及语言模型并连接到 web 服务中。">
<meta property="og:image" content="http://wdxtub.com/images/14597702456209.jpg">
<meta property="og:image" content="http://wdxtub.com/images/14597807939944.jpg">
<meta property="og:image" content="http://wdxtub.com/images/14597892672911.jpg">
<meta property="og:image" content="http://wdxtub.com/images/14597894033284.jpg">
<meta property="og:image" content="http://wdxtub.com/images/14597906061008.jpg">
<meta property="og:image" content="http://wdxtub.com/images/14597911497608.jpg">
<meta property="og:image" content="http://wdxtub.com/images/14598964785155.jpg">
<meta property="og:image" content="http://wdxtub.com/images/14599011423568.jpg">
<meta property="og:image" content="http://wdxtub.com/images/14599117908402.jpg">
<meta property="og:updated_time" content="2016-04-06T05:14:09.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="云计算 第 19 课 用 MapReduce 进行批处理">
<meta name="twitter:description" content="从这节课开始我们进入了一个新的阶段，开始具体来应用 MapReduce 编程模型，这次主要是计算文本的 N-Gram 及语言模型并连接到 web 服务中。">
<meta name="twitter:image" content="http://wdxtub.com/images/14597702456209.jpg">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 4016951,
      author: '博主'
    }
  };
</script>

  <title>
  

  
    云计算 第 19 课 用 MapReduce 进行批处理 | 小土刀
  
</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  





  <!-- hexo-inject:begin --><!-- hexo-inject:end --><script type="text/javascript">
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=59042340";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <div style="display: none;">
    <script src="http://s6.cnzz.com/stat.php?id=1260625611&web_id=1260625611" type="text/javascript"></script>
  </div>





  
  
    
  

  <div class="container one-collumn sidebar-position-left  ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">小土刀</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Agony is my triumph</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-work">
          <a href="/2016/09/11/work-page" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-pencil"></i> <br />
            
            作品
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tech">
          <a href="/2009/09/11/tech-page" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-battery-full"></i> <br />
            
            技术
          </a>
        </li>
      
        
        <li class="menu-item menu-item-life">
          <a href="/1990/09/11/life-page" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-bolt"></i> <br />
            
            生活
          </a>
        </li>
      
        
        <li class="menu-item menu-item-booklist">
          <a href="/1997/09/11/booklist-page" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-diamond"></i> <br />
            
            书单
          </a>
        </li>
      
        
        <li class="menu-item menu-item-thanks">
          <a href="/thanks" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-gift"></i> <br />
            
            关于我
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="#" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    
    
      <p>从这节课开始我们进入了一个新的阶段，开始具体来应用 MapReduce 编程模型，这次主要是计算文本的 N-Gram 及语言模型并连接到 web 服务中。</p>
<a id="more"></a>
<hr>
<h2 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h2><ol>
<li>列举不同的并行和分布式编程模型</li>
<li>解释 MapReduce 编程的执行流程</li>
<li>使用 MapReduce 处理文本数据集</li>
<li>使用 MapReduce 计算 n-gram 已经构造语言模型</li>
<li>直接把 MapReduce 的结果载入到后端存储</li>
<li>搭建前端用来连接后端并显示结果</li>
</ol>
<p>一般来说我们根据运行时的延迟以及执行的频率会把分布式编程模型分为以下三种：</p>
<ul>
<li>批量数据处理系统 Batch Data Processing Systems    <ul>
<li>用于批量处理历史数据</li>
<li>MapReduce</li>
</ul>
</li>
<li>内存中迭代批量数据处理系统 In-Memory Iterative Batch Data Processing Systems<ul>
<li>MapReduce 需要在每次迭代后保存当前计算结果</li>
<li>对于需要多次迭代直到收敛的问题，不够高效</li>
<li>这种处理方式会把数据保存在内存中来解决这个问题</li>
<li>Spark</li>
</ul>
</li>
<li>流/实时处理系统 Streaming or Real-time processing systems<ul>
<li>前两种都是处理历史数据的</li>
<li>这种处理方式则能够实时处理数据</li>
<li>Spark Streaming, Apache Storm, Apache Samza</li>
</ul>
</li>
</ul>
<h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><h3 id="MapReduce-介绍"><a href="#MapReduce-介绍" class="headerlink" title="MapReduce 介绍"></a>MapReduce 介绍</h3><p>更加详细的介绍可以看我的<a href="./2016/03/20/hadoop-guide/">Hadoop 指南</a></p>
<p><a href="http://hadoop.apache.org/" target="_blank" rel="external">Hadoop</a> 是 Google MapReduce 的开源实现。在 MapReduce 程序中，数据以键值对形式存储，然后通过 Mapper 和 Reducer 进行处理：</p>
<p><img src="/images/14597702456209.jpg" alt="MapReduce Overview"></p>
<p><a href="https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/InputFormat.html" target="_blank" rel="external">InputFormat</a> 定义了 Mapper 如何从文件读入数据，并写入为 <a href="https://hadoop.apache.org/docs/current/api/org/apache/hadoop/io/Writable.html" target="_blank" rel="external">Writable</a> 类型</p>
<p>Mapper: <code>Map(k1, v1) --&gt; list(k2, v2)</code></p>
<p>然后会进行 Shuffle 和 Sort（按照 key 的值），接着就到 Reducer，会针对同一个 key 的所有 value 进行处理</p>
<p>Reducer: <code>Reduce(k2, list (v2)) --&gt; list(v3)</code></p>
<p>具体的单词统计的例子可以参考以下资料，这里不赘述</p>
<ul>
<li>代码：<a href="http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Example:_WordCount_v1.0" target="_blank" rel="external">简单的例子</a>；<a href="http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Example:_WordCount_v2.0" target="_blank" rel="external">复杂的例子</a></li>
<li>视频：<a href="https://www.youtube.com/watch?v=3O5e6zGb1dw" target="_blank" rel="external">代码讲解</a>；<a href="https://www.youtube.com/watch?v=iWGqAhViyfY" target="_blank" rel="external">EMR 使用指南</a></li>
</ul>
<p>在这个例子中，打包代码的时候不建议使用 maven 或者 eclipse，因为可能会弄错 Hadoop 包的版本，使用下面的命令（代码文件名为 <code>WordCount.java</code>）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="built_in">cd</span> ~</div><div class="line">mkdir wordcount_classes</div><div class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-common-2.4.0-amzn-3.jar .</div><div class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-mapreduce-client-core-2.4.0-amzn-3.jar .</div><div class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-mapreduce-client-common-2.4.0-amzn-3.jar .</div><div class="line">javac -classpath hadoop-common-2.4.0-amzn-3.jar:hadoop-mapreduce-client-core-2.4.0-amzn-3.jar:hadoop-mapreduce-client-common-2.4.0-amzn-3.jar <span class="_">-d</span> wordcount_classes WordCount.java</div><div class="line">jar -cvf wordcount.jar -C wordcount_classes/ .</div></pre></td></tr></table></figure>
<ul>
<li>然后需要把输入数据放入到 HDFS 中，如 <code>hadoop fs -put /input</code></li>
<li>然后执行 <code>hadoop jar wordcount.jar WordCount /input /output</code> 来进行 MapReduce 工作</li>
<li>查看结果 <code>hadoop fs -ls /output</code></li>
</ul>
<p>上面的部分是用命令行来进行执行，实际我们可以直接在 web 界面操作</p>
<ul>
<li>创建 EMR 的时候选择 Advanced Opitons</li>
<li>在 Steps 中选择 Custom JAR，然后 Configure and Add 具体的 JAR 包以及参数（比如 JAR 在 S3 中的位置）</li>
<li>然后执行即可</li>
</ul>
<h3 id="N-Grams-介绍"><a href="#N-Grams-介绍" class="headerlink" title="N-Grams 介绍"></a>N-Grams 介绍</h3><p>N-Grams 的定义在<a href="https://en.wikipedia.org/wiki/N-gram" target="_blank" rel="external">这里</a>，不过直接看下图也就很清晰了</p>
<p><img src="/images/14597807939944.jpg" alt=""></p>
<p>我们这次的任务只需要计算从 1-gram 到 5-gram（虽然图中也写了 6-gram）</p>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><p>这里我们需要构建一个输入文本预测器，通过 n-gram 及对应的语言模型，预测用户之后可能会输入的内容，具体步骤如下：</p>
<ol>
<li>[40%] 在 Wiki 数据上计算 ngram</li>
<li>[40%] 构造语言模型</li>
<li>[20%] 代码质量</li>
<li>[10%(bonus)] 词语自动完成</li>
</ol>
<p>环境要求</p>
<ul>
<li>打上标签：<code>Project:4.1</code></li>
<li>AWS Elastic MapReducer(EMR)，用 <code>m3</code> 开头的机器</li>
<li>预算 <code>$20</code></li>
<li>MapReduce 的 java 程序需要用 JRE 1.7 编译（因为 Amazon EMR 只支持这个）</li>
</ul>
<h2 id="任务-1-构造-n-gram-模型"><a href="#任务-1-构造-n-gram-模型" class="headerlink" title="任务 1 构造 n-gram 模型"></a>任务 1 构造 n-gram 模型</h2><ul>
<li>数据集 <code>s3://cmucc-datasets/enwiki-20160204-pages</code></li>
<li>n-gram 格式 <code>&lt;phrase&gt;&lt;\t&gt;&lt;count&gt;</code></li>
</ul>
<p>格式的一个例子</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">this        1000</div><div class="line">this is     500</div><div class="line">this is a   250</div></pre></td></tr></table></figure>
<p>算出所有的 n-gram 之后，需要选出出现次数最多的 100 个 n-gram，如果次数一样就按照字母序排列，完成之后保存到文件中，之后会用来评分。这里最好使用 Hive 的 SQL 语法来选择，不过其他任何方法都行。</p>
<p>具体步骤：</p>
<ol>
<li>因为原始数据（总大小 6.2 GB）是 XML 格式，所以需要进行数据清洗<ul>
<li>移除 <code>&lt;ref&gt;</code> 和 <code>&lt;/ref&gt;</code>（如果有具体的属性，也要过滤掉，比如 <code>&lt;ref name=&quot;iaf-ifa.org&quot;/&gt;</code> 整个都要过滤掉 - 考虑用正则撸掉）</li>
<li>移除所有的 URL，也就是以 HTTP/HTTPS/FTP 开头的内容（这里千万要注意）</li>
<li>保留单词中的 <code>&#39;</code> 号，比如 <code>it&#39;s</code> 合法，但是在单词外的，比如 <code>students&#39;</code> 就要过滤掉，但除了 <code>&#39;</code> 之外其他都必须是字母 [A-Za-z]，其他的标点符号（包括下划线 <code>_</code>）和数字都可以截取掉，需要去掉的字符都可以用空格代替，但是不要把换行符弄掉</li>
<li>单词之间不要有连续两个以上的空格</li>
<li>所有的字母都应该是小写字母（<code>toLower</code>）</li>
<li>以行作为计算 n-gram 的最小单位，跨行的都不需要考虑</li>
</ul>
</li>
<li>使用清洗后的数据，在同一个 MapReduce job 中生成 1-gram, 2-gram, 3-gram, 4-gram, 5-gram（尽量使用竞价实例，不能用 EMR streaming）<ul>
<li>不要输出空字符</li>
<li>先在小数据上测试，没有问题才继续做</li>
<li>EMR 每个小时不要超过 <code>$2</code>（使用on-demand 价格）</li>
</ul>
</li>
<li>可以把处理完成的数据保存在到 S3 中，在 MapReduce 程序中可以直接是用 <code>s3cmd</code>进行 S3 写入<ul>
<li>如果要在 S3 和 HDFS 间传输数据，可以使用 <code>hadoop distcp</code> 命令</li>
<li>如果本地存储不够的话，可以把结果拷贝到 <code>/mnt</code> 中（外置存储）</li>
</ul>
</li>
</ol>
<p>输入数据中的一行：</p>
<p><code>&#39;&#39;&#39;Anarchism&#39;&#39;&#39; is a [[political philosophy]] that advocates [[self-governance|self-governed]] societies with voluntary institutions. These are often described as [[stateless society|stateless societies]],&lt;ref&gt;&quot;ANARCHISM, a social philosophy that rejects authoritarian government and maintains that voluntary institutions are best suited to express man&#39;s natural social tendencies.&quot; George Woodcock. &quot;Anarchism&quot; at The Encyclopedia of Philosophy&lt;/ref&gt;&lt;ref name=&quot;iaf-ifa.org&quot;/&gt;&quot;In a society developed on these lines, the voluntary associations which already now begin to cover all the fields of human activity would take a still greater extension so as to substitute themselves for the state in all its functions.&quot; [http://www.theanarchistlibrary.org/HTML/Petr_Kropotkin___Anarchism__from_the_Encyclopaedia_Britannica.html Peter Kropotkin. &quot;Anarchism&quot; from the Encyclopædia Britannica]&lt;/ref&gt;&lt;ref&gt;&quot;Anarchism.&quot; The Shorter Routledge Encyclopedia of Philosophy. 2005. p. 14 &quot;Anarchism is the view that a society without the state, or government, is both possible and desirable.&quot;&lt;/ref&gt; &lt;ref&gt;&quot;anarchists are opposed to irrational (e.g., illegitimate) authority, in other words, hierarchy — hierarchy being the institutionalisation of authority within a society.&quot; [http://www.theanarchistlibrary.org/HTML/The_Anarchist_FAQ_Editorial_Collective__An_Anarchist_FAQ__03_17_.html#toc2 &quot;B.1 Why are anarchists against authority and hierarchy?&quot;] in [[An Anarchist FAQ]]&lt;/ref&gt;</code></p>
<p>清洗之后应该是</p>
<p><code>anarchism is a political philosophy that advocates self governance self governed societies with voluntary institutions these are often described as stateless society stateless societies anarchism a social philosophy that rejects authoritarian government and maintains that voluntary institutions are best suited to express man&#39;s natural social tendencies george woodcock anarchism at the encyclopedia of philosophy in a society developed on these lines the voluntary associations which already now begin to cover all the fields of human activity would take a still greater extension so as to substitute themselves for the state in all its functions peter kropotkin anarchism from the encyclop dia britannica anarchism the shorter routledge encyclopedia of philosophy p anarchism is the view that a society without the state or government is both possible and desirable anarchists are opposed to irrational e g illegitimate authority in other words hierarchy hierarchy being the institutionalisation of authority within a society b why are anarchists against authority and hierarchy in an anarchist faq</code></p>
<p>操作步骤</p>
<ol>
<li>开启一个 EMR 集群，确保 Hive, HBase 和 Hadoop 都要安装，使用 AMI version 3.10.0<ul>
<li>连接 <code>ssh -i ../demo.pem hadoop@ec2-54-86-122-167.compute-1.amazonaws.com</code></li>
<li>安装 tmux <code>sudo yum install tmux</code></li>
<li>复制代码到服务器 <code>scp -i ../demo.pem ./WordCount.java hadoop@ec2-54-86-122-167.compute-1.amazonaws.com:~/ngram/</code></li>
</ul>
</li>
<li>计算完成后，把前 100 个次数最多的 ngram 结果保存在名为 <code>ngrams</code> 的文件中</li>
</ol>
<p>所用命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 创建文件夹</span></div><div class="line">$ <span class="built_in">cd</span> ~; mkdir ngram; <span class="built_in">cd</span> ngram</div><div class="line"><span class="comment"># 拷贝相关 jar 包</span></div><div class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-common-2.4.0-amzn-3.jar .</div><div class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-mapreduce-client-core-2.4.0-amzn-3.jar .</div><div class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-mapreduce-client-common-2.4.0-amzn-3.jar .</div><div class="line"><span class="comment"># 这里需要多拷贝一个文件，不然会有警告（虽然不知道会不会有影响，但是没有警告总是好的）</span></div><div class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-annotations-2.4.0-amzn-3.jar .</div><div class="line"><span class="comment"># 浏览 jar 包</span></div><div class="line">ls /usr/share/aws/emr/hadoop-state-pusher/lib/</div><div class="line"><span class="comment"># 编译</span></div><div class="line">mkdir class</div><div class="line">javac -classpath hadoop-common-2.4.0-amzn-3.jar:hadoop-mapreduce-client-core-2.4.0-amzn-3.jar:hadoop-mapreduce-client-common-2.4.0-amzn-3.jar:hadoop-annotations-2.4.0-amzn-3.jar <span class="_">-d</span> class WordCount.java</div><div class="line"><span class="comment"># 生成 jar 包</span></div><div class="line">jar -cvf wordcount.jar -C ./class .</div><div class="line"></div><div class="line"><span class="comment"># 重新编译系列脚本</span></div><div class="line">rm -r class/*</div><div class="line">rm wordcount.jar</div><div class="line">javac -classpath hadoop-common-2.4.0-amzn-3.jar:hadoop-mapreduce-client-core-2.4.0-amzn-3.jar:hadoop-mapreduce-client-common-2.4.0-amzn-3.jar:hadoop-annotations-2.4.0-amzn-3.jar <span class="_">-d</span> class WordCount.java</div><div class="line">jar -cvf wordcount.jar -C ./class .</div><div class="line"></div><div class="line"><span class="comment"># 拷贝数据到 hdfs</span></div><div class="line"><span class="built_in">cd</span> /mnt</div><div class="line"><span class="comment"># 这里我还是用原来的方法，因为本地有一个备份，也可以直接用 hadoop distcp 命令</span></div><div class="line">hadoop fs -mkdir /ngram</div><div class="line"><span class="comment"># 注意空间可能不够，去 /mnt 比较好</span></div><div class="line">aws s3 cp s3://cmucc-datasets/enwiki-20160204-pages ./</div><div class="line"><span class="comment"># S3 上的大小为 6663676215</span></div><div class="line">wget http://s3.amazonaws.com/cmucc-datasets/enwiki-20160204-pages</div><div class="line">head -n 1000 enwiki-20160204-pages &gt; testset</div><div class="line">hadoop fs -put ./testset /ngramtest</div><div class="line">hadoop fs -put ./enwiki-20160204-pages /ngram</div><div class="line"><span class="comment"># 查看文件</span></div><div class="line">hadoop fs -ls /ngram</div><div class="line"><span class="comment"># 在 jar 包所在的文件夹</span></div><div class="line"><span class="comment"># 测试数据集，注意 output 文件夹不能存在</span></div><div class="line">hadoop jar wordcount.jar WordCount /ngramtest /output</div><div class="line"><span class="comment"># 完整数据集 </span></div><div class="line"><span class="comment"># [1st period 1+4] 21:29-22:00 </span></div><div class="line"><span class="comment"># [2nd period 1+3] 10:46-11:30</span></div><div class="line"><span class="comment"># [3rd period 1+4] 17:23-17:55</span></div><div class="line">hadoop jar wordcount.jar WordCount /ngram /ngramresult</div><div class="line"><span class="comment"># 查看测试结果</span></div><div class="line">hadoop fs -ls /output</div><div class="line">hadoop fs -cat /output/part-r-00000</div><div class="line"><span class="comment"># 查看完整数据集结果</span></div><div class="line">hadoop fs -ls /ngramresult</div><div class="line"><span class="comment"># 复制到本地</span></div><div class="line">hadoop fs -get /output ./</div><div class="line"><span class="comment"># 这个命令我直接空间不够了</span></div><div class="line">hadoop fs -get /ngramresult ./</div><div class="line"><span class="comment"># 直接从 hdfs 导入到 s3</span></div><div class="line">hadoop distcp /ngramresult/ s3://project4dawang/ngram</div></pre></td></tr></table></figure>
<p>用 Hive 进行统计要比自己手动排序方便很多，这里直接上命令，解释在注释中</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 进入 hive shell</span></div><div class="line">hive</div><div class="line"><span class="comment"># 这里之后的命令都是在 hive  shell 中操作，创建一个 EXTERNAL 表</span></div><div class="line"><span class="comment"># 好处是导入数据只需要把 mapreduce 得到的结果复制到 /data/ngram 文件夹</span></div><div class="line">CREATE EXTERNAL TABLE ngram(gram string, num bigint)</div><div class="line">ROW FORMAT DELIMITED</div><div class="line">FIELDS TERMINATED BY <span class="string">'\t'</span></div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION <span class="string">'/data/ngram'</span>;</div><div class="line"><span class="comment"># 测试表</span></div><div class="line">CREATE EXTERNAL TABLE <span class="built_in">test</span>(gram string, num bigint)</div><div class="line">ROW FORMAT DELIMITED</div><div class="line">FIELDS TERMINATED BY <span class="string">'\t'</span></div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION <span class="string">'/data/test'</span>;</div><div class="line"><span class="comment"># 查看所有表</span></div><div class="line">show tables;</div><div class="line"><span class="comment"># 查看表结构</span></div><div class="line">desc ngram;</div><div class="line">desc <span class="built_in">test</span></div><div class="line"><span class="comment"># 删除某个表</span></div><div class="line">drop table ngram;</div><div class="line">drop table <span class="built_in">test</span>;</div><div class="line"></div><div class="line"><span class="comment"># 这里是在 terminal 中执行</span></div><div class="line"><span class="comment"># 复制文件并删掉没用的 _SUCCESS 文件，只要复制过去就算是导入完成了</span></div><div class="line">hadoop fs -mv /ngramresult/part-* /data/ngram</div><div class="line">hadoop fs -rm /data/ngram/_SUCCESS</div><div class="line"><span class="comment"># 复制测试表的文件</span></div><div class="line">hadoop fs -mv /output/part-* /data/<span class="built_in">test</span></div><div class="line"><span class="comment"># 从本地复制过去</span></div><div class="line"><span class="built_in">cd</span> /mnt; mkdir data</div><div class="line">aws s3 cp s3://project4dawang/ngram/ngramresult/ ./data --recursive</div><div class="line"><span class="built_in">cd</span> data</div><div class="line">hadoop fs -put ./ /data/ngram</div><div class="line">hadoop fs -mv /data/ngram/data/* /data/ngram</div><div class="line">hadoop fs -rm -r /data/ngram/data</div><div class="line"></div><div class="line"><span class="comment"># 查看文件</span></div><div class="line">hadoop fs -ls /data/ngram</div><div class="line">hadoop fs -ls /data/<span class="built_in">test</span></div><div class="line"></div><div class="line"><span class="comment"># hive sql 语句</span></div><div class="line"><span class="comment"># ORDER BY 全局排序，只有一个Reduce任务</span></div><div class="line"><span class="comment"># SORT BY 只在本机做排序</span></div><div class="line">select * from ngram order by num desc <span class="built_in">limit</span> 200;</div><div class="line"><span class="comment"># 测试语句 </span></div><div class="line">select * from <span class="built_in">test</span> order by num desc <span class="built_in">limit</span> 100;</div><div class="line"><span class="comment"># 把查询写入到本地文件，这里使用 200 保证不出问题（因为后面还要按字母排序）</span></div><div class="line"><span class="comment"># 大概一次要 25 分钟的样子</span></div><div class="line"><span class="comment"># 这个命令中的输出文件夹必须不存在（因为会递归删除该文件夹所有内容），保险做法直接显示在命令行里复制粘贴</span></div><div class="line">INSERT OVERWRITE LOCAL DIRECTORY <span class="string">'/mnt/ngram'</span> select * from ngram order by num desc <span class="built_in">limit</span> 200;</div><div class="line"><span class="comment"># 测试导出</span></div><div class="line">INSERT OVERWRITE LOCAL DIRECTORY <span class="string">'/mnt/test'</span> select * from <span class="built_in">test</span> order by num desc <span class="built_in">limit</span> 100;</div><div class="line"></div><div class="line"><span class="comment"># 复制回本地进行处理</span></div><div class="line"><span class="comment"># 默认的分隔符是 ^A，需要后期处理一下</span></div><div class="line">scp -i ../demo.pem -r hadoop@ec2-54-86-122-167.compute-1.amazonaws.com:/mnt/ngram/* ./</div></pre></td></tr></table></figure>
<p>把 <code>ngrams</code> 文件和 MapReduce 代码（以及排序的代码）放到一个文件夹中（这里就是命令中的 <code>submit</code>），用下面的命令来提交</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">mkdir ~/submit</div><div class="line"><span class="built_in">cd</span> ~/submit</div><div class="line">wget https://s3.amazonaws.com/15-319<span class="_">-s</span>16/ngram_submitter</div><div class="line">chmod +x ngram_submitter</div><div class="line">cp ~/ngram/WordCount.java ./</div><div class="line">./ngram_submitter</div></pre></td></tr></table></figure>
<p>根据 TPZ 上的提示，我是 <code>cite web</code> 这里出问题了，只好把数据集下载下来，看看到底出了啥问题</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 过滤出带有 cite web 的行</span></div><div class="line">grep <span class="string">'cite'</span> <span class="built_in">test</span> &gt; greprs; grep <span class="string">'web'</span> greprs &gt; result</div><div class="line"><span class="comment"># 大概知道问题所在，是过滤 url 的时候没处理好，找测试用例试验一下</span></div></pre></td></tr></table></figure>
<h2 id="任务-2-构造语言模型"><a href="#任务-2-构造语言模型" class="headerlink" title="任务 2 构造语言模型"></a>任务 2 构造语言模型</h2><p>直接上公式</p>
<p><img src="/images/14597892672911.jpg" alt="Probability of a word appearing after a phrase"></p>
<p>举个例子</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">this                   1000</div><div class="line">this is                 500</div><div class="line">this is a               125</div><div class="line">this is a blue           60</div><div class="line">this is a blue house     20</div></pre></td></tr></table></figure>
<p><img src="/images/14597894033284.jpg" alt="Probabilities to be calculated"></p>
<p>根据上面的要求，我们需要使用 MapReduce job，从 HDFS 读取前一个阶段生成的 ngram，计算所有单词和短语的语言模型，然后直接写入到 HBase 中。</p>
<ul>
<li>需要自己设计 HBase 的 schema，也就是在某个短语后面出现某个单词的概率，也需要考虑界面展示，用户会输入一个短语，然后要显示一个预测下一个次的列表</li>
<li>集群可以直接使用上一步中开启的 EMR（如果没有关掉的话）</li>
<li>作为短语，出现次数必须大于 2 次才进行计算，不然就跳过</li>
<li>对于每个短语，保存 n 个最可能的输入，如果有概率相同的，按字母排序，n 的具体的数值由命令行参数指定，下面有一个排序的例子可以参考</li>
<li>使用 <code>apache.commons.cli</code> 包中的 <code>GenericOptionsParser</code> 类来解析命令行参数</li>
<li>先在小数据上测试，没有问题才继续做</li>
<li>EMR 每个小时不要超过 <code>$2</code>（使用on-demand 价格），也不要超过 5 个实例</li>
</ul>
<p><img src="/images/14597906061008.jpg" alt="Sorting probabilities"></p>
<p>这一部分的 Mapper 和 Reducer 没有第一步这么直观，所以这里简要介绍一下我的思路。</p>
<h3 id="Mapper"><a href="#Mapper" class="headerlink" title="Mapper"></a>Mapper</h3><p>同样用上面的例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">this                   1000</div><div class="line">that is                   1</div><div class="line">this is                 500</div><div class="line">this is a               125</div></pre></td></tr></table></figure>
<p>我们要做的实际上就是把目前的键值对，拆成 phrase 和 word 的形式，但是这里有两个地方要注意：</p>
<ol>
<li>如果 key 拆分之后只有一个单词，需要过滤掉</li>
<li>如果 value 为 1，需要过滤掉</li>
</ol>
<p>然后我们要做的就比较简单了，假设 key 拆分之后有 <code>n</code> 个词，那么把前 <code>n-1</code> 词拼成 phrase，最后一个词作为 word，不过需要注意的是，还要把之前的计数加上去（不然就丢失信息了）</p>
<p>分别过一遍上面的四个例子</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">这里为了方便看，在 \t 两边各加了一个空格，实际上是不需要的 </div><div class="line">this \t 1000 -&gt; 扔掉，key 只有 1 个单词</div><div class="line">that is \t 1 -&gt; 扔掉，value 为 1</div><div class="line">this is \t 500 -&gt; this \t is 500</div><div class="line">this is a \t 125 -&gt; this is \t a 125</div></pre></td></tr></table></figure>
<h3 id="Reducer"><a href="#Reducer" class="headerlink" title="Reducer"></a>Reducer</h3><p>Reducer 这里主要需要进行的工作就是排序，另外因为需要直接写入到 HBase 中，建议使用 <code>TableReducer</code>。具体的步骤如下：</p>
<ol>
<li>遍历一个 key 的 value，计算出出现的总次数</li>
<li>给 value 中的各个不同的 word 进行排序（虽然后面的 PHP 代码也有排序，但是这里需要选取前 n 个，所以还是得排序）</li>
<li>计算概率（需要除以总数），然后写入到记录中，包括单词和具体的概率</li>
<li>具体组织代码的形式要参考下一个任务中 PHP 代码的访问形式（我觉得尽量别改，面得增加复杂度）</li>
</ol>
<h3 id="工作日志"><a href="#工作日志" class="headerlink" title="工作日志"></a>工作日志</h3><p>所用的命令参考 </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># HBase 建表</span></div><div class="line">hbase shell</div><div class="line">&gt; create <span class="string">'wp'</span>,<span class="string">'data'</span></div><div class="line">&gt; list</div><div class="line">&gt; describe <span class="string">'wp'</span></div><div class="line">&gt; <span class="built_in">exit</span></div><div class="line"><span class="comment"># 删除表</span></div><div class="line">&gt; <span class="built_in">disable</span> <span class="string">'wp'</span></div><div class="line">&gt; drop <span class="string">'wp'</span></div><div class="line"><span class="comment"># 查询</span></div><div class="line">&gt; get <span class="string">'wp'</span>, <span class="string">'the'</span></div><div class="line"></div><div class="line"><span class="comment"># 创建文件夹</span></div><div class="line"><span class="built_in">cd</span> ~; mkdir lmodel; <span class="built_in">cd</span> lmodel</div><div class="line"><span class="comment"># 这句在本地执行，复制代码</span></div><div class="line">scp -i ../demo.pem ./LanguageModel.java hadoop@ec2-54-86-122-167.compute-1.amazonaws.com:~/lmodel/</div><div class="line"><span class="comment"># 拷贝相关 jar 包</span></div><div class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-common-2.4.0-amzn-3.jar .</div><div class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-mapreduce-client-core-2.4.0-amzn-3.jar .</div><div class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-mapreduce-client-common-2.4.0-amzn-3.jar .</div><div class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-annotations-2.4.0-amzn-3.jar .</div><div class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/commons-cli-1.2.jar .</div><div class="line">cp /home/hadoop/hbase/hbase-0.94.18.jar .</div><div class="line"><span class="comment"># 浏览 jar 包</span></div><div class="line">ls /usr/share/aws/emr/hadoop-state-pusher/lib/</div><div class="line"><span class="comment"># 编译</span></div><div class="line">mkdir class</div><div class="line">javac -classpath hadoop-common-2.4.0-amzn-3.jar:hadoop-mapreduce-client-core-2.4.0-amzn-3.jar:hadoop-mapreduce-client-common-2.4.0-amzn-3.jar:hadoop-annotations-2.4.0-amzn-3.jar:hbase-0.94.18.jar:commons-cli-1.2.jar <span class="_">-d</span> class LanguageModel.java </div><div class="line"><span class="comment"># 生成 jar 包</span></div><div class="line">jar -cvf languagemodel.jar -C ./class .</div><div class="line"></div><div class="line"><span class="comment"># 重新编译系列脚本</span></div><div class="line">rm -r class/*</div><div class="line">rm languagemodel.jar</div><div class="line">javac -classpath hadoop-common-2.4.0-amzn-3.jar:hadoop-mapreduce-client-core-2.4.0-amzn-3.jar:hadoop-mapreduce-client-common-2.4.0-amzn-3.jar:hadoop-annotations-2.4.0-amzn-3.jar:hbase-0.94.18.jar:commons-cli-1.2.jar <span class="_">-d</span> class LanguageModel.java</div><div class="line">jar -cvf languagemodel.jar -C ./class .</div><div class="line"></div><div class="line"><span class="comment"># 在 jar 包所在的文件夹</span></div><div class="line"><span class="comment"># 测试数据集，注意是根据前面的文件夹来设置参数的</span></div><div class="line"><span class="comment"># 没有 output 文件夹，因为直接写入到 hbase</span></div><div class="line">hadoop jar languagemodel.jar LanguageModel /data/<span class="built_in">test</span></div><div class="line"><span class="comment"># 完整数据集 </span></div><div class="line"><span class="comment"># [1st period 1+4] 19:42-20:00</span></div><div class="line"><span class="comment"># [2nd period 1+4] 20:18-20:40</span></div><div class="line"><span class="comment"># [3rd period 1+4] 20:58-21:22</span></div><div class="line"><span class="comment"># [4th period 1+4] 20:40-21:02</span></div><div class="line"><span class="comment"># [5th period 1+4] 21:12-21:34</span></div><div class="line"><span class="comment"># [6th period 1+4] 23:37-23:59</span></div><div class="line"><span class="comment"># [7th period 1+4] 00:10-00:32</span></div><div class="line"><span class="comment"># [8th period 1+4] 00:36-00:48</span></div><div class="line"><span class="comment"># [9th period 1+4] 00:55-01:07</span></div><div class="line">hadoop jar languagemodel.jar LanguageModel /data/ngram</div></pre></td></tr></table></figure>
<h2 id="任务-3-用-web-展示语言模型"><a href="#任务-3-用-web-展示语言模型" class="headerlink" title="任务 3 用 web 展示语言模型"></a>任务 3 用 web 展示语言模型</h2><p>总体的架构为</p>
<p><img src="/images/14597911497608.jpg" alt="architecture"></p>
<p>SSH 到 master 节点，开启 HBase 的 RESTful API 服务 <code>hbase-daemon.sh start rest</code>，开启/重启 Apache 服务 <code>sudo service httpd restart</code>。</p>
<p>用下面的命令下载样例代码并启动对应服务</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">sudo su</div><div class="line"><span class="built_in">cd</span> /home/hadoop/hbase/bin</div><div class="line">./hbase-daemon.sh start rest</div><div class="line"><span class="built_in">cd</span> /var/www/html</div><div class="line">wget https://s3.amazonaws.com/15-319<span class="_">-s</span>16/proj4_web.tgz</div><div class="line"><span class="comment"># 解压文件，这里包含 样例代码和submitter</span></div><div class="line">sudo tar xzf proj4_web.tgz</div><div class="line"><span class="comment"># 如果遇到权限问题，执行</span></div><div class="line">sudo chmod -R 777 /var/www/html/</div><div class="line">sudo chmod -R 777 /var/www/html/proj4_web</div></pre></td></tr></table></figure>
<p>开启服务之后，可以访问 <code>http://masterdns/proj4_web/info.php</code> 来测试（注意安全组允许所有流量）。如果不能见到正常的页面，重启 apache 服务器并查看 <code>/var/log/httpd/error_log</code> 中的错误日志。</p>
<p><img src="/images/14598964785155.jpg" alt="测试结果"></p>
<p>需要修改的代码是 <code>request.php</code>，输入测试的页面是 <code>http://masterdns/proj4_web/index.html</code></p>
<p>我们把代码下载下来 </p>
<ul>
<li><code>scp -i ../demo.pem hadoop@ec2-54-86-122-167.compute-1.amazonaws.com:/var/www/html/proj4_web/request.php ./</code></li>
<li><code>scp -i ../demo.pem hadoop@ec2-54-86-122-167.compute-1.amazonaws.com:/var/www/html/proj4_web/index.html ./</code></li>
</ul>
<p>需要修改的部分有：</p>
<ul>
<li>表名（和生成语言模型中的一样）</li>
<li>列族名（和生成语言模型中的一样）</li>
<li>EMR Master 的 DNS</li>
</ul>
<p>上传回去 <code>scp -i ../demo.pem ./request.php hadoop@ec2-54-86-122-167.compute-1.amazonaws.com:/var/www/html/proj4_web/index.html</code></p>
<p>我们只需要在 PHP 代码中根据自己设计的 schema 来进行修改对应接口即可（master 的 dns，表名，列名）。默认的设计中，短语是 rowkey，所有可能出现的单词是对应的列（这句话我还是不懂到底 schema 是什么）</p>
<p>完成之后应该可以看到一些推荐结果：</p>
<p><img src="/images/14599011423568.jpg" alt=""></p>
<p>需要把文件复制过来，命令为 </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 在 submitter 所在文件夹</span></div><div class="line">cp /home/hadoop/lmodel/LanguageModel.java .</div><div class="line">cp /home/hadoop/ngram/WordCount.java .</div><div class="line"><span class="comment"># 创建一个文件存命令</span></div><div class="line">vim <span class="built_in">command</span></div><div class="line"><span class="comment"># 提交</span></div><div class="line">./submitter</div></pre></td></tr></table></figure>
<blockquote>
<p>特别提醒</p>
</blockquote>
<p>这次的作业有两个评分组件</p>
<ul>
<li>任务 1 中 ngram 使用 <code>ngram_submitter</code></li>
<li>任务 2 与 3 中使用 <code>submitter</code></li>
</ul>
<p>需要把所有的 MapReduce 代码放到与 <code>submitter</code> 同一个文件夹统一进行提交</p>
<h2 id="额外任务-单词自动完成"><a href="#额外任务-单词自动完成" class="headerlink" title="额外任务 单词自动完成"></a>额外任务 单词自动完成</h2><p>这一部分是额外任务，用户输入单词的一部分，给出最可能的完整单词</p>
<ol>
<li>使用 Wiki 数据集来统计每个单词出现的次数，确保完成了数据清洗工作（和前面一样）</li>
<li>用 HBase 来保存模型（类似前面的语言模型）</li>
<li>输入是一部分的单词，展示的结果是自动完成的建议（给出 5 个单词建议），界面和之前 ngram 的类似</li>
<li>使用 <code>wget https://s3.amazonaws.com/15-319-s16/bonus_submitter</code> 下载，并用 <code>bonus_submitter</code> 来提交（注意各种代码也要一并附上）</li>
</ol>
<p>一个例子，如果输入是 <code>carne</code>，那么建议可能是 <code>carnegie</code>, <code>carney</code>, <code>carnes</code>, <code>carneiro</code> and <code>carnell</code></p>
<p><img src="/images/14599117908402.jpg" alt=""></p>
<p>具体需要完成的有</p>
<ul>
<li>用之前 WordCount.java 的代码生成 1-gram，并保存到结果中</li>
<li>计算模型<ul>
<li>Mapper 部分：假设一个键值对是 <code>abcd \t 1</code>，那么需要变成 <code>a \t abcd 1</code>, <code>ab \t abcd 1</code>, <code>abc \t abcd 1</code>（这里在 <code>\t</code> 两边加了空格，实际不需要）</li>
<li>Reducer 部分：针对每个 key，的所有 value，进行排列，并保存到数据库中</li>
</ul>
</li>
<li>具体怎么测试还没弄清楚，得研究一下 PHP 代码，但感觉应该是和前面的两步无关的</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 这句在本地执行，复制代码</span></div><div class="line">scp -i ../demo.pem ./BonusWordCount.java hadoop@ec2-54-86-122-167.compute-1.amazonaws.com:~/bonus/</div><div class="line">scp -i ../demo.pem ./BonusWordModel.java hadoop@ec2-54-86-122-167.compute-1.amazonaws.com:~/bonus/</div><div class="line"></div><div class="line"><span class="comment"># 编译 bonuswordcount</span></div><div class="line">mkdir class</div><div class="line">javac -classpath hadoop-common-2.4.0-amzn-3.jar:hadoop-mapreduce-client-core-2.4.0-amzn-3.jar:hadoop-mapreduce-client-common-2.4.0-amzn-3.jar:hadoop-annotations-2.4.0-amzn-3.jar <span class="_">-d</span> class BonusWordCount.java</div><div class="line"><span class="comment"># 生成 jar 包</span></div><div class="line">jar -cvf bonuswordcount.jar -C ./class .</div><div class="line"><span class="comment"># 测试数据集</span></div><div class="line">hadoop jar bonuswordcount.jar BonusWordCount /ngramtest /bonusoutput</div><div class="line">hadoop fs -cat /bonusoutput/part-r-00000</div><div class="line"></div><div class="line"><span class="comment"># 完整数据集</span></div><div class="line"><span class="comment"># [1st period 1+4] 10:48-10:54</span></div><div class="line">hadoop jar bonuswordcount.jar BonusWordCount /ngram /bonusresult</div><div class="line"></div><div class="line"><span class="comment"># 编译 bonuswordmodel</span></div><div class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/commons-cli-1.2.jar .</div><div class="line">cp /home/hadoop/hbase/hbase-0.94.18.jar .</div><div class="line">rm -r class/*</div><div class="line">rm bonuswordcount.jar</div><div class="line">rm bonuswordmodel.jar</div><div class="line">javac -classpath hadoop-common-2.4.0-amzn-3.jar:hadoop-mapreduce-client-core-2.4.0-amzn-3.jar:hadoop-mapreduce-client-common-2.4.0-amzn-3.jar:hadoop-annotations-2.4.0-amzn-3.jar:hbase-0.94.18.jar:commons-cli-1.2.jar <span class="_">-d</span> class BonusWordModel.java </div><div class="line">jar -cvf bonuswordmodel.jar -C ./class .</div><div class="line"></div><div class="line"><span class="comment"># 测试数据集</span></div><div class="line">hadoop jar bonuswordmodel.jar BonusWordModel /bonusoutput</div><div class="line"><span class="comment"># 完整数据集</span></div><div class="line"><span class="comment"># [1st period 1+4] 11:00-11:02</span></div><div class="line">hadoop jar bonuswordmodel.jar BonusWordModel /bonusresult</div><div class="line"></div><div class="line"><span class="comment"># 在 submitter 文件夹</span></div><div class="line">wget https://s3.amazonaws.com/15-319<span class="_">-s</span>16/bonus_submitter</div><div class="line">cp /home/hadoop/bonus/BonusWordModel.java .</div><div class="line">cp /home/hadoop/bonus/BonusWordCount.java .</div><div class="line">chmod 777 bonus_submitter</div><div class="line">./bonus_submitter</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html" target="_blank" rel="external">MapReduce Tutorial</a></li>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html" target="_blank" rel="external">HDFS Command Guide</a></li>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html" target="_blank" rel="external">Apache Hadoop tutorial</a></li>
<li><a href="http://hbase.apache.org/book/" target="_blank" rel="external">HBase</a></li>
<li><a href="http://blog.csdn.net/yfkiss/article/details/7776406" target="_blank" rel="external">hive数据导入</a></li>
<li><a href="https://sites.google.com/site/hadoopandhive/home/how-to-output-a-table-to-a-local-file-in-hive" target="_blank" rel="external">How to output a table or result of a query to a local file or HDFS in Hive</a></li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7256833" target="_blank" rel="external">Hadoop Hive sql语法详解</a></li>
<li><a href="http://www.iteblog.com/archives/955" target="_blank" rel="external">Hive几种数据导出方式</a></li>
<li><hadoop the="" definitive="" guide=""> Tom White</hadoop></li>
<li><hbase the="" definitive="" guide=""> Lars George</hbase></li>
</ul>

    
  </div>


          </div>
          

  <p>热评文章</p>
  <div class="ds-top-threads" data-range="weekly" data-num-items="4"></div>


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="vault/cc-19.html"
           data-title="云计算 第 19 课 用 MapReduce 进行批处理" data-url="http://wdxtub.com/vault/cc-19.html">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/misc/avatar.jpg"
               alt="wdxtub" />
          <p class="site-author-name" itemprop="name">wdxtub</p>
          <p class="site-description motion-element" itemprop="description">人文/科学/读书/写作/思考/编程/架构/数据/广交朋友/@SYSU/@CMU</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">710</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">分类</span>
              
            </div>
          

          
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">874</span>
                <span class="site-state-item-name">标签</span>
              
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/wdxtub" target="_blank" title="GitHub">
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/wdxtub" target="_blank" title="微博">
                  
                  微博
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://douban.com/people/wdx" target="_blank" title="豆瓣">
                  
                  豆瓣
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/wdxtub" target="_blank" title="知乎">
                  
                  知乎
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              不妨看看
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://zhchbin.github.io/" title="zhchbin" target="_blank">zhchbin</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.algorithmdog.com/" title="算法狗" target="_blank">算法狗</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.52cs.org/" title="我爱计算机" target="_blank">我爱计算机</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://jackqdyulei.github.io/" title="雷雷" target="_blank">雷雷</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://guojiex.github.io/" title="瓜瓜" target="_blank">瓜瓜</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://wdxtub.lofter.com/" title="我的 Lofter" target="_blank">我的 Lofter</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://wdxtub.com/interview/" title="刷题笔记" target="_blank">刷题笔记</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2013 - 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wdxtub</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

<div class="busuanzi-count">

  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv"><i class="fa fa-user"></i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span></span>
  

  
    <span class="site-pv"><i class="fa fa-eye"></i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span>
  
  
</div>



        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"wdxblog"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementById('footer')
      || document.getElementById('footer')).appendChild(ds);
    })();
  </script>

  
    
      
      <script src="/vendors/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
      <script src="/js/src/hook-duoshuo.js"></script>
    
  





  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
       search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();

    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
    'use strict';
    $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
            // get the contents from search data
            isfetched = true;
            $('.popup').detach().appendTo('.header-inner');
            var datas = $( "entry", xmlResponse ).map(function() {
                return {
                    title: $( "title", this ).text(),
                    content: $("content",this).text(),
                    url: $( "url" , this).text()
                };
            }).get();
            var $input = document.getElementById(search_id);
            var $resultContent = document.getElementById(content_id);
            $input.addEventListener('input', function(){
                var matchcounts = 0;
                var str='<ul class=\"search-result-list\">';                
                var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                $resultContent.innerHTML = "";
                if (this.value.trim().length > 1) {
                // perform local searching
                datas.forEach(function(data) {
                    var isMatch = true;
                    var content_index = [];
                    var data_title = data.title.trim().toLowerCase();
                    var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                    var data_url = data.url;
                    var index_title = -1;
                    var index_content = -1;
                    var first_occur = -1;
                    // only match artiles with not empty titles and contents
                    if(data_title != '' && data_content != '') {
                        keywords.forEach(function(keyword, i) {
                            index_title = data_title.indexOf(keyword);
                            index_content = data_content.indexOf(keyword);
                            if( index_title < 0 && index_content < 0 ){
                                isMatch = false;
                            } else {
                                if (index_content < 0) {
                                    index_content = 0;
                                }
                                if (i == 0) {
                                    first_occur = index_content;
                                }
                            }
                        });
                    }
                    // show search results
                    if (isMatch) {
                        matchcounts += 1;
                        str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                        var content = data.content.trim().replace(/<[^>]+>/g,"");
                        if (first_occur >= 0) {
                            // cut out 100 characters
                            var start = first_occur - 20;
                            var end = first_occur + 80;
                            if(start < 0){
                                start = 0;
                            }
                            if(start == 0){
                                end = 50;
                            }
                            if(end > content.length){
                                end = content.length;
                            }
                            var match_content = content.substring(start, end);
                            // highlight all keywords
                            keywords.forEach(function(keyword){
                                var regS = new RegExp(keyword, "gi");
                                match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                            });
                            
                            str += "<p class=\"search-result\">" + match_content +"...</p>"
                        }
                        str += "</li>";
                    }
                })};
                str += "</ul>";
                if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
                if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
                $resultContent.innerHTML = str;
            });
            proceedsearch();
        }
    });}

    // handle and trigger popup window;
    $('.popup-trigger').mousedown(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };

    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src=""></script>


  

  
<script type="text/javascript" async src="//push.zhanzhang.baidu.com/push.js">
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


</body>
</html>
