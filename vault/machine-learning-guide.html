<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content=",,," />





  <link rel="alternate" href="/atom.xml" title="小土刀" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="本文是我学习机器学习时候的一些笔记和总结，因为时间比较长了，参考链接都遗失了，在这里感谢各位老司机在我学习过程中给我的帮助。">
<meta name="keywords">
<meta property="og:type" content="website">
<meta property="og:title" content="机器学习指南">
<meta property="og:url" content="http://wdxtub.com/vault/machine-learning-guide.html">
<meta property="og:site_name" content="小土刀">
<meta property="og:description" content="本文是我学习机器学习时候的一些笔记和总结，因为时间比较长了，参考链接都遗失了，在这里感谢各位老司机在我学习过程中给我的帮助。">
<meta property="og:image" content="http://wdxtub.com/images/14734728090031.gif">
<meta property="og:updated_time" content="2016-09-10T15:53:35.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习指南">
<meta name="twitter:description" content="本文是我学习机器学习时候的一些笔记和总结，因为时间比较长了，参考链接都遗失了，在这里感谢各位老司机在我学习过程中给我的帮助。">
<meta name="twitter:image" content="http://wdxtub.com/images/14734728090031.gif">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 4016951,
      author: '博主'
    }
  };
</script>

  <title>
  

  
    机器学习指南 | 小土刀
  
</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  





  <!-- hexo-inject:begin --><!-- hexo-inject:end --><script type="text/javascript">
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=59042340";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <div style="display: none;">
    <script src="http://s6.cnzz.com/stat.php?id=1260625611&web_id=1260625611" type="text/javascript"></script>
  </div>





  
  
    
  

  <div class="container one-collumn sidebar-position-left  ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">小土刀</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Agony is my triumph</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-work">
          <a href="/2016/09/11/work-page" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-pencil"></i> <br />
            
            作品
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tech">
          <a href="/2009/09/11/tech-page" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-battery-full"></i> <br />
            
            技术
          </a>
        </li>
      
        
        <li class="menu-item menu-item-life">
          <a href="/1990/09/11/life-page" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-bolt"></i> <br />
            
            生活
          </a>
        </li>
      
        
        <li class="menu-item menu-item-booklist">
          <a href="/1997/09/11/booklist-page" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-diamond"></i> <br />
            
            书单
          </a>
        </li>
      
        
        <li class="menu-item menu-item-thanks">
          <a href="/thanks" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-gift"></i> <br />
            
            关于我
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="#" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    
    
      <p>本文是我学习机器学习时候的一些笔记和总结，因为时间比较长了，参考链接都遗失了，在这里感谢各位老司机在我学习过程中给我的帮助。</p>
<a id="more"></a>
<hr>
<h2 id="数学基础知识"><a href="#数学基础知识" class="headerlink" title="数学基础知识"></a>数学基础知识</h2><h3 id="线性代数-Linear-Algebra-："><a href="#线性代数-Linear-Algebra-：" class="headerlink" title="线性代数 (Linear Algebra)："></a>线性代数 (Linear Algebra)：</h3><p>这门学科对于Learning是必备的基础，对它的透彻掌握是必不可少的。</p>
<p><strong>Introduction to Linear Algebra (3rd Ed.)  by Gilbert Strang</strong></p>
<p>这本书是MIT的线性代数课使用的教材，也是被很多其它大学选用的经典教材。它的难度适中，讲解清晰，重要的是对许多核心的概念讨论得比较透彻。我个人觉得，学习线性代数，最重要的不是去熟练矩阵运算和解方程的方法——这些在实际工作中MATLAB可以代劳，关键的是要深入理解几个基础而又重要的概念：子空间(Subspace)，正交(Orthogonality)，特征值和特征向量(Eigenvalues and eigenvectors)，和线性变换(Linear transform)。从我的角度看来，一本线代教科书的质量，就在于它能否给这些根本概念以足够的重视，能否把它们的联系讲清楚。Strang的这本书在这方面是做得很好的。</p>
<p>而且，这本书有个得天独厚的优势。书的作者长期在MIT讲授线性代数课(18.06)，<a href="http://ocw.mit.edu/OcwWeb/Mathematics/18-06Spring-2005/CourseHome/index.htm" target="_blank" rel="external">课程的video</a>在MIT的Open courseware网站上有提供。有时间的朋友可以一边看着名师授课的录像，一边对照课本学习或者复习。</p>
<h3 id="概率和统计-Probability-and-Statistics"><a href="#概率和统计-Probability-and-Statistics" class="headerlink" title="概率和统计 (Probability and Statistics)"></a>概率和统计 (Probability and Statistics)</h3><p>Linear Algebra (线性代数) 和 Statistics (统计学) 是最重要和不可缺少的。这代表了Machine Learning中最主流的两大类方法的基础。一种是以研究函数和变换为重点的代数方法，比如Dimension reduction，feature extraction，Kernel等，一种是以研究统计模型和样本分布为重点的统计方法，比如Graphical model, Information theoretical models等。它们侧重虽有不同，但是常常是共同使用的，对于代数方法，往往需要统计上的解释，对于统计模型，其具体计算则需要代数的帮助。以代数和统计为出发点，继续往深处走，我们会发现需要更多的数学。</p>
<p><strong>Applied Multivariate Statistical Analysis_(5th Ed.)  by Richard A. Johnson and Dean W. Wichern</strong></p>
<p>这本书是我在刚接触向量统计的时候用于学习的，我在香港时做研究的基础就是从此打下了。实验室的一些同学也借用这本书学习向量统计。这本书没有特别追求数学上的深度，而是以通俗易懂的方式讲述主要的基本概念，读起来很舒服，内容也很实用。对于Linear regression, factor analysis, principal component analysis (PCA), and canonical component analysis (CCA)这些Learning中的基本方法也展开了初步的论述。</p>
<p>之后就可以进一步深入学习贝叶斯统计和Graphical models。一本理想的书是</p>
<p><strong>Introduction to Graphical Models (draft version).  by M. Jordan and C.Bishop.</strong></p>
<p>这本书从基本的贝叶斯统计模型出发一直深入到复杂的统计网络的估计和推断，深入浅出，statistical learning的许多重要方面都在此书有清楚论述和详细讲解。</p>
<p>Graph Theory（图论)，图，由于它在表述各种关系的强大能力以及优雅的理论，高效的算法，越来越受到Learning领域的欢迎。经典图论，在Learning中的一个最重要应用就是graphical models了，它被成功运用于分析统计网络的结构和规划统计推断的流程。Graphical model所取得的成功，图论可谓功不可没。在Vision里面，maxflow(graphcut)算法在图像分割，Stereo还有各种能量优化中也广受应用。另外一个重要的图论分支就是Algebraic graph theory (代数图论)，主要运用于图的谱分析，著名的应用包括Normalized Cut和Spectral Clustering。近年来在semi-supervised learning中受到特别关注。</p>
<h3 id="分析-Analysis"><a href="#分析-Analysis" class="headerlink" title="分析 (Analysis)"></a>分析 (Analysis)</h3><p>微积分或者数学分析是很多学科的基础，值得推荐的教科书莫过于</p>
<p><strong>Principles of Mathematical Analysis, by Walter Rudin</strong></p>
<p>有点老，但是绝对经典，深入透彻。缺点就是比较艰深——这是Rudin的书的一贯风格，适合于有一定基础后回头去看。</p>
<p>Learning研究的大部分问题是在连续的度量空间进行的，无论代数还是统计，在研究优化问题的时候，对一个映射的微分或者梯度的分析总是不可避免。而在统计学中，Marginalization和积分更是密不可分——不过，以解析形式把积分导出来的情况则不多见。</p>
<p>在分析这个方向，接下来就是泛函分析(Functional Analysis)。</p>
<p><strong>Introductory Functional Analysis with Applications, by Erwin Kreyszig.</strong></p>
<p>适合作为泛函的基础教材，容易切入而不失全面。在分析这个方向，还有一个重要的学科是测度理论(Measure theory)，但是我看过的书里面目前还没有感觉有特别值得介绍的。</p>
<p>Measure Theory(测度理论)，这是和实分析关系非常密切的学科。但是测度理论并不限于此。从某种意义上说，Real Analysis可以从Lebesgue Measure（勒贝格测度）推演，不过其实还有很多别的测度体系——概率本身就是一种测度。测度理论对于Learning的意义是根本的，现代统计学整个就是建立在测度理论的基础之上——虽然初级的概率论教科书一般不这样引入。在看一些统计方面的文章的时候，你可能会发现，它们会把统计的公式改用测度来表达，这样做有两个好处：所有的推导和结论不用分别给连续分布和离散分布各自写一遍了，这两种东西都可以用同一的测度形式表达：连续分布的积分基于Lebesgue测度，离散分布的求和基于计数测度，而且还能推广到那种既不连续又不离散的分布中去（这种东西不是数学家的游戏，而是已经在实用的东西，在Dirchlet Process或者Pitman-Yor Process里面会经常看到)。而且，即使是连续积分，如果不是在欧氏空间进行，而是在更一般的拓扑空间（比如微分流形或者变换群），那么传统的黎曼积分（就是大学一年级在微积分课学的那种）就不work了，你可能需要它们的一些推广，比如Haar Measure或者 Lebesgue-Stieltjes积分。</p>
<h3 id="拓扑-Topology"><a href="#拓扑-Topology" class="headerlink" title="拓扑 (Topology)"></a>拓扑 (Topology)</h3><p>Topology（拓扑学)，这是学术中很基础的学科。它一般不直接提供方法，但是它的很多概念和定理是其它数学分支的基石。看很多别的数学的时候，你会经常接触这样一些概念：Open set / Closed set，set basis，Hausdauf, continuous function，metric space, Cauchy sequence, neighborhood, compactness, connectivity。</p>
<p><strong>Topology (2nd Ed.)  by James Munkres</strong></p>
<p>这本书是Munkres教授长期执教MIT拓扑课的心血所凝。对于一般拓扑学(General topology)有全面介绍，而对于代数拓扑(Algebraic topology)也有适度的探讨。此书不需要特别的数学知识就可以开始学习，由浅入深，从最基本的集合论概念（很多书不屑讲这个）到 Nagata-Smirnov Theorem和Tychonoff theorem等较深的定理（很多书避开了这个）都覆盖了。讲述方式思想性很强，对于很多定理，除了给出证明过程和引导你思考其背后的原理脉络，很多令人赞叹的亮点——我常读得忘却饥饿，不愿释手。很多习题很有水平。</p>
<p>当然，我们研究learning也许不需要深究这些数学概念背后的公理体系，但是，打破原来定义的概念的局限在很多问题上是必须的——尤其是当你研究的东西它不是在欧氏空间里面的时候——正交矩阵，变换群，流形，概率分布的空间，都属于此。</p>
<h3 id="流形理论-Manifold-theory"><a href="#流形理论-Manifold-theory" class="headerlink" title="流形理论 (Manifold theory)"></a>流形理论 (Manifold theory)</h3><p>对于拓扑和分析有一定把握时，方可开始学习流形理论，否则所学只能流于浮浅。我所使用的书是</p>
<p><strong>Introduction to Smooth Manifolds.  by John M. Lee</strong></p>
<p>虽然书名有introduction这个单词，但是实际上此书涉入很深，除了讲授了基本的manifold, tangent space, bundle, sub-manifold等，还探讨了诸如纲理论(Category theory)，德拉姆上同调(De Rham cohomology)和积分流形等一些比较高级的专题。对于李群和李代数也有相当多的讨论。行文通俗而又不失严谨，不过对某些记号方式需要熟悉一下。</p>
<p>Differential Manifold (微分流形)，通俗地说它研究的是平滑的曲面。一个直接的印象是它是不是可以用来fitting一个surface什么的——当然这算是一种应用，但是这是非常初步的。本质上说，微分流形研究的是平滑的拓扑结构。一个空间构成微分流形的基本要素是局部平滑：从拓扑学来理解，就是它的任意局部都同胚于欧氏空间，从解析的角度来看，就是相容的局部坐标系统。当然，在全局上，它不要求和欧氏空间同胚。它除了可以用于刻画集合上的平滑曲面外，更重要的意义在于，它可以用于研究很多重要的集合。一个n-维线性空间的全部k-维子空间</p>
<p>虽然李群论是建基于平滑流形的概念之上，不过，也可能从矩阵出发直接学习李群和李代数——这种方法对于急需使用李群论解决问题的朋友可能更加实用。而且，对于一个问题从不同角度看待也利于加深理解。下面一本书就是这个方向的典范：</p>
<p><strong>Lie Groups, Lie Algebras, and Representations: An Elementary Introduction.  by Brian C. Hall</strong></p>
<p>此书从开始即从矩阵切入，从代数而非几何角度引入矩阵李群的概念。并通过定义运算的方式建立 exponential mapping，并就此引入李代数。这种方式比起传统的通过“左不变向量场(Left-invariant vector field)“的方式定义李代数更容易为人所接受，也更容易揭示李代数的意义。最后，也有专门的论述把这种新的定义方式和传统方式联系起来。</p>
<p>群论在 Learning 中用得较多的是它的一个重要方向 Lie group。定义在平滑流形上的群，并且其群运算是平滑的话，那么这就叫李群。因为 Learning 和编码不同，更多关注的是连续空间，因为 Lie group 在各种群中对于 Learning 特别重要。各种子空间，线性变换，非奇异矩阵都基于通常意义的矩阵乘法构成李群。在李群中的映射，变换，度量，划分等等都对于 Learning 中代数方法的研究有重要指导意义。</p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>主要是基本概念的辨析，都是最最基础和常规的</p>
<h3 id="监督学习与非监督学习"><a href="#监督学习与非监督学习" class="headerlink" title="监督学习与非监督学习"></a>监督学习与非监督学习</h3><ul>
<li>应用领域：企业数据</li>
</ul>
<p>监督学习需要标注数据(KNN, NB, SVM, DT, BP, RF, GBRT)，这类算法必须知道预测什么，即目标变量的分类信息。对具有标记的训练样本进行学习，以尽可能对训练样本集外的数据进行分类预测。</p>
<p>非监督学习(KMEANS, DL)数据没有类别信息，也不会给定目标值，对未标记的样本进行训练学习，比发现这些样本中的结构知识。将数据集合分成由类似的对象组成的多个类的过程被称为<strong>聚类</strong></p>
<h3 id="半监督式学习"><a href="#半监督式学习" class="headerlink" title="半监督式学习"></a>半监督式学习</h3><ul>
<li>应用领域：图像识别（存在大量非标识数据）</li>
</ul>
<p>在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如图论推理算法(Graph Inference)或者拉普拉斯支持向量机(Laplacian SVM)等。</p>
<h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><ul>
<li>应用领域：机器人控制、系统控制</li>
</ul>
<p>在这种学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻作出调整。常见的应用场景包括动态系统以及机器人控制等。常见算法包括 Q-Learning 以及时间差学习(Temporal difference learning)。</p>
<h3 id="离散数据与连续数据"><a href="#离散数据与连续数据" class="headerlink" title="离散数据与连续数据"></a>离散数据与连续数据</h3><p>离散数据（标称型）的目标变量结果只在有限目标集中取值，比方说真与假，一般用于<strong>分类</strong></p>
<p>连续数据（数值型）目标变量主要用于<strong>回归</strong>分析，通过给定数据点的最优拟合曲线</p>
<h3 id="生成方法"><a href="#生成方法" class="headerlink" title="生成方法"></a>生成方法</h3><ul>
<li>例子：NB</li>
</ul>
<p>生成方法：由数据学习联合概率密度分布 <code>P(X,Y)</code>，然后求出条件概率分布 <code>P(Y|X)</code> 作为预测的模型，即生成模型：<code>P(Y|X)= P(X,Y) / P(X)</code>。基本思想是首先建立样本的联合概率概率密度模型 <code>P(X,Y)</code>，然后再得到后验概率 <code>P(Y|X)</code>，再利用它进行分类，就像上面说的那样。</p>
<p>生成方法学习联合概率密度分布 <code>P(X,Y)</code>，所以就可以从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度。但它不关心到底划分各类的那个分类边界在哪。生成方法可以还原出联合概率分布 <code>P(Y|X)</code>，而判别方法不能。生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快的收敛于真实模型，当存在隐变量时，仍可以用生成方法学习。此时判别方法就不能用。</p>
<p><strong>由生成模型可以得到判别模型，但由判别模型得不到生成模型。</strong></p>
<h3 id="判别方法"><a href="#判别方法" class="headerlink" title="判别方法"></a>判别方法</h3><ul>
<li>例子：k 近邻，决策树</li>
</ul>
<p>由数据直接学习决策函数 <code>Y=f(X)</code> 或者条件概率分布 <code>P(Y|X)</code> 作为预测的模型，即判别模型。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。典型的判别模型包括k近邻，感知级，决策树，支持向量机等。</p>
<p>判别方法直接学习的是决策函数 <code>Y=f(X)</code> 或者条件概率分布 <code>P(Y|X)</code>。不能反映训练数据本身的特性。但它寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。直接面对预测，往往学习的准确率更高。由于直接学习 <code>P(Y|X)</code> 或 <code>P(X)</code>，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。</p>
<h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><p>如果一味的去提高训练数据的预测能力，所选模型的复杂度往往会很高，这种现象称为过拟合。所表现的就是模型训练时候的误差很小，但在测试的时候误差很大。</p>
<p>产生原因</p>
<ul>
<li>因为参数太多，导致模型复杂度上升，容易过拟合</li>
<li>权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征</li>
<li>解决方法：交叉验证法、减少特征、正则化、权值衰减、验证数据</li>
</ul>
<p><strong>泛化能力是指模型对未知数据的预测能力</strong></p>
<h3 id="线性分类器与非线性分类器"><a href="#线性分类器与非线性分类器" class="headerlink" title="线性分类器与非线性分类器"></a>线性分类器与非线性分类器</h3><ul>
<li>如果模型是参数的线性函数，并且存在线性分类面，那么就是线性分类器，否则不是</li>
<li>常见的线性分类器有：LR, 贝叶斯分类，单层感知机，线性回归</li>
<li>常见的非线性分类器：决策树、RF、GBDT、多层感知机</li>
<li>SVM两种都有(看线性核还是高斯核)</li>
<li>线性分类器速度快、编程方便，但是可能拟合效果不会很好</li>
<li>非线性分类器编程复杂，但是效果拟合能力强</li>
</ul>
<blockquote>
<p>特征比数据量还大时，选择什么样的分类器？</p>
</blockquote>
<p>线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分</p>
<blockquote>
<p>对于维度很高的特征，你是选择线性还是非线性分类器？</p>
</blockquote>
<p>线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分</p>
<blockquote>
<p>对于维度极低的特征，你是选择线性还是非线性分类器？</p>
</blockquote>
<p>非线性分类器，因为低维空间可能很多特征都跑到一起了，导致线性不可分</p>
<h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3><p>极大似然估计，只是一种概率论在统计学中的应用，它是参数评估的方法之一。说的 已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计通过若干次实验，观察其结果，利用结果推出参数的大概值。极大似然估计是建立在这样的思想上的：已知某个参数能使这个样本出现的概率最大。我们当然不会再去选择其他其他小概率的样本，所以干脆就把这个参数作为估计的真实值。</p>
<h3 id="后验概率"><a href="#后验概率" class="headerlink" title="后验概率"></a>后验概率</h3><p>后验概率是信息论的基本概念之一。在一个通信系统中，在收到某个消息之后，接收端所了解到的该消息发送的概率称为后验证概率。后验概率是指在得到”结果“的信息后重新修正的概率，如贝叶斯公式中的。是执果寻因的问题。后验概率和先验概率有着不可分割的联系，后验的计算要以先验概率为基础，其实说白了后验概率其实就是条件概率。</p>
<h2 id="朴素贝叶斯-NB"><a href="#朴素贝叶斯-NB" class="headerlink" title="朴素贝叶斯(NB)"></a>朴素贝叶斯(NB)</h2><ul>
<li>优点<ul>
<li>对小规模的数据表现很好</li>
<li>适合多分类任务</li>
<li>适合增量式训练</li>
</ul>
</li>
<li>缺点<ul>
<li>对输入数据的表达形式很敏感</li>
</ul>
</li>
<li>适用数据范围<ul>
<li>标称型</li>
</ul>
</li>
<li>算法类型<ul>
<li>分类算法</li>
</ul>
</li>
</ul>
<p>朴素贝叶斯是贝叶斯理论的一部分，贝叶斯决策理论的核心思想，即选择具有高概率的决策。朴素贝叶斯之所以冠以朴素开头，是因为其在贝叶斯理论的基础上做出了两点假设：</p>
<ol>
<li>每个特征之间相互独立。</li>
<li>每个特征同等重要。</li>
</ol>
<p>贝叶斯准则是构建在条件概率的基础之上的，其公式如下：</p>
<p>$$P(H|X)=\frac{P(X|H)}{P(X)}$$</p>
<p><code>P(H|X）</code>是根据 <code>X</code> 参数值判断其属于类别 <code>H</code> 的概率，称为后验概率。<code>P(H)</code> 是直接判断某个样本属于 <code>H</code> 的概率，称为先验概率。<code>P(X|H)</code> 是在类别 <code>H</code> 中观测到 <code>X</code> 的概率（后验概率），<code>P(X)</code> 是在数据库中观测到X的概率。可见贝叶斯准则是基于条件概率并且和观测到样本的先验概率和后验概率是分不开的。</p>
<p>总结：对于分类而言，使用概率有事要比使用硬规则更为有效。贝叶斯概率及贝叶斯准则提供了一种利用已知值来估计未知概率的有效方法。可以通过特征之间的条件独立性假设，降低对数据量的需求。尽管条件独立性的假设并不正确，但是朴素贝叶斯仍然是一种有效的分类器。</p>
<p>一些要注意的地方：</p>
<ul>
<li>给出的特征向量长度可能不同，所以需要归一化为统一长度的向量。比如说文本分类，如果特征是句子单词的话，则长度为整个词汇量的长度，对应位置是该单词出现的次数</li>
<li>利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率。如果其中一个概率值为 0，那么最后乘积也为 0。为了降低这种影响，可以将所有词出现数字初始化为 1，并将分母初始化为 2。拉普拉斯平滑法将每个 k 值出现次数事先都加 1，通俗讲就是假设他们都出现过一次。</li>
<li>另一个遇到的问题是下溢出，这是由于太多很小的数相乘造成的，这里取对数，就可以把乘法变为加法，并且对最后结果没有影响。</li>
<li>遇到特征之间不独立问题，参考改进的贝叶斯网络，使用 DAG 来进行概率图的描述</li>
</ul>
<h2 id="线性回归-Linear-Regression"><a href="#线性回归-Linear-Regression" class="headerlink" title="线性回归(Linear Regression)"></a>线性回归(Linear Regression)</h2><ul>
<li>优点<ul>
<li>结果易于理解</li>
<li>计算上不复杂。</li>
</ul>
</li>
<li>缺点<ul>
<li>对非线性数据拟合不好。</li>
</ul>
</li>
<li>适用数据类型<ul>
<li>数值型</li>
<li>标称型</li>
</ul>
</li>
<li>算法类型<ul>
<li>回归算法</li>
</ul>
</li>
</ul>
<p>在统计学中，线性回归(Linear Regression)是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合（自变量都是一次方）。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。</p>
<p>线性方程的模型函数的向量表示形式为： $h_\theta(x)=\theta^TX$</p>
<p>通过训练数据集寻找向量系数的最优解，即为求解模型参数。其中求解模型系数的优化器方法可以用“最小二乘法”、“梯度下降”算法，来求解损失函数的最优解：</p>
 $$J(\theta)=\frac{1}{2}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)})$$  
 $$min_\theta \; J_\theta$$ 
<h3 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h3><p>将训练特征表示为 <code>X</code> 矩阵，结果表示成 <code>y</code> 向量，仍然是线性回归模型，误差函数不变。那么 <code>θ</code> 可以直接由下面公式得出</p>
<p>$$\theta=(X^TX)^{-1}X^Ty$$</p>
<p>这里 <code>y</code> 是向量，此方法要求 <code>X</code> 是列满秩的，而且求矩阵的逆比较慢。</p>
<p>而在 LWLR（局部加权线性回归）中，参数的计算表达式为:</p>
<p>$$\theta=(X^TX)^{-1}X^TWy$$</p>
<p>因为此时优化的是  $\sum_i w^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2$ </p>
<p>由此可见 LWLR 与 LR 不同，LWLR 是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。</p>
<h3 id="岭回归（ridge-regression）"><a href="#岭回归（ridge-regression）" class="headerlink" title="岭回归（ridge regression）"></a>岭回归（ridge regression）</h3><p>岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价，获得回归系数更为符合实际、更可靠的回归方法，对病态数据的耐受性远远强于最小二乘法。</p>
<p>岭回归分析法是从根本上消除复共线性影响的统计方法。岭回归模型通过在相关矩阵中引入一个很小的岭参数 <code>K</code>（1&gt;K&gt;0），并将它加到主对角线元素上，从而降低参数的最小二乘估计中复共线特征向量的影响，减小复共线变量系数最小二乘估计的方法，以保证参数估计更接近真实情况。岭回归分析将所有的变量引入模型中，比逐步回归分析提供更多的信息。</p>
<p>总结：与分类一样，回归也是预测目标值的过程。回归与分类的不同点在于，前者预测连续型的变量，而后者预测离散型的变量。回归是统计学中最有力的工具之一。在回归方程里，求得特征对应的最佳回归系统的方法是最小化误差的平方和。</p>
<h2 id="k-近邻算法-kNN"><a href="#k-近邻算法-kNN" class="headerlink" title="k-近邻算法(kNN)"></a>k-近邻算法(kNN)</h2><ul>
<li>优点<ul>
<li>精度高</li>
<li>对异常值不敏感</li>
<li>无数据输入假定</li>
</ul>
</li>
<li>缺点<ul>
<li>计算复杂度高</li>
<li>空间复杂度高</li>
</ul>
</li>
<li>适用数据范围<ul>
<li>数值型</li>
<li>标称型</li>
</ul>
</li>
<li>算法类型<ul>
<li>分类算法。</li>
</ul>
</li>
</ul>
<p>存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中对应的特征进行比较，然后算法提取样本集中前 k 个最相似的数据。最后，选择 k 个最相似数据中出现次数最多的分类，作为新数据的分类。</p>
<p>为了避免不同的特征的数值不同所导致的影响不同，可能需要进行归一化，也就是把特征值转换成 [0,1] 值</p>
<p>k-近邻算法是分类数据最简单最有效的算法，必须保存全部数据集，如果训练数据集很大，必须使用大量的存储空间。此外，由于必须对数据集中的每个数据计算距离值，实际使用可能非常耗时。<strong>k决策树</strong>是其优化版本，可以节省大量的计算开销。</p>
<p>另一个却显示它无法给出任何数据的基础结构信息，因此我们也无法知晓平均实例样本和典型实例样本具有什么特征。使用<strong>概率测量方法</strong>可以解决这个问题。</p>
<p>三要素</p>
<ul>
<li>k值的选择</li>
<li>距离的度量（常见的距离度量有欧式距离，马氏距离等）</li>
<li>分类决策规则 （多数表决规则）</li>
</ul>
<p><code>k</code> 值的选择</p>
<ul>
<li><code>k</code> 值越小表明模型越复杂，更加容易过拟合</li>
<li>但是k值越大，模型越简单，如果 <code>k=N</code> 的时候就表明无论什么点都是训练集中类别最多的那个类</li>
</ul>
<p>所以一般 <code>k</code> 会取一个较小的值，然后用过交叉验证来确定。这里所谓的交叉验证就是将样本划分一部分出来为预测样本，比如 95% 训练，5% 预测，然后 <code>k</code> 分别取1，2，3，4，5之类的，进行预测，计算最后的分类误差，选择误差最小的k</p>
<h3 id="KD树"><a href="#KD树" class="headerlink" title="KD树"></a>KD树</h3><p>KD 树是一个二叉树，表示对K维空间的一个划分，可以进行快速检索（那KNN计算的时候不需要对全样本进行距离的计算了）</p>
<p>在k维的空间上循环找子区域的中位数进行划分的过程。假设现在有K维空间的数据集 <code>T={x1,x2,x3,…xn},xi={a1,a2,a3..ak}</code></p>
<ol>
<li>首先构造根节点，以坐标 <code>a1</code> 的中位数 <code>b</code> 为切分点，将根结点对应的矩形局域划分为两个区域，区域 1 中 <code>a1b</code></li>
<li>构造叶子节点，分别以上面两个区域中 <code>a2</code> 的中位数作为切分点，再次将他们两两划分，作为深度1的叶子节点，（如果 a2 = 中位数 ，则 a2 的实例落在切分面）</li>
<li>不断重复 2 的操作，深度为j的叶子节点划分的时候，索取的<code>ai</code> 的 <code>i=j%k+1</code>，直到两个子区域没有实例时停止</li>
</ol>
<p>KD树的搜索</p>
<ol>
<li>首先从根节点开始递归往下找到包含x的叶子节点，每一层都是找对应的xi</li>
<li>将这个叶子节点认为是当前的“近似最近点”</li>
<li>递归向上回退，如果以 <code>x</code> 圆心，以“近似最近点”为半径的球与根节点的另一半子区域边界相交，则说明另一半子区域中存在与 <code>x</code> 更近的点，则进入另一个子区域中查找该点并且更新”近似最近点“</li>
<li>重复 3 的步骤，直到另一子区域与球体不相交或者退回根节点</li>
<li>最后更新的”近似最近点“与 <code>x</code> 真正的最近点</li>
</ol>
<p><strong>KD树进行KNN查找</strong></p>
<p>通过 KD 树的搜索找到与搜索目标最近的点，这样 KNN 的搜索就可以被限制在空间的局部区域上了，可以大大增加效率。</p>
<p><strong>KD树搜索的复杂度</strong></p>
<p>当实例随机分布的时候，搜索的复杂度为 log(N)，N 为实例的个数，KD 树更加适用于实例数量远大于空间维度的 KNN 搜索，如果实例的空间维度与实例个数差不多时，它的效率基于等于线性扫描。</p>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>决策树的主要优势在于数据形式非常容易理解。决策树很多任务都是为了数据中所蕴含的知识信息，因此决策树可以使用不熟悉的数据集合，并从中提取出一系列规则，机器学习算法最终将使用这些机器从数据集中创造的规则。</p>
<ul>
<li>优点<ul>
<li>计算复杂度不高</li>
<li>输出结果易于理解</li>
<li>对中间值的缺失不敏感</li>
<li>可以处理不相关特征数据</li>
</ul>
</li>
<li>缺点<ul>
<li>容易过拟合（后面出现了随机森林）</li>
</ul>
</li>
<li>适用数据类型<ul>
<li>数值型</li>
<li>标称型</li>
</ul>
</li>
<li>算法类型<ul>
<li>分类算法。</li>
</ul>
</li>
<li>数据要求<ul>
<li>树的构造只适用于标称型的数据，因此数值型数据必须离散化。</li>
</ul>
</li>
</ul>
<p>在构造决策树时，我们需要解决的第一个问题就是，当前数据集上哪个特征在划分数据分类时起决定性作用。为了找到决定性的特征，划分出最好的结果，我们必须评估每个特征。完成测试之后，原始数据集就被划分为几个数据子集。这些数据子集会分布在第一个决策点的所有分支上。</p>
<p>创建分支的伪代码如下：</p>
<pre><code>检测数据集中的每个子项是否属于同一分类：
   if so return 类标签；
   else
       寻找数据集的最好特征
       划分数据集
       创建分支结点
           for 每个划分的子集
               调用函数createBranch并增加返回结果到分支结点中
           return 分支结点
</code></pre><p>划分数据集的大原则是：将无序的数据变得更加有序。组织杂乱无章数据的一种方法就是使用信息论度量信息。<strong>信息增益(information gain)</strong>和<strong>熵(entropy)</strong></p>
<p>信息熵的计算公式为：</p>
 $$H=-\sum P(x_i)log_2P(x_i)$$ 
<p>其中  $P(x_i)$ 表示选择该分类的概率。</p>
<p>决策树分类器就像带有终止块的流程图，终止块表示分类结果。开始处理数据时，我们首先需要测量集合中数据的不一致性，也就是熵，然后寻找最优方案划分数据集，直到数据集中的所有数据属于同一分类。</p>
<p>ID3 算法可以用于划分标称型数据集。构造决策树时，我们通常采用递归的方法将数据集转化为决策树。</p>
<p>决策树可能会产生过多的数据集划分，从而产生过度匹配数据集的问题。我们可以通过裁剪决策树，合并相邻的无法产生大量信息增益的叶节点，消除过度匹配的问题。</p>
<h3 id="ID3、C4-5-amp-CART"><a href="#ID3、C4-5-amp-CART" class="headerlink" title="ID3、C4.5&amp;CART"></a>ID3、C4.5&amp;CART</h3><p>其实不同的决策树学习算法只是它们选择特征的依据不同，决策树的生成过程都是一样的（根据当前环境对特征进行贪婪的选择）。</p>
<p>ID3 算法的核心是在决策树各个节点上应用信息增益准则选择特征，每一次都选择使得信息增益最大的特征进行分裂，递归地构建决策树。</p>
<p>ID3 算法以信息增益作为划分训练数据集的特征，有一个致命的缺点。选择取值比较多的特征往往会具有较大的信息增益，所以 ID3 偏向于选择取值较多的特征。</p>
<p>针对 ID3 算法的不足，C4.5 算法根据信息增益比来选择特征，对这一问题进行了校正。</p>
<p>CART 指的是分类回归树，它既可以用来分类，又可以被用来进行回归。CART 用作回归树时用平方误差最小化作为选择特征的准则，用作分类树时采用基尼指数最小化原则，进行特征选择，递归地生成二叉树。</p>
<p>决策树的剪枝：我们知道，决策树在生成的过程中采用了贪婪的方法来选择特征，从而达到对训练数据进行更好地拟合（其实从极端角度来看，决策树对训练集的拟合可以达到零误差）。而决策树的剪枝是为了简化模型的复杂度，防止决策树的过拟合问题。具体的决策树剪枝策略可以参见李航的《统计学习方法》。 </p>
<h2 id="Logistic-回归"><a href="#Logistic-回归" class="headerlink" title="Logistic 回归"></a>Logistic 回归</h2><ul>
<li>优点<ul>
<li>计算代价不高</li>
<li>易于理解和实现</li>
</ul>
</li>
<li>缺点<ul>
<li>容易欠拟合</li>
<li>分类精度可能不高</li>
</ul>
</li>
<li>适用数据类型<ul>
<li>数值型</li>
<li>标称型</li>
</ul>
</li>
<li>算法类别<ul>
<li>分类算法</li>
</ul>
</li>
<li>适用场景<ul>
<li>二分类问题</li>
</ul>
</li>
</ul>
<p>Logistic 回归的目的是寻找一个非线性函数 Sigmoid 的最佳拟合参数，求解过程可以由最优化算法来完成。在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法又可以简化为随机梯度上升算法。</p>
<p>logistic函数表达式为：</p>
 $$h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}$$ 
<p>Sigmoid 函数的定义为 </p>
<p>$$g(z) = \frac{1}{1+e^{-z}}$$</p>
<p>导数形式为</p>
<p>$$g’(z)=g(z)(1-g(z))$$</p>
<p>函数值域范围(0,1)。可以用来做分类器。Sigmoid函数的函数曲线如下：</p>
<p><img src="/images/14734728090031.gif" alt=""></p>
<p>逻辑回归模型分解如下：</p>
<p>(1)首先将不同维度的属性值和对应的一组权重加和，公式如下： </p>
 $$z=w_0+w_1x_1+w_2x_2+\dots+w_mx_m$$ 
<p>其中 $x_1,x_2,\dots,x_m$是某样本数据的各个特征，维度为m</p>
<p>这里就是一个线性回归。W权重值就是需要经过训练学习到的数值，具体W向量的求解，就需要用到极大似然估计和将似然估计函数代入到 优化算法来求解。最常用的最后化算法有 梯度上升算法。</p>
<p>单个样本的后验概率为：</p>
 $$p(y\;|\;x;\theta)=(h_\theta(x))^y(1-h_\theta(x))^{(1-y)}$$ 
<p>整个样本的后验概率：</p>
 $$L(\theta)=\sum_{i=1}^mp(y^{(i)}\;|\;x^{(i)},\theta)$$ 
<p>其中 </p>
 $$P(y=1\;|\;x, \theta)=h_\theta(x)$$ 
 $$P(y=0\;|\;x, \theta)=1-h_\theta(x)$$ 
<p>对整个样本的后验概率取对数得到：</p>
 $$\ell(\theta)=logL(\theta)=\sum_{i=1}^my^{(i)}log\;h(x^{(i)})+(1-y^{(i)})log(1-h(x^{(i)}))$$ 
<p>然后利用梯度下降来进行求解，得到最终的 $\theta$</p>
<p>由上面可见：逻辑回归函数虽然是一个非线性的函数，但其实其去除Sigmoid映射函数之后，其他步骤都和线性回归一致。</p>
<p>(2)然后将上述的线性目标函数 <code>z</code> 代入到 sigmoid 逻辑回归函数，可以得到值域为（0,0.5)和（0.5,1）两类值，等于 0.5 的怎么处理还以自己定。这样其实就得到了2类数据，也就体现了二分类的概念。</p>
<p>总结：Logistic 回归的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，参数的求解过程可以由最优化算法来完成。在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法有可以简化为随机梯度上升算法。</p>
<p>随机梯度上升算法与梯度上升算法的效果相当，但占用更少的计算资源。此外，随机梯度上升是一个在线算法，它可以在新数据到来时就完成参数更新，而不需要重新读取整个数据集来进行批处理运算。</p>
<p>处理数据中的缺失值的技巧：</p>
<ul>
<li>使用可用特征的均值来填补缺失值</li>
<li>使用特殊值来填补缺失值，如 -1</li>
<li>忽略有缺失值的样本</li>
<li>使用相似样本的均值填补缺失值</li>
<li>使用另外的机器学习算法预测缺失值</li>
</ul>
<h3 id="关于-LR-的过拟合问题"><a href="#关于-LR-的过拟合问题" class="headerlink" title="关于 LR 的过拟合问题"></a>关于 LR 的过拟合问题</h3><p>如果我们有很多的特性，在训练集上拟合得很好，但是在预测集上却达不到这种效果</p>
<ol>
<li>减少 feature 个数（人工定义留多少个 feature、算法选取这些 feature）</li>
<li>正则化（留下所有的 feature，但对于部分 feature 定义其 parameter 非常小）</li>
</ol>
<h3 id="关于LR的多分类-softmax"><a href="#关于LR的多分类-softmax" class="headerlink" title="关于LR的多分类 softmax"></a>关于LR的多分类 softmax</h3><p>softmax 假设离散型随机变量Y的取值集合是 {1,2,..,k},则多分类的LR为</p>
 $$P(Y=a\;|\;x)=exp(w_ax)/(1-\sum_{i=1}^kw_ix)$$ 
<p>这里会输出当前样本下属于哪一类的概率，并且满足全部概率加起来=1</p>
<h3 id="关于-softmax-和-k-个-LR-的选择"><a href="#关于-softmax-和-k-个-LR-的选择" class="headerlink" title="关于 softmax 和 k 个 LR 的选择"></a>关于 softmax 和 k 个 LR 的选择</h3><p>如果类别之间是否互斥（比如音乐只能属于古典音乐、乡村音乐、摇滚月的一种）就用softmax</p>
<p>否则类别之前有联系（比如一首歌曲可能有影视原声，也可能包含人声，或者是舞曲），这个时候使用k个LR更为合适</p>
<h2 id="树回归"><a href="#树回归" class="headerlink" title="树回归"></a>树回归</h2><ul>
<li>优点<ul>
<li>可以对复杂和非线性的数据建模。</li>
</ul>
</li>
<li>缺点<ul>
<li>结果不易理解。</li>
</ul>
</li>
<li>适用数据类型<ul>
<li>数值型</li>
<li>标称型</li>
</ul>
</li>
<li>算法类型<ul>
<li>回归算法</li>
</ul>
</li>
</ul>
<p>简述：线性回归方法可以有效的拟合所有样本点(局部加权线性回归除外）。当数据拥有众多特征并且特征之间关系十分复杂时，构建全局模型的回归算法是比较困难的。此外，实际中很多问题为非线性的，例如常见的分段函数，不可能用全局线性模型类进行拟合。树回归将数据集切分成多份易建模的数据，然后利用线性回归进行建模和拟合。较为经典的树回归算法为 CART(classification and regreesion trees 分类回归树)。</p>
<p>分类回归树(Classification And Regression Tree)是一个决策二叉树，在通过递归的方式建立，每个节点在分裂的时候都是希望通过最好的方式将剩余的样本划分成两类，这里的分类指标：</p>
<ul>
<li>分类树：基尼指数最小化(<code>gini_index</code>)</li>
<li>回归树：平方误差最小化</li>
</ul>
<p>分类树：</p>
<ol>
<li>首先是根据当前特征计算他们的基尼增益</li>
<li>选择基尼增益最小的特征作为划分特征</li>
<li>从该特征中查找基尼指数最小的分类类别作为最优划分点</li>
<li>将当前样本划分成两类，一类是划分特征的类别等于最优划分点，另一类就是不等于</li>
<li>针对这两类递归进行上述的划分工作，直达所有叶子指向同一样本目标或者叶子个数小于一定的阈值</li>
</ol>
<p>GINI 用来度量分布不均匀性（或者说不纯），总体的类别越杂乱，GINI 指数就越大（跟熵的概念很相似）</p>
<h3 id="解决决策树的过拟合"><a href="#解决决策树的过拟合" class="headerlink" title="解决决策树的过拟合"></a>解决决策树的过拟合</h3><ol>
<li>剪枝<ol>
<li>前置剪枝：在分裂节点的时候设计比较苛刻的条件，如不满足则直接停止分裂（这样干决策树无法到最优，也无法得到比较好的效果）</li>
<li>后置剪枝：在树建立完之后，用单个节点代替子树，节点的分类采用子树中主要的分类（这种方法比较浪费前面的建立过程）</li>
</ol>
</li>
<li>交叉验证</li>
<li>随机森林</li>
</ol>
<h2 id="随机森林-RF"><a href="#随机森林-RF" class="headerlink" title="随机森林 RF"></a>随机森林 RF</h2><p><strong>优缺点</strong></p>
<ol>
<li>能够处理大量特征的分类，并且还不用做特征选择</li>
<li>能够处理具有高维特征的输入样本，而且不需要降维</li>
<li>在训练完成之后能给出哪些feature的比较重要</li>
<li>在生成过程中，能够获取到内部生成误差的一种无偏估计</li>
<li>对于缺省值问题也能够获得很好得结果</li>
<li>训练速度很快，很容易并行</li>
<li>实现相对来说较为简单</li>
</ol>
<p>随机森林就是通过集成学习的思想将多棵树集成的一种算法，它的基本单元是决策树，而它的本质属于机器学习的一大分支——集成学习(Ensemble Learning)方法。随机森林的名称中有两个关键词，一个是“随机”，一个就是“森林”。“森林”我们很好理解，一棵叫做树，那么成百上千棵就可以叫做森林了，这样的比喻还是很贴切的，其实这也是随机森林的主要思想–集成思想的体现。</p>
<p>其实从直观角度来解释，每棵决策树都是一个分类器（假设现在针对的是分类问题），那么对于一个输入样本，N棵树会有N个分类结果。而随机森林集成了所有的分类投票结果，将投票次数最多的类别指定为最终的输出，这就是一种最简单的 Bagging 思想。</p>
<p>bagging 的代价是不用单棵决策树来做预测，具体哪个变量起到重要作用变得未知，所以 bagging 改进了预测准确率但损失了解释性。</p>
<p>一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。</p>
<p>随机森林分类效果（错误率）与两个因素有关：</p>
<ul>
<li>森林中任意两棵树的相关性：相关性越大，错误率越大；</li>
<li>森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。</li>
</ul>
<p>减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。</p>
<p>构建随机森林的关键问题就是如何选择最优的m，要解决这个问题主要依据计算袋外错误率 oob error（out-of-bag error）。</p>
<p>随机森林有一个重要的优点就是，没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。它可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。</p>
<p>我们知道，在构建每棵树时，我们对训练集使用了不同的bootstrap sample（随机且有放回地抽取）。所以对于每棵树而言（假设对于第k棵树），大约有1/3的训练实例没有参与第k棵树的生成，它们称为第k棵树的 oob 样本。<br>而这样的采样特点就允许我们进行 oob 估计，它的计算方式如下：</p>
<ol>
<li>对每个样本，计算它作为 oob 样本的树对它的分类情况（约1/3的树）；</li>
<li>然后以简单多数投票作为该样本的分类结果；</li>
<li>最后用误分个数占样本总数的比率作为随机森林的 oob 误分率。</li>
</ol>
<p><strong>学习过程</strong></p>
<ol>
<li>现在有 N 个训练样本，每个样本的特征为 M 个，需要建 K 颗树</li>
<li>从 N 个训练样本中有放回的取 N 个样本作为一组训练集（其余未取到的样本作为预测分类，评估其误差），每棵树的训练集都是不同的，而且里面包含重复的训练样本</li>
<li>从 M 个特征中取 m 个特征左右子集特征(<code>m&lt;&lt;M</code>)，随机地从 M 个特征中选取 m 个特征子集，每次树进行分裂时，从这 m 个特征中选择最优的；</li>
<li>对采样的数据使用完全分裂的方式来建立决策树，这样的决策树每个节点要么无法分裂，要么所有的样本都指向同一个分类，每棵树都尽最大程度的生长，并且没有剪枝过程。</li>
<li>重复 2 的过程 K 次，即可建立森林</li>
</ol>
<p><strong>预测过程</strong></p>
<ol>
<li>将预测样本输入到K颗树分别进行预测</li>
<li>如果是分类问题，直接使用投票的方式选择分类频次最高的类别</li>
<li>如果是回归问题，使用分类之后的均值作为结果</li>
</ol>
<p><strong>参数问题</strong></p>
<ol>
<li>这里的一般取 <code>m=sqrt(M)</code></li>
<li>关于树的个数 K，一般都需要成百上千，但是也有具体的样本有关（比如特征数量）</li>
<li>树的最大深度，（太深可能可能导致过拟合）</li>
<li>节点上的最小样本数、最小信息增益</li>
</ol>
<p><strong>学习算法</strong></p>
<ol>
<li>ID3 算法：处理离散值的量</li>
<li>C45 算法：处理连续值的量</li>
<li>Cart 算法：离散和连续 </li>
</ol>
<p>随机森林是一种集成学习+决策树的分类模型，它可以利用集成的思想（投票选择的策略）来提升单颗决策树的分类性能（通俗来讲就是“三个臭皮匠，顶一个诸葛亮”）。</p>
<p>集集成学习和决策树于一身，随机森林算法具有众多的优点，其中最为重要的就是在随机森林算法中每棵树都尽最大程度的生长，并且没有剪枝过程。</p>
<p>随机森林引入了两个随机性——随机选择样本（bootstrap sample）和随机选择特征进行训练。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）</p>
<h3 id="GDBT"><a href="#GDBT" class="headerlink" title="GDBT"></a>GDBT</h3><p>迭代决策树 GBDT(Gradient Boosting Decision Tree）也被称为是 MART(Multiple Additive Regression Tree)或者是 GBRT(Gradient Boosting Regression Tree)，也是一种基于集成思想的决策树模型，但是它和 Random Forest 有着本质上的区别。不得不提的是，GBDT 是目前竞赛中最为常用的一种机器学习算法，因为它不仅可以适用于多种场景，更难能可贵的是，GBDT 有着出众的准确率。这也是为什么很多人称 GBDT 为机器学习领域的“屠龙刀”。</p>
<p>这么牛叉的算法，到底是怎么做到的呢？说到这里，就不得不说一下 GBDT 中的 “GB”（Gradient Boosting）。Gradient Boosting 的原理相当的复杂，但是看不懂它也不妨碍我们对 GBDT 的理解和认识</p>
<p>Boosting，迭代，即通过迭代多棵树来共同决策。这怎么实现呢？难道是每棵树独立训练一遍，比如 A 这个人，第一棵树认为是 10 岁，第二棵树认为是 0 岁，第三棵树认为是 20 岁，我们就取平均值 10 岁做最终结论？当然不是！且不说这是投票方法并不是 GBDT，只要训练集不变，独立训练三次的三棵树必定完全相同，这样做完全没有意义。之前说过，GBDT 是把所有树的结论累加起来做最终结论的，所以可以想到每棵树的结论并不是年龄本身，而是年龄的一个累加量。GBDT 的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是 18 岁，但第一棵树的预测年龄是 12 岁，差了 6 岁，即残差为 6 岁。那么在第二棵树里我们把 A 的年龄设为 6 岁去学习，如果第二棵树真的能把 A 分到 6 岁的叶子节点，那累加两棵树的结论就是 A 的真实年龄；如果第二棵树的结论是 5 岁，则 A 仍然存在 1 岁的残差，第三棵树里 A 的年龄就变成 1 岁，继续学。这就是 Gradient Boosting 在 GBDT 中的意义。</p>
<h2 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h2><ul>
<li>优点<ul>
<li>泛化错误率低</li>
<li>计算开销不大</li>
<li>结果易解释</li>
</ul>
</li>
<li>缺点<ul>
<li>对参数调节和核函数的选择敏感</li>
<li>原始分类器不加修改仅适用于处理二元分类问题</li>
</ul>
</li>
<li>适用数据类型<ul>
<li>数值型</li>
<li>标称型</li>
</ul>
</li>
<li>类别<ul>
<li>分类算法</li>
</ul>
</li>
<li>适用场景<ul>
<li>解决二分类问题。</li>
</ul>
</li>
</ul>
<p>将数据集分隔开来的直线称为<strong>分隔超平面(separating hyperplane)</strong>。如果数据对象是 1024 维的，那么就需要一个1023维的某某对象来对数据进行分隔，这个对象就叫<strong>超平面(hyperplane)</strong>，也就是分类的决策边界。分布在超平面一侧的所有数据都属于某个类别，而分布在另一侧的所有数据则属于另一个类别。</p>
<p>我们希望能找到离分隔超平面最近的点，确保它们离分隔面的距离尽可能远。这里点到分隔面的距离被称为<strong>间隔(margin)</strong>。我们希望间隔尽可能大，因为如果我们犯错或者在有限数据上训练分类器的话，我们希望分类器尽可能健壮。</p>
<p><strong>支持向量(support vector)</strong>就是离分隔超平面最近的那些点。接下来要试着最大化支持向量到分隔面的距离。</p>
<p>分隔超平面的形式可以写成 $w^T+b$。要计算点 A 到分隔超平面的距离，就必须给出点到分隔面的法线或垂线的长度，值为 $$\frac{|w^TA+b|}{||w||}$$。这里的常数 b 类似于 Logistic 回归中的截距 $w_0$。</p>
<p>当计算数据点到分隔面的距离并确定分隔面的放置位置时，间隔是通过 $label\times(w^T+b)$来计算的。如果数据点处于正方向(+1)，$w^Tx+b$ 会是一个很大的正数，同时 $label\times(w^T+b)$也会是一个很大的正数；而处于负方向时(-1)，$label\times(w^T+b)$ 仍然会是一个很大的正数。</p>
<p>现在的目标就是找出分类器定义中的 w 和 b。为此，我们必须找到具有最小间隔的数据点，而这些数据点也就是前面提到的支持向量。一旦找到具有最小间隔的数据点，我们就需要对该间隔最大化：</p>
 $$arg max_{w,b}\{min_n(label·(w^Tx+b))·\frac{1}{||w||}\}$$ 
<p>直接求解上述问题相当困难，所以需要将它转换成为另一种更容易求解的形式。</p>
<p>先考察一下大括号中的部分。由于对乘积进行优化是一件很讨厌的事情，因此我们要做的是固定其中一个因子而最大化其他因子。如果令所有支持向量的 $label\times(w^T+b)$  都为 1，那么就可以通过求 <code>||w||</code> 的最大值来得到最终解。但是，并非所有数据点的 $label\times(w^T+b)$  都等于 1，只有那些离分割超平面最近的点得到的值才为 1。而离超平面越远的数据点，其 $label\times(w^T+b)$  的值也就越大。</p>
<p>这里的约束条件就是 $label\times(w^T+b) \ge 1$ 。对于这类优化问题，有一个非常著名的求解方法，拉格朗日乘子法。通过引入拉格朗日乘子，我们就可以基于约束条件来表述原来的问题。由于这里的约束条件都是基于数据点的，因此我们就可以将超平面写成数据点的形式，优化函数就变成</p>
 $$max_\alpha[\sum_{i=1}^m\alpha-\frac{1}{2}label^{(i)}·label^{(j)}·\alpha_i·\alpha_j\langle x^{(i)},x^{(j)}\rangle]$$ 
<blockquote>
<p>$label\times(w^T+b)$ 被称为点到分隔面的函数间隔，$\frac{|w^TA+b|}{||w||}$ 称为点到分隔面的几何间隔<br>尖括号表示两个向量的内积</p>
</blockquote>
<p>约束条件为</p>
 $$\alpha \ge 0 \; and \; \sum_{i=1}^m \alpha_i·label^{(i)} = 0$$ 
<p>考虑到数据不可能非常完美，就需要引入<strong>松弛变量(slack variable)</strong>来允许有些数据点可以处于分隔面错误的一侧。这样我们的优化目标就能保持仍然不变，但约束条件变成：</p>
 $$C \ge\alpha \ge 0 \; and \; \sum_{i=1}^m \alpha_i·label^{(i)} = 0$$
<p>这里的常数 C 用于控制“最大化间隔”和“保证大部分点的函数间隔小于1.0”这两个目标的权重。在优化算法的实现代码中，常数 C 是一个参数，因此我们就可以通过调节该参数得到不同的结果。一旦求出了所有的 α，那么分隔超平面就可以通过这些 α 来表达。这一结论十分直接，SVM 中的主要工作就是求解这些 α。</p>
<h3 id="SMO-高效优化算法（Sequential-Minimal-Optimization，SMO）"><a href="#SMO-高效优化算法（Sequential-Minimal-Optimization，SMO）" class="headerlink" title="SMO 高效优化算法（Sequential Minimal Optimization，SMO）"></a>SMO 高效优化算法（Sequential Minimal Optimization，SMO）</h3><p><strong>Platt 的 SMO 算法</strong></p>
<p>SMO 表示<strong>序列最小化(Sequential Minimal Optimization)</strong>。Platt 的 SMO 算法是将大优化问题分解为多个小优化问题来求解的。这些小优化问题往往很容易求解，并且对它们进行顺序求解的结果与将它们作为整体来求解的结果是完全一致的。</p>
<p>SMO 算法的目标是求出一系列 α 和 b，一旦求出了这些 α，就很容易计算出权重向量 w 并得到分隔超平面。</p>
<p>它选择凸二次规划的两个变量，其他的变量保持不变，然后根据这两个变量构建一个二次规划问题，这个二次规划关于这两个变量解会更加的接近原始二次规划的解，通过这样的子问题划分可以大大增加整个算法的计算速度，关于这两个变量：</p>
<ol>
<li>其中一个是严重违反KKT条件的一个变量</li>
<li>另一个变量是根据自由约束确定，好像是求剩余变量的最大化来确定的。</li>
</ol>
<p>SMO 算法的工作原理是：每次循环中选择两个 α 进行优化处理。一旦找到一对合适的 α，那么就增大其中一个同时减小另一个。这里所谓的“合适”就是指两个 α 必须要符合一定的条件，条件之一就是这两个 α 必须要在间隔边界之外，第二个条件则是这两个 α 还没有进行过区间化处理或者不在边界上。</p>
<p>Platt SMO 算法中的外循环确定要优化的最佳 α 对。而简化版会跳过这一部分，首先在数据集上遍历每一个 α，然后在剩下的 α 集合中随机选择另一个 α，从而构建 α 对。这一点相当重要，要同时改变，因为我们有一个约束条件：</p>
 $$\alpha \ge 0 \; and \; \sum_{i=1}^m \alpha_i·label^{(i)} = 0$$
<p>由于改变一个 α 可能会导致该约束条件失效，因此我们总是同时改变两个 α。</p>
<p>伪代码：</p>
<pre><code>创建一个 α 向量并将其初始化为 0 向量
当迭代次数小于最大迭代次数时(外循环)
    对数据集中的每个数据向量(内循环):
        如果该数据向量可以被优化:
            随机选择另外一个数据向量
            同时优化这两个向量
            如果两个向量都不能被优化，退出内循环
    如果所有向量都没有被优化，增加迭代数量，继续下一次循环
</code></pre><p><strong>完整的Platt SMO算法</strong></p>
<p>在在选择第一个 α 值后，算法会通过一个内循环来选择第二个 α 值。在优化过程中，会通过<strong>最大化步长</strong>的方式来获得第二个 α 值。</p>
<h3 id="在复杂数据上应用核函数"><a href="#在复杂数据上应用核函数" class="headerlink" title="在复杂数据上应用核函数"></a>在复杂数据上应用核函数</h3><p>核函数(kernel) 和 径向基函数(fadial basis function)</p>
<h4 id="利用核函数将数据映射到高维空间"><a href="#利用核函数将数据映射到高维空间" class="headerlink" title="利用核函数将数据映射到高维空间"></a>利用核函数将数据映射到高维空间</h4><p>从某个特征空间到另一个特征空间的映射是通过核函数来实现的。可以把核函数想象成一个<strong>包装器(wrapper)</strong>或者是<strong>接口(interface)</strong>，它能把数据从某个很难处理的形式转换成另一种较易处理的形式。</p>
<p>SVM 优化中一个特别好的地方是，所有的运算都可以写成<strong>内积(inner product)</strong>的形式。向量的内积指的是两个向量相乘，之后得到单个标量或者数值。我们可以把内积运算替换成核函数，而不必做简化处理。将内积替换成核函数的方式被称为<strong>核技巧(kernel trick)</strong>或者<strong>核变电(kernel substation)</strong>。</p>
<h4 id="径向基核函数"><a href="#径向基核函数" class="headerlink" title="径向基核函数"></a>径向基核函数</h4><p>采用向量作为自变量的函数，基于向量距离运算输出一个标量。这个距离可以是从<0, 0="">向量或者其他向量开始计算的距离，我们使用径向基函数的高斯版本，公式如下：</0,></p>
<p>$$k(x,y)=exp(\frac{-||x-y||^2}{2\sigma^2})$$</p>
<p>上述高斯核函数将数据从其特征空间映射到更高维的空间，具体来说这里是映射到一个无穷维的空间。</p>
<p>支持向量的数目存在一个最优值。SVM 的优点在于它能对数据进行高效分类。如果支持向量太少，就可能会得到一个很差的决策边界；如果支持向量太多，也就相当于每次都利用整个数据集进行分类，这种分类方法称为 k近邻。</p>
<p>可以这么看 SVM 比 k 近邻好的地方在于，从很多数据中找到最有代表性的数据点来作为分类的依据，可以有效减少多余的计算。</p>
<h3 id="SVM-多分类方法"><a href="#SVM-多分类方法" class="headerlink" title="SVM 多分类方法"></a>SVM 多分类方法</h3><p><strong>一对多</strong></p>
<p>其中某个类为一类，其余 n-1 个类为另一个类，比如 A,B,C,D 四个类，第一次 A 为一个类，{B,C,D} 为一个类训练一个分类器，第二次 B 为一个类,{A,C,D} 为另一个类,按这方式共需要训练 4 个分类器，最后在测试的时候将测试样本经过这4个分类器f1(x), f2(x), f3(x) 和 f4(x),取其最大值为分类器(这种方式由于是 1 对 M 分类，会存在偏置，很不实用)</p>
<p><strong>一对一(libsvm实现的方式)</strong></p>
<p>任意两个类都训练一个分类器，那么n个类就需要 <code>n*(n-1)/2</code> 个svm分类器。</p>
<p>还是以 A,B,C,D 为例,那么需要 {A,B},{A,C},{A,D},{B,C},{B,D},{C,D} 为目标共6个分类器，然后在预测的将测试样本通过这6个分类器之后进行投票选择最终结果。（这种方法虽好，但是需要 <code>n*(n-1)/2</code> 个分类器代价太大，不过有好像使用循环图来进行改进）</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>支持向量机的泛化错误率较低，也就是说它具有良好的学习能力，且学到的结果具有很好的推广性。这些优点使得向量机十分流行，有些人认为它是监督学习中最好的定式算法。</p>
<p>支持向量机试图通过求解一个二次优化问题来最大化分类间隔。</p>
<p>和方法或者说核技巧会将数据(有时是非线性数据)从一个低维空间映射到一个高维空间，可以将一个在低维空间中的非线性问题转换成高维空间下的线性问题来求解。和方法不止在 SVM 中适用，还可以用于其他算法中。而其中径向基函数是一个常用的度量两个向量距离的核函数。</p>
<p>支持向量机是一个二元分类器。当用其解决多元问题时，则需要额外的方法对其进行扩展。SVM 的效果也对优化参数和所用核函数中的参数敏感。</p>
<h2 id="Bagging-和-Boosting"><a href="#Bagging-和-Boosting" class="headerlink" title="Bagging 和 Boosting"></a>Bagging 和 Boosting</h2><p>使用机器学习方法解决问题时，有较多模型可供选择。 一般的思路是先根据数据的特点，快速尝试某种模型，选定某种模型后， 再进行模型参数的选择（当然时间允许的话，可以对模型和参数进行双向选择）</p>
<p>因为不同的模型具有不同的特点， 所以有时也会将多个模型进行组合，以发挥”三个臭皮匠顶一个诸葛亮的作用”，这样的思路，反应在模型中，主要有两种思路：Bagging和Boosting</p>
<h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><p>Bagging 可以看成是一种圆桌会议，或是投票选举的形式，其中的思想是：”群众的眼光是雪亮的”，可以训练多个模型，之后将这些模型进行加权组合，一般这类方法的效果，都会好于单个模型的效果。 在实践中，在特征一定的情况下，大家总是使用Bagging的思想去提升效果。例如kaggle上的问题解决，因为大家获得的数据都是一样的，特别是有些数据已经过预处理。</p>
<p>基本的思路比较简单，就是：训练时，使用 replacement 的 sampling 方法， sampling 一部分训练数据k次并训练 k 个模型；预测时，使用 k 个模型，如果为分类，则让 k 个模型均进行分类并选择出现次数最多的类(每个类出现的次数占比可以视为置信度)；如为回归，则为各类器返回的结果的平均值。</p>
<p>在该处，Bagging 算法可以认为每个分类器的权重都一样。</p>
<h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p>在 Bagging 方法中，我们假设每个训练样本的权重都是一致的； 而 Boosting 算法则更加关注错分的样本，越是容易错分的样本，约要花更多精力去关注。对应到数据中，就是该数据对模型的权重越大，后续的模型就越要拼命将这些经常分错的样本分正确。 最后训练出来的模型也有不同权重，所以 boosting 更像是会整，级别高，权威的医师的话语权就重些。</p>
<p>Bagging 和 Boosting 都可以视为比较传统的集成学习思路。 现在常用的 Random Forest，GBDT，GBRank 其实都是更加精细化，效果更好的方法</p>
<h2 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h2><ul>
<li>优点<ul>
<li>泛化错误率低</li>
<li>易编码，可以应用在大部分分类器上</li>
<li>无参数调整</li>
</ul>
</li>
<li>缺点<ul>
<li>对离群点敏感</li>
</ul>
</li>
<li>适用数据类型<ul>
<li>数值型</li>
<li>标称型</li>
</ul>
</li>
</ul>
<p>boosting 是一种与 bagging 很类似的技术。不论是在 boosting 还是 bagging 当中，所使用的多个分类器的类型都是一致的。但是在前者当中，不同的分类器是通过串行训练而获得的，每个新分类器都根据已训练出的分类器的性能来进行训练。boosting 是通过集中关注被已有分类器错分的那些数据来获得新的分类器。</p>
<p>由于 boosting 分类的结果是基于所有分类器的加权求和结果的，因此 boosting 与 bagging 不太一样。bagging 中的分类器权重是相等的，而 boosting 中的分类器权重并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。</p>
<p>boosting 方法拥有多个版本，这里只关注最流行的 AdaBoost</p>
<p>能否使用弱分类器和多个实例来构建一个强分类器？这是一个非常有趣的理论问题。这里的“弱”意味着分类器的性能比所及猜测要略好，但是也不会好太多。AdaBoost 算法即脱胎于上述理论问题。</p>
<p>AdaBoost 是 adaptive boosting(自适应 boosting)的缩写，其运行过程如下：训练数据中的每个样本各有一个权重，这些权重构成了向量 D。一开始，这些权重都初始化成相等值。首先在训练数据上训练处一个弱分类器并计算该分类器的错误率，然后在同一数据集上再次训练弱分类器。在分类器的第二次训练当中，将会重新调整每个样本的权重，其中第一次分对的样本的权重将会降低，而第一次分错的样本的权重将会提高。</p>
<p>为了从所有弱分类器中得到最终的分类结果，AdaBoost 为每个分类器都分配了一个权重值 α，这些 α 值是基于每个弱分类器的错误率进行计算的。错误率和 α 的公式为</p>
<p>$$\epsilon=\frac{未正确分类的样本数目}{所有样本数目}$$</p>
<p>$$\alpha=\frac{1}{2}ln(\frac{1-\epsilon}{\epsilon})$$</p>
<p>计算出 α 值之后，可以对权重向量 D 进行更新，以使那些正确分类的样本的权重降低而错分样本的权重升高。D 的计算方法如下：</p>
<p>正确情况：</p>
 $$D_i^{(i+1)}=\frac{D_i^{(i)}e^{-\alpha}}{\sum D}$$
<p>错误情况：</p>
 $$D_i^{(i+1)}=\frac{D_i^{(i)}e^{\alpha}}{\sum D}$$
<p>计算出 D 之后，AdaBoost 又开始进入下一轮迭代。AdaBoost 算法会不断地重复训练和调整权重的过程，直到训练错误率为 0 或者弱分类器的数目达到用户的指定值为止。</p>
<p>构建弱分类器：<strong>单层决策树(decision stump, 决策树桩)</strong>是一种简单的决策树，仅基于单个特征来做决策。</p>
<h2 id="KMeans"><a href="#KMeans" class="headerlink" title="KMeans"></a>KMeans</h2><ul>
<li>优点<ul>
<li>容易实现</li>
<li>对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是 O(nkt)，其中 n 是所有对象的数目，k 是簇的数目,t 是迭代的次数。通常 k&lt;&lt;n。这个算法通常局部收敛</li>
<li>算法尝试找出使平方误差函数值最小的 k 个划分。当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好</li>
</ul>
</li>
<li>缺点<ul>
<li>可能收敛到局部最小值</li>
<li>在大规模数据集上收敛较慢</li>
<li>k-平均方法只有在簇的平均值被定义的情况下才能使用，且对有些分类属性的数据不适合</li>
<li>要求用户必须事先给出要生成的簇的数目 k</li>
<li>不适合于发现非凸面形状的簇，或者大小差别很大的簇</li>
<li>对于”噪声”和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响</li>
</ul>
</li>
<li>使用数据类型<ul>
<li>数值型</li>
</ul>
</li>
<li>算法类型<ul>
<li>聚类算法</li>
</ul>
</li>
</ul>
<p>K-Means的基本步骤：</p>
<ol>
<li>从数据对象中随机的初始化K个初始点作为质心。然后将数据集中的每个点分配到一个簇中，具体来讲每个点找到距其最近的质心，并将其分配给该质心所对应的簇。</li>
<li>计算每个簇中样本点的均值，然后用均值更新掉该簇的质心。然后划分簇结点。</li>
<li>迭代重复（2）过程，当簇对象不再发生变化时，或者误差在评测函数预估的范围时，停止迭代。</li>
</ol>
<p>选择批次距离尽可能远的K个点</p>
<p>首先随机选取一个点作为初始点，然后选择距离与该点最远的那个点作为中心点，再选择距离与前两个点最远的点作为第三个中心点，以此类推，直至选取大k个</p>
<p>选用层次聚类或者 Canopy 算法进行初始聚类</p>
<p>聚类属于无监督学习，以往的回归、朴素贝叶斯、SVM 等都是有类别标签 y 的，也就是说样例中已经给出了样例的分类。而聚类的样本中却没有给定 y，只有特征 x，比如假设宇宙中的星星可以表示成三维空间中的点集(x,y,z)。聚类的目的是找到每个样本x潜在的类别y，并将同类别y的样本x放在一起。</p>
<p>在聚类问题中，给我们的训练样本是 {x(1),…,x(m)}，每个 x(i)∈ R^n，没有了 y。</p>
<p>下面累述一下 K-means 与 EM 的关系，首先回到初始问题，我们目的是将样本分成 k 个类，其实说白了就是求每个样例 x 的隐含类别 y，然后利用隐含类别将 x 归类。由于我们事先不知道类别 y，那么我们首先可以对每个样例假定一个 y吧，但是怎么知道假定的对不对呢？怎么评价假定的好不好呢？我们使用样本的极大似然估计来度量，这里是就是x和y的联合分布 P(x,y)了。如果找到的 y 能够使 P(x,y) 最大，那么我们找到的 y 就是样例 x 的最佳类别了，x 顺手就聚类了。但是我们第一次指定的 y 不一定会让 P(x,y) 最大，而且 P(x,y) 还依赖于其他未知参数，当然在给定 y 的情况下，我们可以调整其他参数让 P(x,y) 最大。但是调整完参数后，我们发现有更好的 y 可以指定，那么我们重新指定 y，然后再计算 P(x,y) 最大时的参数，反复迭代直至没有更好的 y 可以指定。</p>
<p>这个过程有几个难点，第一怎么假定 y？是每个样例硬指派一个 y 还是不同的 y 有不同的概率，概率如何度量。第二如何估计P(x,y)，P(x,y) 还可能依赖很多其他参数，如何调整里面的参数让 P(x,y) 最大。</p>
<p>这里只是指出 EM 的思想，E 步就是估计隐含类别 y 的期望值，M 步调整其他参数使得在给定类别 y 的情况下，极大似然估计 P(x,y) 能够达到极大值。然后在其他参数确定的情况下，重新估计 y，周而复始，直至收敛。</p>
<p>上面的阐述有点费解，对应于K-$x^{(i)}$对应隐含变量也就是最佳类别 $c^{(i)}$。最开始可以随便指定一个 $c^{(i)}$ 给它，然后为了让 P(x,y) 最大（这里是要让 J 最小），我们求出在给定 c 情况下，J最小时的  $u_j$（前面提到的其他未知参数），然而此时发现，可以有更好的 $c^{(i)}$（质心与样例$x^{(i)}$ 距离最小的类别）指定给样例 $x^{(i)}$，那么 $c^{(i)}$ 得到重新调整，上述过程就开始重复了，直到没有更好的 $c^{(i)}$ 指定。</p>
<p>这样从K-means里我们可以看出它其实就是EM的体现，E步是确定隐含类别变量 $c^{(i)}$，M步更新其他参数 <code>u</code> 来使J最小化。这里的隐含类别变量指定方法比较特殊，属于硬指定，从k个类别中硬选出一个给样例，而不是对每个类别赋予不同的概率。总体思想还是一个迭代优化过程，有目标函数，也有参数变量，只是多了个隐含变量，确定其他参数估计隐含变量，再确定隐含变量估计其他参数，直至目标函数最优。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>如果将样本看作观察值，潜在类别看作是隐藏变量，那么聚类问题也就是参数估计问题，只不过聚类问题中参数分为隐含类别变量和其他参数，这犹如在 x-y 坐标系中找一个曲线的极值，然而曲线函数不能直接求导，因此什么梯度下降方法就不适用了。但固定一个变量后，另外一个可以通过求导得到，因此可以使用坐标上升法，一次固定一个变量，对另外的求极值，最后逐步逼近极值。对应到EM上，E步估计隐含变量，M步估计其他参数，交替将极值推向最大。EM中还有“硬”指定和“软”指定的概念，“软”指定看似更为合理，但计算量要大，“硬”指定在某些场合如 K-means 中更为实用（要是保持一个样本点到其他所有中心的概率，就会很麻烦）。</p>
<p>另外，EM 的收敛性证明方法确实很牛，能够利用 log 的凹函数性质，还能够想到利用创造下界，拉平函数下界，优化下界的方法来逐步逼近极大值。而且每一步迭代都能保证是单调的。最重要的是证明的数学公式非常精妙，硬是分子分母都乘以z的概率变成期望来套上Jensen不等式，前人都是怎么想到的。</p>
<h2 id="PCA-主成分分析"><a href="#PCA-主成分分析" class="headerlink" title="PCA 主成分分析"></a>PCA 主成分分析</h2><ul>
<li>优点<ul>
<li>降低数据的复杂性</li>
<li>识别最重要的多个特征。</li>
</ul>
</li>
<li>缺点<ul>
<li>不一定需要</li>
<li>且可能损失有用信息。</li>
</ul>
</li>
<li>适用适用类型<ul>
<li>数值型</li>
</ul>
</li>
<li>技术类型<ul>
<li>降维技术</li>
</ul>
</li>
</ul>
<p>按照一定的数学变换方法，把给定的一组相关变量（维度）通过线性变换转成另一组不相关的变量，这些新的变量按照方差依次递减的顺序排列。在数学变换中保持变量的<code>总方差</code>不变，使<code>第一变量</code>具有<code>最大的方差</code>，称为<code>第一主成分</code>，<code>第二变量</code>的<code>方差次大</code>，并且和第一变量不相关，称为第二主成分。依次类推，I个变量就有I个主成分。</p>
<p><strong>通过低维表征的向量和特征向量矩阵，可以基本重构出所对应的原始高维向量</strong></p>
<h3 id="总结与讨论"><a href="#总结与讨论" class="headerlink" title="总结与讨论"></a>总结与讨论</h3><p>PCA技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。</p>
<p>PCA技术的一个很大的优点是，它是完全无参数限制的。在PCA的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。</p>
<p>但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。</p>
<h2 id="SVD-singular-value-decomposition-奇异值分解"><a href="#SVD-singular-value-decomposition-奇异值分解" class="headerlink" title="SVD(singular value decomposition) 奇异值分解"></a>SVD(singular value decomposition) 奇异值分解</h2><ul>
<li>优点<ul>
<li>简化数据</li>
<li>去除噪声</li>
<li>提高算法的结果。</li>
</ul>
</li>
<li>缺点<ul>
<li>数据转换可能难以理解。</li>
</ul>
</li>
<li>适用数据类型<ul>
<li>数值型</li>
</ul>
</li>
<li>SVD是矩阵分解的一种类型。</li>
</ul>
<p>总结：SVD是一种强大的降维工具，我们可以利用SVD来逼近矩阵并从中提取重要特征。通过保留矩阵 80%~90% 的能量，就可以得到重要的特征并去掉噪声。SVD已经运用到多个应用中，其中一个成功的应用案例就是推荐引擎。推荐引擎将物品推荐给用户，协同过滤则是一种基于用户喜好和行为数据的推荐和实现方法。协同过滤的核心是相似度计算方法，有很多相似度计算方法都可以用于计算物品或用户之间的相似度。通过在低维空间下计算相似度，SVD提高了推荐引擎的效果。</p>
<p>SVD将矩阵分解为三个矩阵的乘积，公式如下所示：</p>
 $$Data_{m\times n}=U_{m\times m}\Sigma_{m\times n}V_{n\times n}^T$$
<p>中间的矩阵sigma为对角矩阵，对角元素的值为Data矩阵的奇异值(注意奇异值和特征值是不同的)，且已经从大到小排列好了。即使去掉特征值小的那些特征，依然可以很好的重构出原始矩阵。</p>
<p>如果 m 代表商品的个数，n 代表用户的个数，则 U 矩阵的每一行代表商品的属性，现在通过降维 U 矩阵（取深色部分）后，每一个商品的属性可以用更低的维度表示（假设为 k 维）。这样当新来一个用户的商品推荐向量 X，则可以根据公式  $X'U_1\Sigma_1^{-1}$ 得到一个 k 维的向量，然后在 V’ 中寻找最相似的那一个用户（相似度测量可用余弦公式等），根据这个用户的评分来推荐（主要是推荐新用户未打分的那些商品）。</p>
<p>SVD++ 可以说是SVD模型的加强版，除了打分关系，SVD++还可以对隐含的回馈(implicit feedback) 进行建模。</p>
<p>这种隐含的回馈可以是打分动作（谁对某个商品打过分），或者是浏览记录等。 只要有类似的隐含回馈，客观上也表示了 user 对某个 item 的偏好。 毕竟，user 不会无缘无故地浏览一个 item，肯定有什么原因， 比如 user 喜欢紫色，恰恰这个 item 也是紫色的，那通过隐含回馈就可以对 user 对紫色的偏好建模出来。</p>
<p>现实中，隐含回馈的原因比较复杂，专门给一部分参数空间去建模，肯定对用户的建模有一些帮助。</p>
<p>除了在 SVD 中定义的向量外，每个 item 对应一个向量 yi ，来通过 user 隐含回馈过的 item 的集合来刻画用户的偏好。</p>
<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>特征工程大概包括两个部分：特征提取和特征选择。</p>
<p>一般来说，领域内的知识主要是应用在特征提取。举个例子的话，比如说基于内容的购物推荐，性别就是一个很重要的领域知识，男性和女性关注的物品差别就比较大，推荐也应该体现出这种差别，那么这个特征就是这个问题一个重要特征，应该重点从已知数据提取，甚至专门为它再构建一个机器学习问题。</p>
<p>一般简单且常见的都是卡方检验，互信息和信息增益这三种</p>
<p>关于特征工程(Feature Engineering)，已经是很古老很常见的话题了，坊间常说：“数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已”。由此可见，特征工程在机器学习中占有相当重要的地位。在实际应用当中，可以说特征工程是机器学习成功的关键。纵观 Kaggle、KDD 等国内外大大小小的比赛，每个竞赛的冠军其实并没有用到很高深的算法，大多数都是在特征工程这个环节做出了出色的工作，然后使用一些常见的算法，比如LR，就能得到出色的性能。遗憾的是，在很多的书籍中并没有直接提到 Feature Engineering，更多的是 Feature selection。这也并不，很多ML书籍都是以讲解算法为主，他们的目的是从理论到实践来理解算法，所以用到的数据要么是使用代码生成的，要么是已经处理好的数据，并没有提到特征工程。在这篇文章，我打算自我总结下特征工程，让自己对特征工程有个全面的认识。在这我要说明一下，我并不是说那些书写的不好，其实都很有不错，主要是因为它们的目的是理解算法，所以直接给出数据相对而言对于学习和理解算法效果更佳。</p>
<p>特征工程是利用数据领域的相关知识来创建能够使机器学习算法达到最佳性能的特征的过程。</p>
<p>简而言之，特征工程就是一个把原始数据转变成特征的过程，这些特征可以很好的描述这些数据，并且利用它们建立的模型在未知数据上的表现性能可以达到最优（或者接近最佳性能）。从数学的角度来看，特征工程就是人工地去设计输入变量X。</p>
<p>（1）特征越好，灵活性越强</p>
<p>只要特征选得好，即使是一般的模型（或算法）也能获得很好的性能，因为大多数模型（或算法）在好的数据特征下表现的性能都还不错。好特征的灵活性在于它允许你选择不复杂的模型，同时运行速度也更快，也更容易理解和维护。</p>
<p>（2）特征越好，构建的模型越简单</p>
<p>有了好的特征，即便你的参数不是最优的，你的模型性能也能仍然会表现的很nice，所以你就不需要花太多的时间去寻找最有参数，这大大的降低了模型的复杂度，使模型趋于简单。</p>
<p>（3）特征越好，模型的性能越出色</p>
<p>显然，这一点是毫无争议的，我们进行特征工程的最终目的就是提升模型的性能。</p>
<h3 id="特征选择-Feature-Selection"><a href="#特征选择-Feature-Selection" class="headerlink" title="特征选择 Feature Selection"></a>特征选择 Feature Selection</h3><p>首先，从特征开始说起，假设你现在有一个标准的Excel表格数据，它的每一行表示的是一个观测样本数据，表格数据中的每一列就是一个特征。在这些特征中，有的特征携带的信息量丰富，有的（或许很少）则属于无关数据(irrelevant data)，我们可以通过特征项和类别项之间的相关性（特征重要性）来衡量。比如，在实际应用中，常用的方法就是使用一些评价指标单独地计算出单个特征跟类别变量之间的关系。如Pearson相关系数，Gini-index（基尼指数），IG（信息增益）等，下面举Pearson指数为例，它的计算方式如下：</p>
 $$r_{xy}^2=(\frac{con(x,y)}{\sqrt{var(x)var(y)}})$$
<p>其中，x 属于 X，X 表一个特征的多个观测值，y 表示这个特征观测值对应的类别列表。</p>
<p>Pearson 相关系数的取值在 0 到 1 之间，如果你使用这个评价指标来计算所有特征和类别标号的相关性，那么得到这些相关性之后，你可以将它们从高到低进行排名，然后选择一个子集作为特征子集（比如top 10%），接着用这些特征进行训练，看看性能如何。此外，你还可以画出不同子集的一个精度图，根据绘制的图形来找出性能最好的一组特征。</p>
<p>这就是特征工程的子问题之一——特征选择，它的目的是从特征集合中挑选一组最具统计意义的特征子集，从而达到降维的效果。</p>
<p>做特征选择的原因是因为这些特征对于目标类别的作用并不是相等的，一些无关的数据需要删掉。做特征选择的方法有多种，上面提到的这种特征子集选择的方法属于 filter（刷选器）方法，它主要侧重于单个特征跟目标变量的相关性。优点是计算时间上较高效,对于过拟合问题也具有较高的鲁棒性。缺点就是倾向于选择冗余的特征,因为他们不考虑特征之间的相关性,有可能某一个特征的分类能力很差，但是它和某些其它特征组合起来会得到不错的效果。另外做特征子集选取的方法还有 wrapper（封装器）和 Embeded(集成方法)。wrapper 方法实质上是一个分类器，封装器用选取的特征子集对样本集进行分类，分类的精度作为衡量特征子集好坏的标准,经过比较选出最好的特征子集。常用的有逐步回归(Stepwise regression)、向前选择(Forward selection)和向后选择(Backward selection)。它的优点是考虑了特征与特征之间的关联性，缺点是：当观测数据较少时容易过拟合，而当特征数量较多时,计算时间又会增长。对于 Embeded 集成方法，它是学习器自身自主选择特征，如使用 Regularization 做特征选择，或者使用决策树思想，细节这里就不做介绍了。这里还提一下，在做实验的时候，我们有时候会用 Random Forest 和 Gradient boosting 做特征选择，本质上都是基于决策树来做的特征选择，只是细节上有些区别。</p>
<h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><p>原则上来讲，特征提取应该在特征选择之前。特征提取的对象是原始数据(raw data)，它的目的是自动地构建新的特征，将原始特征转换为一组具有明显物理意义(Gabor、几何特征[角点、不变量]、纹理[LBP HOG])或者统计意义或核的特征。比如通过变换特征取值来减少原始数据中某个特征的取值个数等。对于表格数据，你可以在你设计的特征矩阵上使用主要成分分析(Principal Component Analysis，PCA)来进行特征提取从而创建新的特征。对于图像数据，可能还包括了线或边缘检测。</p>
<p>常用的方法有：</p>
<ul>
<li>PCA (Principal component analysis，主成分分析)</li>
<li>ICA (Independent component analysis，独立成分分析)</li>
<li>LDA （Linear Discriminant Analysis，线性判别分析）</li>
</ul>
<p>对于图像识别中，还有SIFT方法。</p>
<p>用中文来说就是：特征工程是一个超集，它包括特征提取、特征构建和特征选择这三个子模块。在实践当中，每一个子模块都非常重要，忽略不得。根据答主的经验，他将这三个子模块的重要性进行了一个排名，即：特征构建&gt;特征提取&gt;特征选择。</p>
<p>事实上，真的是这样，如果特征构建做的不好，那么它会直接影响特征提取，进而影响了特征选择，最终影响模型的性能。</p>
<h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><p>我们先说什么是深度学习。其实从整体上来讲，Deep Learning 就是曾经的多层神经网络，整体的思想认为每一个层次都可以被作为一个独立的特征抽象存在，所以最广泛地被用作特征工程上，而 GPU 的存在更是解决了几十年前的 ANN 的训练效率问题。那么简单来说，Deep Learning 可以对抽取出的特征进行非线性组合形成更有效的特征表示。确实，从这一点来说，Deep Learning 确实从理论上很好的解决了机器学习领域很麻烦的“特征抽取”问题，但是在实际的工业界，“特征工程”到底有多复杂？我们看看 Deep Learning 表现最好的 IR 领域吧，曾经是怎么做的呢？据了解微软有个小 Team 专门做的事儿就是从图片上找各种各样的特征，因为算法本身其实已经被锁死在 Random Forest上了，往往特征的微调就能带来算法效果的极大提升，那么 Deep Learning 的出现当然可以很好地取代这项工作(实际效果确实无法得知)，那么总结下Deep Learning的好处：从海量的特征中通过特征工程抽取出有效的特征组合。</p>
<p>但是刨除掉语音和图像领域，转向离我们更近的工作，无论是推荐系统还是数据挖掘，特征是怎么出来的呢？对于一个电影，对于一个用户，满打满算一共就那么多特征，这个时候 Deep Learning 根本无从发挥。那么再退一步说，就算把 User 对于 Item 的标定作为 Item 的特征，由于在实际中大部分的缺失值存在，那么如果你希望用 Deep Learning 来对该矩阵做特征重组，第一件事情就是如何填充缺失值，而这恰恰是比特征工程更困难的事情。</p>
<h2 id="大数据"><a href="#大数据" class="headerlink" title="大数据"></a>大数据</h2><p>大数据其实意味着大样本量，那么大样本量带来的是高置信度以及广覆盖度。例如从 FM 来说，大数据量意味着更全面地了解一个用户的听歌品位，从金融互联网的信用风险评估来说，大数据量意味着不仅仅从消费记录而包含了社交网络信息去对用户做更全面的评价，从用户画像来说意味着建立全面的兴趣图谱和知识图谱，这些都是大数据带给我们的实际意义。说得学术一些，我们不妨认为大数据是频率学派对于贝叶斯学派一次强有力的逆袭。那么既然说到这个份上了，我们不妨思考一下，我们是不是有希望在回归贝叶斯学派，利用先验信息+小数据完成对大数据的反击呢？</p>
<p>另外，既然我们已经说到了大数据的广覆盖度，就针对这个再额外说一下吧。诚然，大数据能够全面地覆盖到所有信息，但是从实际的工业界来看，考虑到实际的计算能力以及效果，大多数公司都会对大数据做“去噪”，那么在去噪的过程中去除的不仅仅是噪音，也包括“异常点”，而这些“异常点”，恰恰把大数据的广覆盖度给降低了，于是利用大数据反而比小数据更容易产生趋同的现象。尤其对于推荐系统来说，这些“异常点”的观察其实才是“个性化”的极致。</p>
<h2 id="用户画像"><a href="#用户画像" class="headerlink" title="用户画像"></a>用户画像</h2><p>任何系统不要脱离产品而存在。先吐个槽，之前在某个公司面试，某个公司上来就问我，你觉得我们的用户画像应该怎么做？这个问题是非常业余的(这个问题就像是有人问我我们网站有性能问题，你说咋办；好吧，这个问题也是这个公司问我的)，任何数据系统都是强产品关联的，这也是太多公司去做数据系统的误区，在这里我还是用户画像为例。 用户画像到底是什么，其实说简单了他就是一个用户宽表，如果偏要我说需要注意的，就是在选择数据库的时候一定要选择列容易扩充的数据库。如果要说具体需要哪些字段，我还真的没法说，我只能把他归类成用户元属性数据，行为统计数据，潜在挖掘数据，至此而已。因为数据系统从来不是一个事先规划好的系统，而是需要随着业务增长来逐渐填充的系统，这也是数据平台难做的原因。 所以我真心无法理解有一些不太大的公司成立了一个部门，这个部门专门做用户画像(例如PPTV)。</p>
<h2 id="实体识别"><a href="#实体识别" class="headerlink" title="实体识别"></a>实体识别</h2><p>命名实体识别(Named EntitiesRecognition, NER)是自然语言处理(Natural LanguageProcessing, NLP)的一个基础任务。其目的是识别语料中人名、地名、组织机构名等命名实体。由于这些命名实体数量不断增加，通常不可能在词典中穷尽列出，且其构成方法具有各自的一些规律性，因而,通常把对这些词的识别从词汇形态处理(如汉语切分)任务中独立处理，称为命名实体识别。命名实体识别技术是信息抽取、信息检索、机器翻译、问答系统等多种自然语言处理技术必不可少的组成部分。</p>
<p>命名实体是命名实体识别的研究主体，一般包括3大类(实体类、时间类和数字类)和7小类(人名、地名、机构名、时间、日期、货币和百分比)命名实体。评判一个命名实体是否被正确识别包括两个方面：实体的边界是否正确；实体的类型是否标注正确。主要错误类型包括文本正确，类型可能错误；反之，文本边界错误,而其包含的主要实体词和词类标记可能正确。</p>
<p>命名实体识别的主要技术方法分为：基于规则和词典的方法、基于统计的方法、二者混合的方法等。</p>
<p>1.基于规则和词典的方法</p>
<p>基于规则的方法多采用语言学专家手工构造规则模板,选用特征包括统计信息、标点符号、关键字、指示词和方向词、位置词(如尾字)、中心词等方法，以模式和字符串相匹配为主要手段，这类系统大多依赖于知识库和词典的建立。基于规则和词典的方法是命名实体识别中最早使用的方法，一般而言，当提取的规则能比较精确地反映语言现象时，基于规则的方法性能要优于基于统计的方法。但是这些规则往往依赖于具体语言、领域和文本风格，编制过程耗时且难以涵盖所有的语言现象，特别容易产生错误，系统可移植性不好，对于不同的系统需要语言学专家重新书写规则。基于规则的方法的另外一个缺点是代价太大，存在系统建设周期长、移植性差而且需要建立不同领域知识库作为辅助以提高系统识别能力等问题。</p>
<p>2.基于统计的方法</p>
<p>基于统计机器学习的方法主要包括：隐马尔可夫模型(HiddenMarkovMode,HMM)、最大熵(MaxmiumEntropy,ME)、支持向量机(Support VectorMachine,SVM)、条件随机场(ConditionalRandom Fields,CRF)等。</p>
<p>在这4种学习方法中，最大熵模型结构紧凑，具有较好的通用性，主要缺点是训练时间复杂性非常高，有时甚至导致训练代价难以承受，另外由于需要明确的归一化计算，导致开销比较大。而条件随机场为命名实体识别提供了一个特征灵活、全局最优的标注框架，但同时存在收敛速度慢、训练时间长的问题。一般说来，最大熵和支持向量机在正确率上要比隐马尔可夫模型高一些，但是隐马尔可夫模型在训练和识别时的速度要快一些，主要是由于在利用Viterbi算法求解命名实体类别序列的效率较高。隐马尔可夫模型更适用于一些对实时性有要求以及像信息检索这样需要处理大量文本的应用,如短文本命名实体识别。</p>
<p>基于统计的方法对特征选取的要求较高，需要从文本中选择对该项任务有影响的各种特征，并将这些特征加入到特征向量中。依据特定命名实体识别所面临的主要困难和所表现出的特性，考虑选择能有效反映该类实体特性的特征集合。主要做法是通过对训练语料所包含的语言信息进行统计和分析，从训练语料中挖掘出特征。有关特征可以分为具体的单词特征、上下文特征、词典及词性特征、停用词特征、核心词特征以及语义特征等。</p>
<p>基于统计的方法对语料库的依赖也比较大，而可以用来建设和评估命名实体识别系统的大规模通用语料库又比较少。</p>
<p>3.混合方法</p>
<p>自然语言处理并不完全是一个随机过程,单独使用基于统计的方法使状态搜索空间非常庞大，必须借助规则知识提前进行过滤修剪处理。目前几乎没有单纯使用统计模型而不使用规则知识的命名实体识别系统，在很多情况下是使用混合方法：</p>
<ol>
<li>统计学习方法之间或内部层叠融合。</li>
<li>规则、词典和机器学习方法之间的融合，其核心是融合方法技术。在基于统计的学习方法中引入部分规则，将机器学习和人工知识结合起来。</li>
<li>将各类模型、算法结合起来，将前一级模型的结果作为下一级的训练数据，并用这些训练数据对模型进行训练，得到下一级模型。</li>
</ol>
<p>这种方法在具体实现过程中需要考虑怎样高效地将两种方法结合起来，采用什么样的融合技术。由于命名实体识别在很大程度上依赖于分类技术,在分类方面可以采用的融合技术主要包括如Voting, XVoting, GradingVa, l Grading等。</p>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>正则化是针对过拟合而提出的，以为在求解模型最优的是一般优化最小的经验风险，现在在该经验风险上加入模型复杂度这一项（正则化项是模型参数向量的范数），并使用一个rate比率来权衡模型复杂度与以往经验风险的权重，如果模型复杂度越高，结构化的经验风险会越大，现在的目标就变为了结构经验风险的最优化，可以防止模型训练过度复杂，有效的降低过拟合的风险。</p>
<p>奥卡姆剃刀原理，能够很好的解释已知数据并且十分简单才是最好的模型。</p>
<p>作用是：</p>
<ol>
<li>数值上更容易求解；</li>
<li>特征数目太大时更稳定；</li>
<li>控制模型的复杂度，光滑性。复杂性越小且越光滑的目标函数泛化能力越强。而加入规则项能使目标函数复杂度减小，且更光滑。</li>
<li>减小参数空间；参数空间越小，复杂度越低。</li>
<li>系数越小，模型越简单，而模型越简单则泛化能力越强（Ng宏观上给出的解释）。</li>
<li>可以看成是权值的高斯先验。</li>
</ol>
<h3 id="L1和L2正则的区别，如何选择L1和L2正则"><a href="#L1和L2正则的区别，如何选择L1和L2正则" class="headerlink" title="L1和L2正则的区别，如何选择L1和L2正则"></a>L1和L2正则的区别，如何选择L1和L2正则</h3><blockquote>
<p>他们都是可以防止过拟合，降低模型复杂度</p>
</blockquote>
<ul>
<li>L1 是在 loss function 后面加上模型参数的1范数（也就是|xi|）</li>
<li>L2 是在 loss function 后面加上模型参数的2范数（也就是<code>sigma(xi^2)</code>），注意L2范数的定义是 <code>sqrt(sigma(xi^2))</code>，在正则项上没有添加 sqrt 根号是为了更加容易优化</li>
<li>L1 会产生稀疏的特征</li>
<li>L2 会产生更多地特征但是都会接近于0</li>
</ul>
<p>L1 会趋向于产生少量的特征，而其他的特征都是 0，而 L2 会选择更多的特征，这些特征都会接近于 0。L1 在特征选择时候非常有用，而L2就只是一种规则化而已。</p>
<h2 id="过拟合-1"><a href="#过拟合-1" class="headerlink" title="过拟合"></a>过拟合</h2><p>一个假设在训练数据上能够获得比其他假设更好的拟合，但是在训练数据外的数据集上却不能很好的拟合数据。此时我们就叫这个假设出现了 overfit 的现象。</p>
<p>过拟合产生的原因：</p>
<p>出现这种现象的主要原因是训练数据中存在噪音或者训练数据太少。</p>
<p>预防或克服措施：</p>
<ol>
<li>增大数据量</li>
<li>减少 feature 个数（人工定义留多少个 feature 或者算法选取这些 feature）</li>
<li>正则化（留下所有的 feature，但对于部分 feature 定义其 parameter 非常小）</li>
<li>交叉验证</li>
</ol>
<h2 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h2><p>对偶性是优化问题中一个非常重要的性质，它能够神奇地将许多非凸的优化问题转化成凸的问题</p>
<p>很多凸优化问题都是通过解对偶问题来求解的，线性规划只是其中一个特例而已。在求解一个规划问题（不限于线性规划）的时候，我们常常需要知道这个问题有没有可行解（有时候约束条件很复杂，不要说最优解，找到可行解都很难），或者是估计一下目前的解离最优解还有多远（大型问题多用迭代解法，如果能大致估计当前的解的质量，就对算法什么时候运行结束有一个大致的把握，如果得到了可接受的近似解也可以提前停止），以及判断原问题的最优解是否无界（万一出现这种情况迭代就停不下来了）。</p>
<p>而对偶问题就是回答这些问题的利器：弱对偶定理给原问题的最优解定了一个界，强对偶定理给出了原问题最优解的一个判定条件。同时，还有很多别的优良性质：例如可以化难为易（把难以求解的约束条件扔到目标函数的位置上去），如果问题的形式合适还可以通过把约束变量和对偶变量互换来把大规模问题转换成小规模问题。线性规划的对偶问题也只是其中的一个特例而已。并且，假如原问题是可行的，还可以给对偶问题找到一个直观解释（经济学中的影子价格）。</p>
<p>举个例子：</p>
 $$min_{x,y} \; px+qy\;\; s.t. x+y \ge 2,x \ge 0,y \ge 0$$
<p>我们令 $a+b = p, a+c=q$ 于是 $px+qy=a(x+y)+bx+cy \ge 2a,a \ge 0, b\ge0, c\ge0$</p>
<p>2a 为元问题的下界，于是我们可以求下面这个问题：</p>
 $$max_{a,b,c}\;2a\;\; s.t.\;a+b=p,a+c=q,a\ge0,b\ge0,c\ge0$$
<p>后一个问题的解正是前一个问题的解，这后面那个问题就叫做原问题的对偶问题。</p>
<h2 id="凸优化"><a href="#凸优化" class="headerlink" title="凸优化"></a>凸优化</h2><p>在机器学习中往往是最终要求解某个函数的最优值，但是一般情况下，任意一个函数的最优值求解比较困难，但是对于凸函数来说就可以有效的求解出全局最优值。</p>
<p><strong>凸集</strong></p>
<p>一个集合 C 是，当前仅当任意 x,y 属于 C 且 0&lt;=theta&lt;=1，都有 $\theta x+(1-\theta)y$ 属于C</p>
<p>用通俗的话来说C集合线段上的任意两点也在C集合中</p>
<p><strong>凸函数</strong></p>
<p>一个函数 f 其定义域(D(f))是凸集，并且对任意 x,y 属于 D(f) 和 0&lt;=theta&lt;=1 都有<br>$f(\theta x+(1-\theta)y)&lt;=\theta f(x)+(1-\theta)f(y)$ —这个叫做jensen不等式</p>
<p>用通俗的话来说就是曲线上任意两点的割线都在曲线的上方</p>
<p><strong>常见的凸函数有</strong></p>
<ul>
<li>指数函数 $f(x)=a^x$ a&gt;1</li>
<li>负对数函数  $-log_a x$ a&gt;1,x&gt;0</li>
<li>开口向上的二次函数等</li>
</ul>
<p><strong>凸函数的判定</strong></p>
<ol>
<li>如果 f 是一阶可导，对于任意数据域内的 x,y 满足 f(y)&gt;=f(x)+f’(x)(y-x)</li>
<li>如果f是二阶可导</li>
</ol>
<p><strong>凸优化应用举例</strong></p>
<ul>
<li>SVM：其中由 max|w| 转向 <code>min(1/2*|w|^2)</code></li>
<li>最小二乘法？</li>
<li>LR的损失函数 <code>sigma(yi*log(hw(x))+(1-yi)*(log(1-hw(x))))</code></li>
</ul>
<h2 id="特征向量"><a href="#特征向量" class="headerlink" title="特征向量"></a>特征向量</h2><h3 id="归一化方法"><a href="#归一化方法" class="headerlink" title="归一化方法"></a>归一化方法</h3><ol>
<li>线性函数转换，表达式如下：<code>y=(x-MinValue)/(MaxValue-MinValue)</code></li>
<li>对数函数转换，表达式如下：<code>y=log10 (x)</code></li>
<li>反余切函数转换 ，表达式如下：<code>y=arctan(x)*2/PI</code></li>
<li>减去均值，乘以方差：<code>y=(x-means)/ variance</code></li>
</ol>
<h3 id="异常值处理"><a href="#异常值处理" class="headerlink" title="异常值处理"></a>异常值处理</h3><p>用均值或者其他统计量代替</p>
<h2 id="ROC、AUC"><a href="#ROC、AUC" class="headerlink" title="ROC、AUC"></a>ROC、AUC</h2><p>ROC和AUC通常是用来评价一个二值分类器的好坏</p>
<h3 id="ROC"><a href="#ROC" class="headerlink" title="ROC"></a>ROC</h3><p>曲线坐标上：</p>
<ul>
<li>X 轴是 FPR（表示假阳率-预测结果为 positive，但是实际结果为 negitive，FP/(N)）</li>
<li>Y 轴式 TPR（表示真阳率-预测结果为 positive，而且的确真实结果也为 positive 的, TP/P）</li>
</ul>
<p>那么平面的上点(X,Y)：</p>
<ul>
<li>(0,1)表示所有的 positive 的样本都预测出来了，分类效果最好</li>
<li>(0,0)表示预测的结果全部为 negitive</li>
<li>(1,0)表示预测的错过全部分错了，分类效果最差</li>
<li>(1,1)表示预测的结果全部为 positive</li>
</ul>
<p>针对落在 x=y 上点，表示是采用随机猜测出来的结果</p>
<h3 id="ROC-曲线建立"><a href="#ROC-曲线建立" class="headerlink" title="ROC 曲线建立"></a>ROC 曲线建立</h3><p>一般默认预测完成之后会有一个概率输出 p，这个概率越高，表示它对 positive 的概率越大。</p>
<p>现在假设我们有一个 threshold，如果 p&gt;threshold，那么该预测结果为 positive，否则为 negative，按照这个思路，我们多设置几个 threshold,那么我们就可以得到多组 positive 和 negative 的结果了，也就是我们可以得到多组 FPR 和 TPR 值将这些 (FPR,TPR) 点投射到坐标上再用线连接起来就是 ROC 曲线了</p>
<p>当threshold取1和0时，分别得到的就是(0,0)和(1,1)这两个点。（threshold=1，预测的样本全部为负样本，threshold=0，预测的样本全部为正样本）</p>
<h3 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h3><p>AUC(Area Under Curve) 被定义为 ROC 曲线下的面积，显然这个面积不会大于1（一般情况下 ROC 会在 x=y 的上方，所以0.5&lt;AUC&lt;1）.</p>
<p>AUC越大说明分类效果越好</p>
<h3 id="为什么要使用ROC和AUC"><a href="#为什么要使用ROC和AUC" class="headerlink" title="为什么要使用ROC和AUC"></a>为什么要使用ROC和AUC</h3><p>因为当测试集中的正负样本发生变化时，ROC 曲线能基本保持不变，但是 precision 和 recall 可能就会有较大的波动。</p>
<h2 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h2><p>在信息论中，熵表示的是不确定性的量度。信息论的创始人香农在其著作《通信的数学理论》中提出了建立在概率统计模型上的信息度量。他把信息定义为”用来消除不确定性的东西“。熵的定义为信息的期望值。</p>
<p>在信息论和概率统计中，熵是表示随机变量不确定性的度量。设X是一个取有限个值的离散随机变量，其概率分布为：</p>
 $$P(X=x_i)=p_i, i=1,2, ... , n$$
<p>则随机变量X的熵定义为：<br> $$H(X)= - \sum p_ilogp_i, i=1,2, ... , n$$</p>
<p>熵只依赖 X 的分布，和 X 的取值没有关系，熵是用来度量不确定性，当熵越大，概率说  $X=x_i$ 的不确定性越大，反之越小，在机器学期中分类中说，熵越大即这个类别的不确定性更大，反之越小。</p>
<p>当 p=0 或 p=1 时，H(p)=0，随机变量完全没有不确定性，当p=0.5时，H(p)=1,此时随机变量的不确定性最大。</p>
<p>条件熵(conditional entropy)：表示在一直随机变量X的条件下随机变量Y的不确定性度量。</p>
<p>设随机变量(X, Y)，其联合概率分布为  $P(X, Y) = p_{ij}(i=1,2, ... , n; j=1,2, ... , m)$，随机变量 X 给定的条件下随机变量Y的条件熵 H(Y|X)，定义为 X 给定条件下 Y 的条件概率分布的熵对 X 的数学期望：</p>
 $$H(Y|X)=\sum p_iH(Y|X=x_i)$$
<p>这里， $p_i=P(X=x_i), i=1,2, ... , n$</p>
<p>熵指的是体系的混乱程度，它在控制论，概率论，数论，天体物理，生命科学等领域都有重要的应用，在不同的学科中也有引申出更为具体的定义，是各个领域十分重要的参量。熵由鲁道夫.克劳修斯提出，并应用在热力学中。后来在，克劳德.埃尔伍德.香农 第一次将熵的概念引入到信息论中来。</p>
<h3 id="信息增益（information-gain）"><a href="#信息增益（information-gain）" class="headerlink" title="信息增益（information gain）"></a>信息增益（information gain）</h3><p>信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。</p>
<p>特征 A 对训练数据集 D 的信息增益 g(D, A)，定义为集合 D 的经验熵 H(D)与特征 A 给定条件下 D 的经验条件熵 H(D|A)之差，即</p>
<p>g(D, A)=H(D)-H(D|A)</p>
<p>信息增益大的特征具有更强的分类能力。</p>
<h3 id="信息增益比（information-gain-ratio）"><a href="#信息增益比（information-gain-ratio）" class="headerlink" title="信息增益比（information gain ratio）"></a>信息增益比（information gain ratio）</h3><p>信息增益比  $g_R(D, A)$定义为其信息增益 g(D, A) 与训练数据集 D 关于特征 A 的值的熵  $H_A(D)$之比，即</p>
 $g_R(D, A)=g(D, A)/H_A(D)$
<p>其中， $H_A(D)=-\sum \frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$, n 是特征 A 取值的个数。</p>
<h3 id="基尼指数（gini-index）"><a href="#基尼指数（gini-index）" class="headerlink" title="基尼指数（gini index）"></a>基尼指数（gini index）</h3><p>分类问题中，假设有 K 个类，样本属于第 k 类的概率为 $p_k$，则概率分布的基尼指数定义为：</p>
 $$Gini(p)=\sum p_k(1-p_k)=1-\sum p_k^2$$
<p>对于二分类问题，若样本点属于第 1 个类的概率是 p，则概率分布的基尼指数为：</p>
<p>Gini(p)=2p(1-p)</p>
<p>对于给定的样本集合D，其基尼指数为：</p>
 $$Gini(D)=1-\sum(\frac{|C_k|}{|D|})^2$$
<p>这里， $C_k$是 D 中属于第 k 类的样本子集，k 是类的个数。</p>
<p>如果样本集合 D 根据特征 A 是否取到某一可能值 a 被分割成 D1 和 D2 两部分，则在特征 A 的条件下，集合 D 的基尼指数定义为：</p>
 $$Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$$
<p>基尼指数 Gini(D) 表示集合 D 的不确定性，基尼指数越大，样本集合的不确定性也就越大，这一点与熵相似。</p>

    
  </div>


          </div>
          

  <p>热评文章</p>
  <div class="ds-top-threads" data-range="weekly" data-num-items="4"></div>


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="vault/machine-learning-guide.html"
           data-title="机器学习指南" data-url="http://wdxtub.com/vault/machine-learning-guide.html">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/misc/avatar.jpg"
               alt="wdxtub" />
          <p class="site-author-name" itemprop="name">wdxtub</p>
          <p class="site-description motion-element" itemprop="description">人文/科学/读书/写作/思考/编程/架构/数据/广交朋友/@SYSU/@CMU</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">710</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">分类</span>
              
            </div>
          

          
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">874</span>
                <span class="site-state-item-name">标签</span>
              
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/wdxtub" target="_blank" title="GitHub">
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/wdxtub" target="_blank" title="微博">
                  
                  微博
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://douban.com/people/wdx" target="_blank" title="豆瓣">
                  
                  豆瓣
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/wdxtub" target="_blank" title="知乎">
                  
                  知乎
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              不妨看看
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://zhchbin.github.io/" title="zhchbin" target="_blank">zhchbin</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.algorithmdog.com/" title="算法狗" target="_blank">算法狗</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.52cs.org/" title="我爱计算机" target="_blank">我爱计算机</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://jackqdyulei.github.io/" title="雷雷" target="_blank">雷雷</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://guojiex.github.io/" title="瓜瓜" target="_blank">瓜瓜</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://wdxtub.lofter.com/" title="我的 Lofter" target="_blank">我的 Lofter</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://wdxtub.com/interview/" title="刷题笔记" target="_blank">刷题笔记</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2013 - 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wdxtub</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

<div class="busuanzi-count">

  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv"><i class="fa fa-user"></i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span></span>
  

  
    <span class="site-pv"><i class="fa fa-eye"></i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span>
  
  
</div>



        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"wdxblog"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementById('footer')
      || document.getElementById('footer')).appendChild(ds);
    })();
  </script>

  
    
      
      <script src="/vendors/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
      <script src="/js/src/hook-duoshuo.js"></script>
    
  





  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
       search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();

    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
    'use strict';
    $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
            // get the contents from search data
            isfetched = true;
            $('.popup').detach().appendTo('.header-inner');
            var datas = $( "entry", xmlResponse ).map(function() {
                return {
                    title: $( "title", this ).text(),
                    content: $("content",this).text(),
                    url: $( "url" , this).text()
                };
            }).get();
            var $input = document.getElementById(search_id);
            var $resultContent = document.getElementById(content_id);
            $input.addEventListener('input', function(){
                var matchcounts = 0;
                var str='<ul class=\"search-result-list\">';                
                var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                $resultContent.innerHTML = "";
                if (this.value.trim().length > 1) {
                // perform local searching
                datas.forEach(function(data) {
                    var isMatch = true;
                    var content_index = [];
                    var data_title = data.title.trim().toLowerCase();
                    var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                    var data_url = data.url;
                    var index_title = -1;
                    var index_content = -1;
                    var first_occur = -1;
                    // only match artiles with not empty titles and contents
                    if(data_title != '' && data_content != '') {
                        keywords.forEach(function(keyword, i) {
                            index_title = data_title.indexOf(keyword);
                            index_content = data_content.indexOf(keyword);
                            if( index_title < 0 && index_content < 0 ){
                                isMatch = false;
                            } else {
                                if (index_content < 0) {
                                    index_content = 0;
                                }
                                if (i == 0) {
                                    first_occur = index_content;
                                }
                            }
                        });
                    }
                    // show search results
                    if (isMatch) {
                        matchcounts += 1;
                        str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                        var content = data.content.trim().replace(/<[^>]+>/g,"");
                        if (first_occur >= 0) {
                            // cut out 100 characters
                            var start = first_occur - 20;
                            var end = first_occur + 80;
                            if(start < 0){
                                start = 0;
                            }
                            if(start == 0){
                                end = 50;
                            }
                            if(end > content.length){
                                end = content.length;
                            }
                            var match_content = content.substring(start, end);
                            // highlight all keywords
                            keywords.forEach(function(keyword){
                                var regS = new RegExp(keyword, "gi");
                                match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                            });
                            
                            str += "<p class=\"search-result\">" + match_content +"...</p>"
                        }
                        str += "</li>";
                    }
                })};
                str += "</ul>";
                if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
                if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
                $resultContent.innerHTML = str;
            });
            proceedsearch();
        }
    });}

    // handle and trigger popup window;
    $('.popup-trigger').mousedown(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };

    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src=""></script>


  

  
<script type="text/javascript" async src="//push.zhanzhang.baidu.com/push.js">
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


</body>
</html>
