<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content=",,,," />





  <link rel="alternate" href="/atom.xml" title="小土刀" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="本来打算借着当助教，看看这门课有没有什么变化。事实上，没啥变化，所以这里先把当时我的笔记放出来，之后的课程可能会主要集中于代码和项目的思路分析，具体理论的东西不会再重复太多。">
<meta name="keywords">
<meta property="og:type" content="website">
<meta property="og:title" content="How to Write Fast Code 第 0 课 往年笔记与问题集">
<meta property="og:url" content="http://wdxtub.com/vault/fastcode-0.html">
<meta property="og:site_name" content="小土刀">
<meta property="og:description" content="本来打算借着当助教，看看这门课有没有什么变化。事实上，没啥变化，所以这里先把当时我的笔记放出来，之后的课程可能会主要集中于代码和项目的思路分析，具体理论的东西不会再重复太多。">
<meta property="og:image" content="http://wdxtub.com/images/14543391583932.jpg">
<meta property="og:image" content="http://wdxtub.com/images/14543391502961.jpg">
<meta property="og:image" content="http://wdxtub.com/images/14543391400653.jpg">
<meta property="og:image" content="http://wdxtub.com/images/14543391286447.jpg">
<meta property="og:image" content="http://wdxtub.com/images/14543392317490.jpg">
<meta property="og:updated_time" content="2016-02-01T15:19:50.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="How to Write Fast Code 第 0 课 往年笔记与问题集">
<meta name="twitter:description" content="本来打算借着当助教，看看这门课有没有什么变化。事实上，没啥变化，所以这里先把当时我的笔记放出来，之后的课程可能会主要集中于代码和项目的思路分析，具体理论的东西不会再重复太多。">
<meta name="twitter:image" content="http://wdxtub.com/images/14543391583932.jpg">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 4016951,
      author: '博主'
    }
  };
</script>

  <title>
  

  
    How to Write Fast Code 第 0 课 往年笔记与问题集 | 小土刀
  
</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  





  <!-- hexo-inject:begin --><!-- hexo-inject:end --><script type="text/javascript">
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=59042340";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <div style="display: none;">
    <script src="http://s6.cnzz.com/stat.php?id=1260625611&web_id=1260625611" type="text/javascript"></script>
  </div>





  
  
    
  

  <div class="container one-collumn sidebar-position-left  ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">小土刀</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Agony is my triumph</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-work">
          <a href="/2016/09/11/work-page" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-pencil"></i> <br />
            
            作品
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tech">
          <a href="/2009/09/11/tech-page" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-battery-full"></i> <br />
            
            技术
          </a>
        </li>
      
        
        <li class="menu-item menu-item-life">
          <a href="/1990/09/11/life-page" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-bolt"></i> <br />
            
            生活
          </a>
        </li>
      
        
        <li class="menu-item menu-item-booklist">
          <a href="/1997/09/11/booklist-page" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-diamond"></i> <br />
            
            书单
          </a>
        </li>
      
        
        <li class="menu-item menu-item-thanks">
          <a href="/thanks" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-gift"></i> <br />
            
            关于我
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="#" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    
    
      <p>本来打算借着当助教，看看这门课有没有什么变化。事实上，没啥变化，所以这里先把当时我的笔记放出来，之后的课程可能会主要集中于代码和项目的思路分析，具体理论的东西不会再重复太多。</p>
<a id="more"></a>
<hr>
<h1 id="Lecture-Note"><a href="#Lecture-Note" class="headerlink" title="Lecture Note"></a>Lecture Note</h1><ul>
<li>Fast Platforms (Multicore platforms, Manycore platforms, Cloud platform) + Good Techniques (Data structure, Algorithm, Software Architecture)</li>
<li>Need is driven by the applications, NOT by the availability of the platform.</li>
<li>Background -&gt; Multicore(openmp) -&gt; Manycore(CUDA) -&gt; cluster(Hadoop) -&gt; Special Topics</li>
</ul>
<h2 id="Multicore-vs-Manycore"><a href="#Multicore-vs-Manycore" class="headerlink" title="Multicore vs Manycore"></a>Multicore vs Manycore</h2><ul>
<li>Multicore: yoke of oxen. Each core optimized for executing a single thread.</li>
<li>Manycore: flock of chickens. Cores optimized for aggregate throughput, deemphasizing individual performance.</li>
</ul>
<h2 id="Instruction-Level-Parallelism-ILP"><a href="#Instruction-Level-Parallelism-ILP" class="headerlink" title="Instruction Level Parallelism (ILP)"></a>Instruction Level Parallelism (ILP)</h2><p>Instructions in a sequence that can be computed at the same time.</p>
<ul>
<li>Advantages<ul>
<li>No changes in sequential software necessary</li>
</ul>
</li>
<li>Disadvantages<ul>
<li>Significantly more complex processor architecture</li>
<li>Longer to design the processor</li>
<li>Longer to verify the correctness of the processor design</li>
<li>Consumes more energy than simple in-order processor</li>
</ul>
</li>
</ul>
<h2 id="Out-of-order-Pipelines"><a href="#Out-of-order-Pipelines" class="headerlink" title="Out-of-order Pipelines"></a>Out-of-order Pipelines</h2><p>Allows instruction re-ordering, register-renaming</p>
<h2 id="SIMD"><a href="#SIMD" class="headerlink" title="SIMD"></a>SIMD</h2><ul>
<li>can be area and power efficient</li>
<li>parallelism exposed to programmer &amp; compiler</li>
</ul>
<p>Locality, Temporal Locality, Spatial Locality</p>
<p>Compulsory misses, Capacity misses, Conflict misses</p>
<ul>
<li>Advantages<ul>
<li>Power-efficient wya to improve instruction throughput</li>
<li>Exploitable in many compute-intensive applications</li>
</ul>
</li>
<li>Disadvantages<ul>
<li>Explicit representation in vector instructions</li>
<li>Software requires re-compilation to take advantage of new SIMD capabilites.</li>
<li>May require hand-tuning to expoit full benefit</li>
</ul>
</li>
</ul>
<h2 id="Simultaneous-multithreading"><a href="#Simultaneous-multithreading" class="headerlink" title="Simultaneous multithreading"></a>Simultaneous multithreading</h2><p>Capturing the opportunity to run faster when more than one thread of instructions are available.</p>
<ul>
<li>Advantages<ul>
<li>Gain power-efficiency by increase processor pipeline utilization</li>
</ul>
</li>
<li>Disadvantages<ul>
<li>Requires multiple threads available</li>
<li>May trigger confilicts in shared cache during execution</li>
<li>Does not improve latency of each thread</li>
</ul>
</li>
</ul>
<h2 id="Concurrency-vs-Parallelism"><a href="#Concurrency-vs-Parallelism" class="headerlink" title="Concurrency vs Parallelism"></a>Concurrency vs Parallelism</h2><ul>
<li>Concurrency: We expose concurrency in our application</li>
<li>Parallelism: We exploit parallelism in our platform</li>
</ul>
<p>Concurrency: A sequence of instructions executes concurrently if they execute independent of each other as if they were executed at the same time.</p>
<ul>
<li>They do not need to be executed truly at the same time though.</li>
<li>On a single processor computer, multi-tasking systems execute programs concurrently by interleaving their operations such that they appear to execute at the same time.</li>
</ul>
<p>Parallelism: Instruction streams that execute in parallel actually execute at the same time</p>
<ul>
<li>Parallelism allows multiple instructions to be executed at the exact same time</li>
<li>Parallelism requires multiple processing units, ranging from small pipeline stages up through multithreaded architectures and multicore and multiprocessing systems</li>
</ul>
<p>Difference</p>
<ul>
<li>Time</li>
<li>In concurrency, at any given time, a single operation is occurring.<ul>
<li>High clock rates and clever interleaving can give the illusion of parallelism</li>
<li>All modern desktop/server OS give you this. Embedded, maybe not.</li>
</ul>
</li>
<li>In parallelism, at a given point in time, multiple operations are occurring.<ul>
<li>This is important to distinguish. Parallelism means it is extremely difficult (often impossible) to predict the interleaving of instructions.</li>
</ul>
</li>
</ul>
<h2 id="The-process-of-problem-solving"><a href="#The-process-of-problem-solving" class="headerlink" title="The process of problem solving:"></a>The process of problem solving:</h2><ul>
<li>Understand the current state<ul>
<li>Running on a platform</li>
<li>Using a specific set of resources</li>
<li>Achieving a specific performance</li>
<li>Meeting a specific criteria/requirement</li>
</ul>
</li>
<li>Observe the internal representation<ul>
<li>Application structure</li>
<li>Implementation concerns<ul>
<li>Task considerations</li>
<li>Data representations</li>
<li>concurrency opportunities</li>
</ul>
</li>
</ul>
</li>
<li>Search among alternatives</li>
<li>Select from a set of choices</li>
</ul>
<h2 id="Kmeans-Problem"><a href="#Kmeans-Problem" class="headerlink" title="Kmeans Problem"></a>Kmeans Problem</h2><ul>
<li>Find K cluster centers that minimize the distance from each data point to a cluster center (centroid)</li>
<li>Important algorithm in machine learning</li>
<li>NP-hard for arbitrary input</li>
<li>Issues<ul>
<li>Worst case running time is super-polynomial</li>
<li>Approximation can be arbitrarily bad</li>
</ul>
</li>
</ul>
<h2 id="How-to-write-fast-code"><a href="#How-to-write-fast-code" class="headerlink" title="How to write fast code"></a>How to write fast code</h2><ul>
<li><strong>Expose</strong> concurrencies in applications and algorithms</li>
<li><strong>Exploit</strong> parallelisms on application platform</li>
<li><strong>Explore</strong> mapping between concurrency and parallelism</li>
</ul>
<h2 id="The-phases-kmeans"><a href="#The-phases-kmeans" class="headerlink" title="The phases(kmeans)"></a>The phases(kmeans)</h2><ul>
<li>Initialization: Randomly select k cluster centers<ul>
<li>Select k samples from data as initial centers [Forgy Partition]</li>
</ul>
</li>
<li>Expectation: Assign each data point go closest center<ul>
<li>Compare each data point (N) to each cluster center (K)</li>
<li>Distance Metric: Euclidean distance (D dimensions)</li>
</ul>
</li>
<li>Maximization: Update centers based on assignments</li>
<li>Evaluate: Re-iterate steps 2-3 until convergence or stopping criteria.</li>
</ul>
<h2 id="Performance-Analysis-Roofline-Model"><a href="#Performance-Analysis-Roofline-Model" class="headerlink" title="Performance Analysis: Roofline Model"></a>Performance Analysis: Roofline Model</h2><ul>
<li>Observe the phases of execution</li>
<li>Characterize the execution time break downs</li>
<li>Reason about why a piece of code is slow</li>
<li>Identify performance bottleneck</li>
</ul>
<h2 id="How-to-evaluate-a-mapping"><a href="#How-to-evaluate-a-mapping" class="headerlink" title="How to evaluate a mapping"></a>How to evaluate a mapping</h2><ul>
<li>Efficiency: Runs quickly, makes good use of computational resources</li>
<li>Simplicity: Easy to understand code is easier to develop, debug, verify and modify</li>
<li>Portability: Should run on widest range of parallel computers</li>
<li>Scalability: Should be effective on a wide range of processing elements</li>
</ul>
<h2 id="Exploiting-Different-Levels-of-Parallelism"><a href="#Exploiting-Different-Levels-of-Parallelism" class="headerlink" title="Exploiting Different Levels of Parallelism"></a>Exploiting Different Levels of Parallelism</h2><ul>
<li>SIMD-Level: using vectorizing compiler and hand-code intrinsics</li>
<li>SMT-Level: OS abstract it to core-level parallelism</li>
<li>Core-Level: Using threads to describe work done on different cores</li>
</ul>
<h2 id="False-Sharing"><a href="#False-Sharing" class="headerlink" title="False Sharing"></a>False Sharing</h2><ul>
<li>Cache loads and stores work with 4-16 word long cache lines(64B for Intel)<ul>
<li>If two threads are wrting to the same cache line, conflicts occurs</li>
</ul>
</li>
<li>Even if the address differs, one will still suffer performance penalty</li>
</ul>
<h2 id="Optimization-Categorization"><a href="#Optimization-Categorization" class="headerlink" title="Optimization Categorization"></a>Optimization Categorization</h2><ul>
<li>Maximizing In-core Performance<ul>
<li>Exploit in-core parallelism (reorder, unroll, SIMD, eliminate branch)</li>
</ul>
</li>
<li>Maximizing Memory Bandwidth<ul>
<li>Exploit NUMA, Hide memory latency (unit-stride streams, memory affinity, sw prefetch, DMA Lists, TLB Blocking)</li>
</ul>
</li>
<li>Minimizing Memory Traffic<ul>
<li>Eliminate Capacity/Conflict/Compulsory misses (cache blocking, array padding, compress data, streaming stores)</li>
</ul>
</li>
</ul>
<h2 id="Measuring-Arithmetic-Intensity"><a href="#Measuring-Arithmetic-Intensity" class="headerlink" title="Measuring Arithmetic Intensity"></a>Measuring Arithmetic Intensity</h2><p>Arithmetic Intensity = (# of FP Operations to run the program) / (# of Bytes Accessed in the Main Memory)</p>
<p>Arithmetic Intensity = FLOPs / (Allocations + Compulsory + Conflict + Capacity)</p>
<h2 id="Roofline-Model"><a href="#Roofline-Model" class="headerlink" title="Roofline Model"></a>Roofline Model</h2><p>Attainable Performance(ij) = min(FLOP/s with Optimization(1-i), AI*Bandwidth with Optimization(1-j)</p>
<p>Plot on log-log scale</p>
<p>Lantency -&gt; Runtime</p>
<p>Throughput -&gt; Performance</p>
<ul>
<li>Throughput<ul>
<li>Usually measured in floating point operations per second(FLOPS)</li>
<li>Floating point operations = addition + multiplication</li>
</ul>
</li>
</ul>
<p>Higher Performance != Shorter Runtime</p>
<p>True Arithmetic Intensity (AI) ~ Total Flops / Total DRAM Bytes</p>
<ul>
<li>Arithmetic intensity is ultimately limited by compulsory traffic</li>
<li>Arithmetic intensity is diminished by conflict or capacity misses</li>
</ul>
<h2 id="Data-Three-Classes-of-Locality"><a href="#Data-Three-Classes-of-Locality" class="headerlink" title="Data: Three Classes of Locality"></a>Data: Three Classes of Locality</h2><ul>
<li>Spatial Locality<ul>
<li>data is transferred from cache to registers in words</li>
<li>However, data is transferred to the cache in 64-128 Byte lines</li>
<li>using every word in a line maximizes spatial locality</li>
<li>transform data structures into struct of arrays(SoA) layout</li>
</ul>
</li>
<li>Temporal Locality<ul>
<li>reusing data(either registers or cachelines) multiple times</li>
<li>amortizes the impact of limited bandwidth</li>
<li>transform loop s or algorithms to maximize reuese.</li>
</ul>
</li>
<li>Sequential Locality<ul>
<li>Many memory address patterns access cache lines sequentially</li>
<li>CPU’s hardware stream prefetchers exploit this observation to hide speculatively load data to memory lantency</li>
<li>Tansform loops to generate long, unit-stride accesses</li>
</ul>
</li>
</ul>
<h2 id="GPU-is-an-Accelerator"><a href="#GPU-is-an-Accelerator" class="headerlink" title="GPU is an Accelerator"></a>GPU is an Accelerator</h2><ul>
<li>Host System (CPU) &lt;—&gt; Device System (GPU)</li>
<li>When Does Using GPU Make Sense?<ul>
<li>Application with a lot of concurrency (1000-way, fine-grained concurrency)</li>
<li>Some memory intensive applications (Aggregate memory bandwidth is higher)</li>
<li>Advantage diminishes when task granularity becomes too large to fit in shared memory</li>
</ul>
</li>
</ul>
<h2 id="GPU-Programming-Model-Stream"><a href="#GPU-Programming-Model-Stream" class="headerlink" title="GPU Programming Model: Stream"></a>GPU Programming Model: Stream</h2><ul>
<li>Stream -&gt; kernel -&gt; stream</li>
<li>Streams: An array of data units</li>
<li>Kernels<ul>
<li>Take streams as input, produce streams at output</li>
<li>Perform computation on streams</li>
<li>Kernels can be linked together</li>
</ul>
</li>
</ul>
<h2 id="CUDA-Compute-Unified-Device-Architecture"><a href="#CUDA-Compute-Unified-Device-Architecture" class="headerlink" title="CUDA: Compute Unified Device Architecture"></a>CUDA: Compute Unified Device Architecture</h2><ul>
<li>Integrated host + device app C program</li>
<li>Serial or modestly parallel parts in host C code</li>
<li>Highly Parallel parts in device SPMP kernel C code</li>
</ul>
<h2 id="CUDA-Programming-Model"><a href="#CUDA-Programming-Model" class="headerlink" title="CUDA Programming Model"></a>CUDA Programming Model</h2><ul>
<li>Executing kernel functions within threads</li>
<li>Threads organization<ul>
<li>Blocks and Grids</li>
</ul>
</li>
<li>Hardware mapping of threads<ul>
<li>Computation-to-core mapping<ul>
<li>Thread -&gt; Core</li>
<li>Thread blocks -&gt; Multi-processors</li>
</ul>
</li>
</ul>
</li>
<li>Thread organization<ul>
<li>an array of threads -&gt; block</li>
<li>an array of blocks -&gt; grid</li>
</ul>
</li>
<li>All threads in one grid execute the same kernel</li>
<li>Grids are executed sequentially</li>
<li>Thread Cooperation<ul>
<li>Threads within a block<ul>
<li>Shared memory</li>
<li>Atomic operation on Share memory &amp; global memory</li>
<li>Barrier</li>
</ul>
</li>
<li>Threads between blocks<ul>
<li>Atomic operation on global memory</li>
</ul>
</li>
<li>Threads between grids<ul>
<li>NO WAY!</li>
</ul>
</li>
</ul>
</li>
<li>Thread Mapping and Scheduling<ul>
<li>A grid of threads takes over the whole device</li>
<li>A block of threads is mapped on one multi-processor<ul>
<li>A multi-processor can take more than one blocks.(Occupancy)</li>
<li>A block can not be preempted until finish</li>
</ul>
</li>
<li>Threads within a blocks are shceduled to run on multi-processors</li>
<li>Threads are grouped into warps(32) as scheduling units</li>
</ul>
</li>
<li>Parallel Memory Sharing<ul>
<li>Local Memory: per-thread</li>
<li>Shared Memory: per-Block</li>
<li>Global Memory: per-application</li>
</ul>
</li>
<li>Shared Memory<ul>
<li>Each Multi-processor has 32KB of Shared Memory - 32 banks of 32bit words</li>
<li>Visible to all threads in a thread block</li>
</ul>
</li>
</ul>
<h2 id="Why-Warps"><a href="#Why-Warps" class="headerlink" title="Why Warps"></a>Why Warps</h2><p>Each Fermi core can maintain 48 warps of architecural context.</p>
<p>Each warp manages a 32-wide SIMD vector worth of computation</p>
<p>With ~20 registers for each trhead:</p>
<p>4(Bytes/register) x 20(Registers) x 32(SIMD lanes) x 48 (Warps) = 128KB per core x 16 (core) = 2MB total of register files</p>
<ul>
<li>Software abstract info hid an extra level of architecture complexity</li>
<li>A 128KB register file is a large memory (takes more than one cycle)</li>
<li>Hardware provide 160wide physical SIMD units, half-pump register files</li>
<li>To simplify the programming model</li>
</ul>
<h2 id="How-to-Deal-with-GPUs-of-Different-Sizes"><a href="#How-to-Deal-with-GPUs-of-Different-Sizes" class="headerlink" title="How to Deal with GPUs of Different Sizes?"></a>How to Deal with GPUs of Different Sizes?</h2><ul>
<li>CUDA provides an abstract infor concurrency to be fully exposed</li>
<li>HW/Runtime provides capability to schedule the computation</li>
</ul>
<h2 id="Thread-Blocks"><a href="#Thread-Blocks" class="headerlink" title="Thread Blocks"></a>Thread Blocks</h2><ul>
<li>Computation is grouped into blocks of independent concurrently execrable work</li>
<li>Fully exposed the concurrency in the application</li>
<li>The HW/Runtime makes the decision to selectively sequentialize the execution as necessary</li>
</ul>
<h2 id="Threads"><a href="#Threads" class="headerlink" title="!! Threads"></a>!! Threads</h2><ul>
<li>Threads are the computation performed in each SIMD lane in a core<ul>
<li>CUDA provides a SIMT programming abstraction to assist users</li>
</ul>
</li>
<li>SIMT: Single Instruction Multiple Threads<ul>
<li>A single instruction multiple processing elements</li>
<li>Different from SIMD</li>
<li>SIMT abstract the # threads in a thread block as a user-specified parameter</li>
</ul>
</li>
<li>SIMT enables programmers to write thread-level parallel code for<ul>
<li>Independent, Scalar threads</li>
<li>Data-parallel code fro coordinated threads</li>
</ul>
</li>
<li>For function correctness, programmers can ignore SIMT behavior</li>
<li>For performance, programmer can tune applications with SIMT in mind</li>
</ul>
<h2 id="About-Data"><a href="#About-Data" class="headerlink" title="About Data"></a>About Data</h2><ul>
<li>SIMD style programming can be very restrictive for communication between SIMD lanes.</li>
<li>On the same chip, in the same core, computations in SMD lanes (physically) takes places very close to each other</li>
</ul>
<h2 id="Shared-Memory-L1-cache"><a href="#Shared-Memory-L1-cache" class="headerlink" title="Shared Memory/L1 cache"></a>Shared Memory/L1 cache</h2><ul>
<li>Manycore processors provide memory local to each core</li>
<li>Computations in SIMD-lanes in the same core can communicate via memory read / write</li>
<li>Two types of memory:<ul>
<li>Programmer-managed scratch pad memory</li>
<li>HW-managed L1 cache</li>
</ul>
</li>
<li>For NVIDIA Fermi architecture, you get 64KB per core with 2 configurations:<ul>
<li>48KB scratch pad (Shared Memory), 16KB L1 cache</li>
<li>16KB scratch pad (Shared Memory), 48KB L1 cache</li>
</ul>
</li>
<li>How many Threads per Thread Block<ul>
<li>In Fermi, 48 warps of context are maintained per core</li>
<li>In Fermi, each thread block can have up to 1024 threads</li>
</ul>
</li>
</ul>
<h2 id="Synchronization"><a href="#Synchronization" class="headerlink" title="Synchronization"></a>Synchronization</h2><ul>
<li><code>__syncthreads()</code><ul>
<li>waits until all threads in the thread block have reached this point and all global and shared memory accesses made by these threads prior to <code>__syncthreads()</code> are visible to all threads in the block</li>
<li>used to coordinate communication between the threads of the same block</li>
</ul>
</li>
</ul>
<h2 id="Compilation"><a href="#Compilation" class="headerlink" title="Compilation"></a>Compilation</h2><ul>
<li>Any source file containing CUDA language extensions must be compiled with NVCC<ul>
<li>NVCC is a compiler driver</li>
<li>Works by invoking all the necessary tools and compilers like cudacc, g++, …</li>
</ul>
</li>
<li>NVCC outputs<ul>
<li>C code (host CPU code)<ul>
<li>Must be compiled with the rest of the application using another tool</li>
</ul>
</li>
<li>PTX<ul>
<li>object code directly</li>
<li>or, PTX source, interpreted at runtime</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="SOA-vs-AOS"><a href="#SOA-vs-AOS" class="headerlink" title="SOA vs AOS"></a>SOA vs AOS</h2><p>Struct of Arrays: 一共只有一个 struct</p>
<pre><code>typedef struct
{
    float* x;
    float* y;
    float* z;
} Constraints;
</code></pre><p>x | x | x | y | y | y | z | z | z</p>
<p>Array of Struct</p>
<pre><code>typedef struct __align__(16)
{
    float3 position;
} Constraint;
</code></pre><p>x | y | z | x | y | z | x | y | z</p>
<p>It depends on the usage of the data.</p>
<p>Note that AoS pads within each struct. While SoA pads between the arrays.</p>
<p>These have the following trade-offs:</p>
<ol>
<li>AoS tends to be more readable to the programmer as each “object” is kept together.</li>
<li>AoS may have better cache locality if all the members of the struct are accessed together.</li>
<li>SoA could potentially be more efficient since grouping same datatypes together sometimes exposes vectorization.</li>
<li>In many cases SoA uses less memory because padding is only between arrays rather than between every struct.</li>
</ol>
<h2 id="Optimization-Strategies"><a href="#Optimization-Strategies" class="headerlink" title="Optimization Strategies"></a>Optimization Strategies</h2><ul>
<li>Global Memory Access Pattern -&gt; Coalescing</li>
<li>Control Flow -&gt; Divergent branch</li>
</ul>
<h2 id="Memory-Coalescing"><a href="#Memory-Coalescing" class="headerlink" title="Memory Coalescing"></a>Memory Coalescing</h2><ul>
<li>Hardware Constraint: DRAM is accessed in ‘segments’ of 32B/64B/128B</li>
<li>Goal: combine multiple memory accesses generated from multiple threads into a single physical transaction</li>
<li>Rules for maximizing DRAM memory bandwidth:<ul>
<li>Possible bus transaction sizes: 32B, 64B, or 128B</li>
<li>Memory segment must be aligned: First address = multiple of segment</li>
<li>Hardware coalescing fro each half-warp: 16-word wide</li>
</ul>
</li>
</ul>
<p>Threads can access any words in any order, including the same words, and a single memory transaction for each segment addressed by a half-warp.</p>
<p>核心想法就是一次载入，尽量多次使用，减少访问次数。</p>
<h2 id="Use-of-Shared-Memory"><a href="#Use-of-Shared-Memory" class="headerlink" title="Use of Shared Memory"></a>Use of Shared Memory</h2><ul>
<li>Process:<ul>
<li>Load from DRAM to shared memory</li>
<li>Synchronize</li>
<li>Perform work on data in shared memory</li>
<li>Synchronize</li>
<li>Write out results to DRAM</li>
</ul>
</li>
</ul>
<p>Trick: Double Buffering</p>
<p>先载入到 global memory 再折腾到 shared memory</p>
<p>Declared a fixed sized variable at compile time</p>
<pre><code>__shared__ float As[BLOCK_SIZE][BLOCK_SIZE];
</code></pre><p>Define a size to be used at run time</p>
<pre><code>mykernel &lt;&lt;&lt;nBloks, nThds, shmemByteSize&gt;&gt;&gt;(a, objects);
在 kernel 函数中也需要进一步处理
</code></pre><h2 id="Memory-Bank-Conflicts"><a href="#Memory-Bank-Conflicts" class="headerlink" title="Memory Bank Conflicts"></a>Memory Bank Conflicts</h2><ul>
<li>Shared memory has 32 banks<ul>
<li>Organized such that successive 32-bit words are assigned to successive banks</li>
<li>Each bank has a bandwidth of 32 bits per two clock cycles (2 cycle latency)</li>
</ul>
</li>
</ul>
<p>A bank conflict occurs if two or more threads access any bytes within different 32-bit words belonging to the same bank.</p>
<p>如果访问的是同一个 bank 的同一个数据，那么多少个线程一起访问也不 conflict</p>
<h2 id="Padding-Technique"><a href="#Padding-Technique" class="headerlink" title="Padding Technique"></a>Padding Technique</h2><p>矩阵的那个如果不是整数可以考虑 padding</p>
<h2 id="Branch-divergence"><a href="#Branch-divergence" class="headerlink" title="Branch divergence"></a>Branch divergence</h2><p>Optimization: Factor out decision variables to have shorter sequence of divergent code</p>
<p>Branch divergence occurs only within a warp</p>
<h2 id="Optimizing-Instruction-Mix"><a href="#Optimizing-Instruction-Mix" class="headerlink" title="Optimizing Instruction Mix"></a>Optimizing Instruction Mix</h2><ul>
<li>Compiler Assisted Loop Unrolling<ul>
<li>Provides more instruction level parallelism for the compiler to use</li>
<li>Improves the ability for the compiler to find the instruction mix that instructions executed per cycle(IPC)</li>
</ul>
</li>
<li>By default, the compiler unrolls small loops with a know trip count</li>
<li>In CUDA, <code>#pragma unroll</code> directive can control unrolling of any given loop<ul>
<li>Must be placed immediately before the loop and only applies to that loop</li>
<li>Optionally followed by a number</li>
</ul>
</li>
</ul>
<h3 id="Device-only-CUDA-intrinsic-function"><a href="#Device-only-CUDA-intrinsic-function" class="headerlink" title="Device-only CUDA intrinsic function"></a>Device-only CUDA intrinsic function</h3><p>常用的数学计算有 gpu 版本替代</p>
<h2 id="Data-Parallel-Algorithms-Map"><a href="#Data-Parallel-Algorithms-Map" class="headerlink" title="Data Parallel Algorithms - Map"></a>Data Parallel Algorithms - Map</h2><p>Map: A fucntion that applies a given function to each element of a list , and returning a list of results</p>
<p>Two important properties:</p>
<ul>
<li>Side-effect free: Only returning a value, no modifications of state with the rest of the application</li>
<li>Independent: Has an independent piece of work, where its input does not depend on another function</li>
</ul>
<h2 id="Data-Parallel-Algorithm-Reduce"><a href="#Data-Parallel-Algorithm-Reduce" class="headerlink" title="Data Parallel Algorithm - Reduce"></a>Data Parallel Algorithm - Reduce</h2><p>Reduce: A function that takes in a list of objects and builds up a return value.</p>
<p>Important properties for parallel reduction:</p>
<ul>
<li>Associativity: a+(b+c) == (a+b)+c</li>
<li>Allows elements to be reduced in prarallel in a ‘tree’</li>
</ul>
<h2 id="Data-Parallel-Algorithms-Scan"><a href="#Data-Parallel-Algorithms-Scan" class="headerlink" title="Data Parallel Algorithms - Scan"></a>Data Parallel Algorithms - Scan</h2><p>Scan(prefix-sum): Takes a binary associative operator ⊕ with identity I, and an array of n elements [a0, a1, …, an-1] and returns the ordered set [I, a0, (a0⊕a1),…, (a0⊕a1⊕…⊕an-2)]</p>
<p>Example:</p>
<p>if ⊕ is addition, than scan on the set [3 1 7 0 4 1 6 3 ] returns the set [0 3 4 11 11 15 16 22]</p>
<p>Scan Algorithm in CUDA 4.0</p>
<h2 id="Data-Parallel-Algorithms-Compact"><a href="#Data-Parallel-Algorithms-Compact" class="headerlink" title="Data Parallel Algorithms - Compact"></a>Data Parallel Algorithms - Compact</h2><p>Compaction: Removing elements from an array - take in an array and produce an shorter array.</p>
<p>How do we perform removal in parallel?</p>
<ul>
<li>Map - create flags (1 keep, 0 remove)</li>
<li>Scan - compute index</li>
<li>Map - copy to new array</li>
</ul>
<p><img src="/images/14543391583932.jpg" alt=""></p>
<h2 id="Data-Parallel-Algorithms-FindUniq"><a href="#Data-Parallel-Algorithms-FindUniq" class="headerlink" title="Data Parallel Algorithms - FindUniq"></a>Data Parallel Algorithms - FindUniq</h2><p>FindUniq: Removing duplicates from an array - take in an set, produces an equal or smaller set of unique values</p>
<p><img src="/images/14543391502961.jpg" alt=""></p>
<p>在某些特殊情况可以利用 hash insertion 去掉 sort 的步骤, hash table 已经是有序的，就是打表的方法。</p>
<h2 id="Parallel-Software-Patterns"><a href="#Parallel-Software-Patterns" class="headerlink" title="Parallel Software Patterns"></a>Parallel Software Patterns</h2><p>A parallel software pattern is a generalizable solution to a class of recurring problems that occurs in the design of parallel software.</p>
<p>Attaches names to well-analyzed solutions that encapsulate the way an expert in the field solves problems.</p>
<p>Aims to achieve three goals:</p>
<ul>
<li>Define a set of vocabularies to communicate</li>
<li>Present a set of expert techniques for beginners to learn</li>
<li>Allows experts to more quickly design complex systems</li>
</ul>
<p><a href="http://parlab.eecs.berkely.edu/wiki/patterns/patterns" target="_blank" rel="external">Our Pattern Language</a></p>
<p>OPL: The Organization</p>
<p><img src="/images/14543391400653.jpg" alt=""></p>
<p>Structural Patterns:</p>
<ul>
<li>!Pipe-and-Filter</li>
<li>Agent-and-Repository</li>
<li>Event-based</li>
<li>Layered Systems</li>
<li>Model-view-constroller</li>
<li>Arbitrary Task Graphs</li>
<li>Puppeteer</li>
<li>Iterator/BSP</li>
<li>!MapReduce</li>
</ul>
<p>Monte Carlo Methods</p>
<h3 id="Applications-to-Your-Term-Projects"><a href="#Applications-to-Your-Term-Projects" class="headerlink" title="Applications to Your Term Projects"></a>Applications to Your Term Projects</h3><p><img src="/images/14543391286447.jpg" alt=""></p>
<h2 id="Distributed-Computing"><a href="#Distributed-Computing" class="headerlink" title="Distributed Computing"></a>Distributed Computing</h2><ul>
<li>Speedup not necessarily from better algorithm, but from scale</li>
<li>When an algorithms is converted to MapReduce it may operate significantly slower than sequential code on a single node</li>
<li>Mapping algorithms to a MapReduce framework is the challenge</li>
</ul>
<h2 id="Big-Data"><a href="#Big-Data" class="headerlink" title="Big Data"></a>Big Data</h2><ul>
<li>Web Data: Search, Advertisements, Behavioral data, Social graphs</li>
<li>Computational Physics Experiments<ul>
<li>Atomic Energy Research</li>
<li>Numerical Wind Tunnels</li>
</ul>
</li>
<li>The Earth Simulator<ul>
<li>Global Climate Change Research</li>
</ul>
</li>
<li>Weather Forecasting</li>
<li>The Human Genome Project, AIDS Research</li>
</ul>
<h2 id="Distributed-and-Cloud-Computing"><a href="#Distributed-and-Cloud-Computing" class="headerlink" title="Distributed and Cloud Computing"></a>Distributed and Cloud Computing</h2><ul>
<li>Distributed Computing<ul>
<li>Using distributed systems to solve computational problems.</li>
<li>Problem is divided in to many tasks, each of which is solved by one or more computers.</li>
</ul>
</li>
<li>Cloud Computing<ul>
<li>Distributed Computing on Cloud Resources</li>
</ul>
</li>
</ul>
<p>Distributed Computing on Cloud Computing Infrastructure = Scalable Computing</p>
<h2 id="Scalabel-Computing"><a href="#Scalabel-Computing" class="headerlink" title="Scalabel Computing"></a>Scalabel Computing</h2><ul>
<li>Embarrassingly parallel problems<ul>
<li>Shared Nothing Architecture</li>
</ul>
</li>
<li>Two dimensions of scalability<ul>
<li>Data: Given twice the amount of data, the same algorithm should take no more than twice as long to run</li>
<li>Resources: Given a cluster of twice the size, the same algorithm should take no more than half as long to run</li>
</ul>
</li>
</ul>
<h2 id="Big-Data-1"><a href="#Big-Data-1" class="headerlink" title="Big Data"></a>Big Data</h2><ul>
<li>Decompose the original problem in smaller, parallel tasks</li>
<li>Schedule tasks on workers distributed in a cluster<ul>
<li>Data locality</li>
<li>Resource availability</li>
</ul>
</li>
<li>Ensure Workers get the data they need</li>
<li>Coordinate synchronization among workers</li>
<li>Share partial results</li>
<li>Handle failures</li>
<li>Implementation details are complex</li>
<li>Shared memory approach(OpenMP)<ul>
<li>Developer needs to take case of almost everything</li>
<li>Synchronization, Concurrency</li>
<li>Resource Allocation</li>
</ul>
</li>
<li>MapReduce: a shared nothing approach<ul>
<li>Most of the above issures are taken care of</li>
<li>Problem decomposition and sharing partial results need particular attention</li>
<li>Optimization(memory and network consumption) are tricky</li>
</ul>
</li>
</ul>
<h2 id="Failures-in-Distributed-Computing"><a href="#Failures-in-Distributed-Computing" class="headerlink" title="Failures in Distributed Computing"></a>Failures in Distributed Computing</h2><ul>
<li>In large-scale distriuted computing, failure is ensured</li>
<li>Without fail-safe mechanisms distributed computing cannot work</li>
<li>HADOOP: MapReduce + HDFS(Hadoop Distributed Filesystem)<ul>
<li>Fail-safe Storage: By default stores 3 separate copies of each block</li>
<li>Fail-safe Task Management: Failed tasks re-scheduled up to 4 times</li>
</ul>
</li>
</ul>
<p>-&gt; Reliable and scalable computing</p>
<h2 id="HADOOP-gt-MapReduce"><a href="#HADOOP-gt-MapReduce" class="headerlink" title="HADOOP -&gt; MapReduce"></a>HADOOP -&gt; MapReduce</h2><ul>
<li>What is MapReduce?<ul>
<li>A programming model<ul>
<li>Inspired by function programming</li>
<li>Model to express distributed computations on massive amounts of data</li>
</ul>
</li>
<li>An execution framework<ul>
<li>Designed for large-scale data processing</li>
<li>Designed to run on clusters of commodity hardware</li>
</ul>
</li>
</ul>
</li>
<li>Separate the what from how<ul>
<li>Abstract away the “distributed” part of the system -&gt; handled by framework</li>
</ul>
</li>
<li>For optimal performance knowledge of framework is key<ul>
<li>Custom data reader/writer</li>
<li>Custom data partitioning</li>
<li>Memory utilization</li>
</ul>
</li>
</ul>
<h3 id="Map-amp-Reduce"><a href="#Map-amp-Reduce" class="headerlink" title="Map &amp; Reduce"></a>Map &amp; Reduce</h3><ul>
<li>Map: (map operation in functional programming)<ul>
<li>Transformation over a dataset</li>
<li>Apply a function f(x) to all elements in isolation</li>
<li>The application of f(x) to each element of a dataset can be parallelized in a straightforward manner</li>
</ul>
</li>
<li>Reduce: (fold operation in functional programming)<ul>
<li>Aggregation operation defined by a function g(x)</li>
<li>Data locality: elements in the list brought together</li>
<li>If we can group elements of the list, then reduce phase can proceed in parallel</li>
</ul>
</li>
<li>The framework coordinates the map and reduce phases<ul>
<li>How intermediate results are grouped for the reduce to happen in parallel</li>
</ul>
</li>
</ul>
<h3 id="Designing-a-MapReduce-algorithm"><a href="#Designing-a-MapReduce-algorithm" class="headerlink" title="Designing a MapReduce algorithm"></a>Designing a MapReduce algorithm</h3><ul>
<li>Key-value pairs are the basic data structures in MapReduce<ul>
<li>Keys and values can be: integers, strings, arbitrary data structures</li>
</ul>
</li>
<li>The design of a MapReduce algorithm involves:<ul>
<li>Define a key-value structures for application</li>
<li>Define mapper and reducer functions<ul>
<li>map: (k1,v1)-&gt;[(k2,v2)]</li>
<li>reduce: (k2,[v2]) -&gt; [(k3,v3)]</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="A-MapReduce-Job"><a href="#A-MapReduce-Job" class="headerlink" title="A MapReduce Job"></a>A MapReduce Job</h3><ul>
<li>Dataset: stored on the underlying distributed filesystem<ul>
<li>Split across files and across machines</li>
</ul>
</li>
<li>Mapper: The mapper is applied to every input key-value pair to generate intermediate key-value pairs</li>
<li>Reducer: The reducer is applied to all values associated with the same intermediate key to generate output key-value pairs</li>
<li>A distributed “group by” operation is implicitly performed between the map and reduce phases<ul>
<li>Intermediate data arrives at each reducer in order, sorted by the key</li>
<li>No ordering is guaranteed across reducers</li>
</ul>
</li>
<li>Output Keys from reducers are written to distributed filesystem</li>
<li>Intermediate keys are transient</li>
</ul>
<h3 id="Simplified-View-of-MapReduce"><a href="#Simplified-View-of-MapReduce" class="headerlink" title="Simplified View of MapReduce"></a>Simplified View of MapReduce</h3><ul>
<li>Mappers applied to all input key-value pairs -&gt; generate intermediate pairs</li>
<li>Reducers applied to all intermediate values associated with the same intermediate key</li>
<li>Between map and reduce lies a barrier that involves a large distributed sort and group by</li>
</ul>
<h3 id="Word-Count-in-MapReduce"><a href="#Word-Count-in-MapReduce" class="headerlink" title="Word Count in MapReduce"></a>Word Count in MapReduce</h3><ul>
<li>Define the appropriate key-value structures?<ul>
<li>Input (docid, doc)</li>
<li>Mapper (word, 1)</li>
<li>Output (word, C(word))</li>
</ul>
</li>
<li>Define Mapper and Reducer functions<ul>
<li>Mapper: tokenize the document, outputs key-value (word, 1)</li>
<li>The framework guarantees all values associated with the same key(word) are brought to the same reducer</li>
<li>Reducer: receives all values associated to some key (word)</li>
<li>Sums the values and writes output key-value pairs(word, C(word))</li>
</ul>
</li>
</ul>
<h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3><ul>
<li>A partitioned is in charge of assigning intermediate keys(words) to reducers<ul>
<li>Partitioner can be customized</li>
</ul>
</li>
<li>How many map and reduce tasks?<ul>
<li>The framework essentially takes care of map tasks</li>
<li>The designer/developer takes care of reduce tasks</li>
</ul>
</li>
</ul>
<h3 id="A-MapReduce-Job-on-Hadoop"><a href="#A-MapReduce-Job-on-Hadoop" class="headerlink" title="A MapReduce Job on Hadoop"></a>A MapReduce Job on Hadoop</h3><p>Master-slave architecture</p>
<ul>
<li>JobTrackerNode creating object for the job, determines number of mappers/reduces, schedules jobs, bookkeeping tasks’ status and progress</li>
<li>TaskTrackerNode: slaves manages individual tasks</li>
</ul>
<h2 id="HADOOP-gt-HDFS"><a href="#HADOOP-gt-HDFS" class="headerlink" title="HADOOP -&gt; HDFS"></a>HADOOP -&gt; HDFS</h2><ul>
<li>Improve computing throughput by co-locating data and computation</li>
<li>Abandon the separation between compute and storage nodes<ul>
<li>Not mandatory but highly desirable for MapReduce computing</li>
</ul>
</li>
<li>Distributed filesystems:<ul>
<li>Write once, read many workloads</li>
<li>Does not handle concurrency, but allows replication</li>
<li>Optimized for throughput not latency</li>
</ul>
</li>
<li>HDFS(Hadoop Distributed FileSystem)<ul>
<li>Tailored to the specific requirements of MapReduce</li>
</ul>
</li>
</ul>
<h3 id="HDFS-I-O"><a href="#HDFS-I-O" class="headerlink" title="HDFS I/O"></a>HDFS I/O</h3><ul>
<li>A typical read from a client involves:<ul>
<li>Contact the NameNode to determine where the actual data is stored</li>
<li>NameNode replies with block identifiers and locations(which DataNode)</li>
<li>Contact the DataNode to fetch data</li>
</ul>
</li>
<li>A typical write from a client invovles:<ul>
<li>Contact the NameNode to update the namespace and verify permissions</li>
<li>NameNode allocates a new block on a suitable DataNode</li>
<li>The client directly streams to the selected DataNode</li>
<li>HDFS files are immutable</li>
</ul>
</li>
<li>Data is never moved through the NameNode -&gt; no bottleneck</li>
</ul>
<h3 id="HDFS-Replication"><a href="#HDFS-Replication" class="headerlink" title="HDFS Replication"></a>HDFS Replication</h3><ul>
<li>By default, HDFS stores 3 separate copies of each block<ul>
<li>Ensures reliability, availability and performance</li>
</ul>
</li>
<li>Replication policy<ul>
<li>Spread replicas across different racks -&gt; Robust against cluster node and rack failures</li>
</ul>
</li>
<li>Block replication benefits MapReduce<ul>
<li>Scheduling decisions can take replicas into account</li>
<li>Exploit better data locality</li>
</ul>
</li>
<li>HDFS also transparently checksums all data during I/O</li>
</ul>
<h3 id="HDFS-Constraints"><a href="#HDFS-Constraints" class="headerlink" title="HDFS Constraints"></a>HDFS Constraints</h3><ul>
<li>Input splits for MapReduce based on individual files<ul>
<li>Mappers are launched for every file</li>
<li>High startup correctness</li>
<li>Inefficient “shuffle and sort”</li>
</ul>
</li>
</ul>
<p>Small number of large files preferred over a large number of small files</p>
<h2 id="Cloud-Computing-Advantages"><a href="#Cloud-Computing-Advantages" class="headerlink" title="Cloud Computing - Advantages"></a>Cloud Computing - Advantages</h2><ul>
<li>Illusion of infinite computing resources on demand</li>
<li>Elimination of an up-front commitment by user</li>
<li>Ability to pay for use of computing resources on a short-term basis as needed</li>
<li>Lowering entry barrier for large scale computing<ul>
<li>Removing equipment fixed cost</li>
</ul>
</li>
<li>Making available economy-of-scale<ul>
<li>Reducing operating variable cost</li>
</ul>
</li>
</ul>
<h2 id="Developing-Algorithms-in-Hadoop"><a href="#Developing-Algorithms-in-Hadoop" class="headerlink" title="Developing Algorithms in Hadoop"></a>Developing Algorithms in Hadoop</h2><ul>
<li>Algorithm development involves:<ul>
<li>preparing the input data</li>
<li>Implement the mapper and the reducer</li>
<li>Optionally, design the combiner and the partitioner</li>
</ul>
</li>
<li>How to recast existing algorithms in MapReduce?<ul>
<li>It is not always obvious how to express algorithms</li>
<li>Data structures play an important role</li>
<li>Optimization is hard</li>
</ul>
</li>
<li>Developer needs to understand the framework</li>
<li>Learn by examples<ul>
<li>Design Patterns</li>
<li>Synchronization is most trick aspect</li>
</ul>
</li>
</ul>
<h2 id="Efficiency-Bottlenecks-amp-Precautions"><a href="#Efficiency-Bottlenecks-amp-Precautions" class="headerlink" title="Efficiency, Bottlenecks &amp; Precautions"></a>Efficiency, Bottlenecks &amp; Precautions</h2><ul>
<li>Efficiency<ul>
<li>Reduces I/O bandwidth(number of intermediate key-value pairs)</li>
<li>Un-necessary object creation and destruction(garbage collection)</li>
</ul>
</li>
<li>Bottlenecks<ul>
<li>In-mapper combining depends on having sufficient memory</li>
<li>Multiple threads compete for same resources</li>
</ul>
</li>
<li>Precautions<ul>
<li>Breaks functional programming paradigm due to state preservation</li>
<li>Preserving state -&gt; algorithm behavior might depend on execution order</li>
</ul>
</li>
</ul>
<h2 id="How-do-you-interpret-speedup-results"><a href="#How-do-you-interpret-speedup-results" class="headerlink" title="How do you interpret speedup results?"></a>How do you interpret speedup results?</h2><p>Based on the PALLAS paper from UC Berkeley</p>
<ul>
<li>(SW) Application Developers<ul>
<li>Provide end-user with new capabilites within cost constraints</li>
<li>Concerned about a specific subset of applications at a time</li>
<li>Pragmatic towards processor platform choices</li>
<li>Gains no value from documenting multiple implementation platforms</li>
<li><strong>Platform as a black box</strong></li>
</ul>
</li>
<li>(HW) Architecture Researchers<ul>
<li>Develop new micro-architectures features for next-gen processors</li>
<li>Understand the pros and cons of architectural features of a <strong>broad range</strong> of platform for a <strong>broad range</strong> of applications</li>
<li>Use <strong>toy problems</strong> to exercise all features</li>
<li><strong>Application as a black box</strong></li>
</ul>
</li>
</ul>
<h2 id="Computational-Finance"><a href="#Computational-Finance" class="headerlink" title="Computational Finance"></a>Computational Finance</h2><p>Use Value-at-Risk(VaR) estimates - based on Monte Carlo methods Pattern.</p>
<ul>
<li><p>VaR</p>
<ul>
<li>Maximum expected loss that will not be exceeded</li>
<li>under normal market considerations</li>
<li>over a predetermined period</li>
<li>at a given confidence level</li>
</ul>
</li>
<li><p>Different Optimization Across Platforms</p>
<ul>
<li>Oganization &amp; Structure: Reduce Computation</li>
<li>Algorithm Strategies: Fast Convergence</li>
<li>Implementation Strategies: Saving Memory BW / Kernel Merge Vectorization</li>
</ul>
</li>
</ul>
<h2 id="Speedup"><a href="#Speedup" class="headerlink" title="Speedup"></a>Speedup</h2><p>Before: Performance x  – After: Performance y – ROI(speedup): y/x</p>
<h2 id="Term-Project-Report"><a href="#Term-Project-Report" class="headerlink" title="Term Project Report"></a>Term Project Report</h2><ul>
<li>Clearly describe what is the <strong>baseline</strong> you are comparing to, in terms of:<ul>
<li>Platform used</li>
<li>Software architecture</li>
<li>Algorithm strategies</li>
<li>Implementation strategies</li>
</ul>
</li>
<li>Present your speed ups, which is often <strong>NOT</strong> only the result of differences in the processor or the platform, but also include:<ul>
<li>Differences in application architecture</li>
<li>Differences in algorithm strategy</li>
<li>Differences in implementation strategy techniques</li>
<li>Differences in the fine-tuning of parameters</li>
</ul>
</li>
</ul>
<hr>
<h1 id="Question-Set"><a href="#Question-Set" class="headerlink" title="Question Set"></a>Question Set</h1><h2 id="Module-1-2"><a href="#Module-1-2" class="headerlink" title="Module 1.2"></a>Module 1.2</h2><ul>
<li>What are the differences between multicore and manycore processors?<ul>
<li>Multicore: yoke of oxen. Each core optimized for executing a single thread.</li>
<li>Manycore: flock of chickens. Cores optimized for aggregate throughput, deemphasizing individual performance.</li>
</ul>
</li>
<li>What is instruction level parallelism? What is SIMD?<ul>
<li>ILP: Instructions in a sequence that can be computed at the same time.</li>
<li>ILP(wiki): a measure of how many of the operations in a computer program can be performed simultaneously</li>
<li>SIMD(wiki): computers with multiple processing elements that perform the same operation on multiple data points simultaneously. data level parallelism. </li>
</ul>
</li>
<li>What is simultaneous multithreading?<ul>
<li>a technique for improving the overall efficiency of superscalar CPUs with hardware multithreading. SMT permits multiple independent threads of execution to better utilize the resources provided by modern processor architectures.</li>
</ul>
</li>
<li>What are the three metrics for a memory hierarchy?<ul>
<li>Capacity: Size, e.g. number of bytes of data</li>
<li>Latency: From start to finish, in units of time, e.g. CPU clock cycles</li>
<li>Throughput: Tasks accomplished per unit time, e.g. GB/s</li>
</ul>
</li>
<li>What are the different system granularity?<ul>
<li>Remote Procedure Call based Implementations</li>
<li>MPI-based Implementations</li>
<li>Pthread-based Implementations</li>
<li>Multicore Task Queue-based Implementations</li>
<li>Manycore Throughput Optimized Implementations</li>
</ul>
</li>
</ul>
<h2 id="Module-1-3"><a href="#Module-1-3" class="headerlink" title="Module 1.3"></a>Module 1.3</h2><ul>
<li>What is the different between concurrency and parallelism?<ul>
<li>Concurrency: We expose concurrency in our application</li>
<li>Parallelism: We exploit parallelism in our platform</li>
</ul>
</li>
<li>What are the four key elements of the human problem solving process?<ul>
<li>Understand the current state</li>
<li>Observe the internal representation</li>
<li>Search among alternatives</li>
<li>Select from a set of choices</li>
</ul>
</li>
<li>What are the characteristics of a current algorithm implementation?<ul>
<li>Efficiency</li>
<li>Simplicity</li>
<li>Portablility</li>
<li>Scalability</li>
</ul>
</li>
<li>What levels of concurrency can be exposed in the kmeans algorithm?<ul>
<li>Expectation: N(independent) k(min reduction) D(sum reduction)</li>
<li>Maximization: D(independent) N(Histogram computation into k bins)</li>
</ul>
</li>
<li>What levels of parallelism are available to be exploited?<ul>
<li>Core level Parallelism</li>
<li>SIMD level parallelism</li>
</ul>
</li>
<li>What mapping between concurrency and parallelism can be explored?<ul>
<li>One level of concurrency could map to multiple levels of parallelism</li>
<li>SIMD &amp; core-level parallelism across data-points<ul>
<li>Update membership for each data point sequentially</li>
<li>Histogram computation</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Module-2-1"><a href="#Module-2-1" class="headerlink" title="Module 2.1"></a>Module 2.1</h2><ul>
<li>What are the exploitable levels of parallelism in a multicore processor?<ul>
<li>SIMD-Level: using vectorizing compiler and hand-code intrinsics</li>
<li>SMT-Level: OS abstract it to core-level parallelism</li>
<li>Core-Level: Using threads to describe work done on different cores</li>
</ul>
</li>
<li>What is SPMD? And how to use OpenMP to do SPMD?<ul>
<li>OpenMP - Pthread-based Implementations(granularity)</li>
<li>SPMD(wiki): SPMD (single program, multiple data) is a technique employed to achieve parallelism; it is a subcategory of MIMD. Tasks are split up and run simultaneously on multiple processors with different input in order to obtain results faster.</li>
<li>OpenMP managed <strong>Fork-Join</strong> Parallelism to do SPMD</li>
</ul>
</li>
<li>What is the difference between critical and atomic?<ul>
<li>critical: 并行程序块，同时只能有一个线程能访问该并行程序块</li>
<li>atomic: 只适用于两种情况：自加减操作以及基本的操作符</li>
<li>critical 与 atomic 的区别在于，atomic 仅适用于两种基本类型操作，而且 atomic 所防护的仅为一句代码。critical 可以对某个并行程序块进行防护。</li>
</ul>
</li>
<li>How to reduce synchronization cost and avoid false sharing?<ul>
<li>Be aware of the cache line sizes for a platform</li>
<li>Avoid accessing the same cache line from different threads</li>
</ul>
</li>
<li>What are the scheduling, reduction, data sharing, and synchronization options for OpenMP?<ul>
<li>scheduling<ul>
<li>static</li>
<li>dynamic</li>
<li>guided</li>
</ul>
</li>
<li>data sharing<ul>
<li>shared</li>
<li>private</li>
<li>firstprivate</li>
<li>lastprivate</li>
</ul>
</li>
<li>synchronization<ul>
<li>ordered</li>
<li>barrier</li>
<li>single</li>
<li>nowait</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Module-2-2"><a href="#Module-2-2" class="headerlink" title="!! Module 2.2"></a>!! Module 2.2</h2><ul>
<li>Why naive matrix-multiply does not achieve peak performance on the CPU?<ul>
<li>Deep memory hierarchy</li>
<li>Pipeline, ILP</li>
<li>Free operations are not free</li>
</ul>
</li>
<li>What are the different data layouts for matrices?<ul>
<li>Column major</li>
<li>Row major</li>
</ul>
</li>
<li>What is cache blocking? Why do we need it?</li>
<li>Is blocking sufficient? What more can we do?<ul>
<li>Strength reduction</li>
<li>Function inlining</li>
<li>Loop unrolling</li>
<li>Common subexpression elimination</li>
<li>Load/Store elimination</li>
<li>Table lookups</li>
<li>Branch elimination</li>
</ul>
</li>
</ul>
<h2 id="Module-2-3"><a href="#Module-2-3" class="headerlink" title="Module 2.3"></a>Module 2.3</h2><ul>
<li>What is the roofline model? What are the metrics and axis used?<ul>
<li>Roofline model: a pedagogical tool for program analysis and optimization</li>
<li>Metric of interest: DRAM bandwidth(GB/s)</li>
<li>y-axis: FLOPs; x-axis: AI</li>
</ul>
</li>
<li>What is the difference between “flop’s per memory instruction” from “flop’s per DRAM byte”?<ul>
<li>?</li>
</ul>
</li>
<li>Consider an image <code>Image[height][width]</code>. If one were to stride through the columns of values, what would be the effects? How would they be mapped to the roofline?<ul>
<li>?</li>
</ul>
</li>
<li>How does one model incomplete SIMDization (e.g. half the flop’s can be SIMDized). insufficient ILP(some dependent flop’s), or an imbalance between FDMUL’s and FPADD’s on the roofline?<ul>
<li>See the complete graph below</li>
</ul>
</li>
<li>How would one model {branch mispredicts, TLB misses, or too many streams for the prefetchers} on the roofline.<ul>
<li>See the complete graph below</li>
</ul>
</li>
</ul>
<p><img src="/images/14543392317490.jpg" alt=""></p>
<h2 id="Module-3-1"><a href="#Module-3-1" class="headerlink" title="Module 3.1"></a>Module 3.1</h2><ul>
<li>What’s the Difference between Multicore and Manycore?<ul>
<li>Multicore: yoke of oxen. Each core optimized for executing a single thread.</li>
<li>Manycore: flock of chickens. Cores optimized for aggregate throughput, deemphasizing individual performance.</li>
</ul>
</li>
<li>When does using a GPU make sense?<ul>
<li>Application with a lot of concurrency (1000-way, fine-grained concurrency)</li>
<li>Some memory intensive applications (Aggregate memory bandwidth is higher)</li>
<li>Advantage diminishes when task granularity becomes too large to fit in shared memory</li>
</ul>
</li>
<li>What is the memory hierarchy inversion? And why is it there?<ul>
<li>Memory hierarchy inversion: more registers than shared memory</li>
<li>Single thread won’t see inverse hierarchy</li>
<li>Inversion comes from parallelism</li>
<li>Registers scale with SIMD and multithreading (Shared memory/L1 cache don’t have to)</li>
</ul>
</li>
<li>What is the memory wall? How to get around it?<ul>
<li>Memory wall: Increasing gap between Processor and DRAM performance</li>
<li>Many core Processors utilize application concurrency to hide memory latency (aka get around the memory wall)</li>
</ul>
</li>
<li>Why warps?<ul>
<li>Software abstract info hid an extra level of architecture complexity</li>
<li>A 128KB register file is a large memory (takes more than one cycle)</li>
<li>Hardware provide 160wide physical SIMD units, half-pump register files</li>
<li>To simplify the programming model</li>
</ul>
</li>
<li>How do we deal with GPUs of different sizes?<ul>
<li>CUDA provides an abstract infor concurrency to be fully exposed</li>
<li>HW/Runtime provides capability to schedule the computation</li>
</ul>
</li>
<li>What are the implications of the thread block abstraction?<ul>
<li>Computation is grouped into blocks of independent concurrently execrable work</li>
<li>Fully exposed the concurrency in the application</li>
<li>The HW/Runtime makes the decision to selectively sequentialize the execution as necessary</li>
</ul>
</li>
<li>How do threads communicate with each other?<ul>
<li>Shared Memory</li>
<li>Manycore processors provide memory local to each core</li>
<li>Computations in SIMD-lanes in the same core can communicate via memory read / write</li>
</ul>
</li>
<li>What is the caveat in synchronizing threads in a thread block?<ul>
<li><code>__syncthreads()</code> 必须在每个线程中都能执行到，而不能有的有有的没有</li>
</ul>
</li>
</ul>
<h2 id="Module-3-2"><a href="#Module-3-2" class="headerlink" title="Module 3.2"></a>Module 3.2</h2><ul>
<li>What are the three ways to improve execution throughput?<ul>
<li>Maximizing Memory Throughput<ul>
<li>SoA vs AoS</li>
<li>Memory coalescing</li>
<li>Use of shared memory</li>
<li>Memory bank conflict</li>
<li>Padding</li>
</ul>
</li>
<li>Maximizing Instruction Throughput<ul>
<li>Branch divergence</li>
<li>Optimize instruction mix</li>
</ul>
</li>
<li>Maximizing Scheduling Throughput</li>
</ul>
</li>
<li>When to use SOA vs AOS?<ul>
<li>Unfortunately, the SoA form is not ideal in all circumstances. For random or incoherent circumstances, gathers are used to access the data and the SoA form can result in extra unneeded data being read into cache, thus reducing performance. In this case, use of the AoS form instead will result in a smaller working set and improved performance. Generally, though, if the computation is to be vectorized, the SoA form is preferred.</li>
</ul>
</li>
<li>What is memory coalescing? When to use it? Why is it important?<ul>
<li>Memory coalescing: combine multiple memory accesses generated from multiple threads into a single physical transaction</li>
<li>Hardware Constraint: DRAM is accessed in ‘segments’ of 32B/64B/128B</li>
</ul>
</li>
<li>What is shared memory? How to use it?<ul>
<li>Manycore processors provide memory local to each core</li>
<li><code>__share__</code></li>
</ul>
</li>
<li>What is memory bank conflict? How to work around it?<ul>
<li>A bank conflict occurs if two or more threads access any bytes within different 32-bit words belonging to the same bank.</li>
<li>If each thread in a halfwarp accesses successive 32bit values there are no bank conflicts.</li>
</ul>
</li>
<li>What is branch divergence?<ul>
<li>threads of a warp diverge via a data-dependent conditional branch</li>
</ul>
</li>
<li>How to optimize for instruction mix?<ul>
<li>Compiler Assisted Loop Unrolling</li>
<li>#pragma unroll</li>
</ul>
</li>
<li>What is occupancy? How to model/measure it?<ul>
<li>Occupancy: Ability of a CUDA kernel to occupy concurrent contexts in a SM</li>
<li>CUDA Occupancy Calculator</li>
<li><code>--ptxas-options=-v</code></li>
</ul>
</li>
<li>How to use the code profiler with CUDA?<ul>
<li>CUDA Profiler Tutorial by Erik Reed</li>
</ul>
</li>
</ul>
<h2 id="Module-3-3"><a href="#Module-3-3" class="headerlink" title="Module 3.3"></a>Module 3.3</h2><ul>
<li>What are the important properties of a Map function?<ul>
<li>Side-effect free: Only returning a value, no modifications of state with the rest of the application</li>
<li>Independent: Has an independent piece of work, where its input does not depend on another function</li>
</ul>
</li>
<li>What are the important properties of a Reduce function?<ul>
<li>Associativity: a+(b+c) == (a+b)+c</li>
<li>Allows elements to be reduced in prarallel in a ‘tree’</li>
</ul>
</li>
<li>What are the important properties of s Scan function?<ul>
<li>return a ordered set</li>
</ul>
</li>
<li>How to compact an array in a data-parallel way?<ul>
<li>Map - create flags (1 keep, 0 remove)</li>
<li>Scan - compute index</li>
<li>Map - copy to new array</li>
</ul>
</li>
<li>How to find unique elements in an array in a data-parallel way?<ul>
<li>Sort</li>
<li>Map - create flags (1 keep, 0 remove)</li>
<li>Scan - compute index</li>
<li>Map - copy to new array</li>
</ul>
</li>
</ul>
<h2 id="Module-3-4"><a href="#Module-3-4" class="headerlink" title="Module 3.4"></a>Module 3.4</h2><ul>
<li>What are parallel software patterns?</li>
<li>What are the three goals the software patterns aim to achieve?</li>
<li>What is a software architecture?</li>
<li>How is it important for writing fast code?</li>
<li>What the the five categories of patterns in OPL?</li>
<li>What are the nine sections in an OPL pattern?</li>
<li>What are the areas of consideration for your Term Project?</li>
</ul>
<h2 id="Module-4-1"><a href="#Module-4-1" class="headerlink" title="Module 4.1"></a>Module 4.1</h2><ul>
<li>Why Distributed Computing?</li>
<li>How common are failures in Large Scale Distributed Computing?</li>
<li>How are failures handled in HADOOP?</li>
<li>What is MapReduce?</li>
<li>When developing a MapReduce application what components and functions need to be defined?</li>
<li>How are data bottlenecks reduced in HDFS?</li>
<li>What are the advantages of Cloud Computing?</li>
</ul>

    
  </div>


          </div>
          

  <p>热评文章</p>
  <div class="ds-top-threads" data-range="weekly" data-num-items="4"></div>


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="vault/fastcode-0.html"
           data-title="How to Write Fast Code 第 0 课 往年笔记与问题集" data-url="http://wdxtub.com/vault/fastcode-0.html">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/misc/avatar.jpg"
               alt="wdxtub" />
          <p class="site-author-name" itemprop="name">wdxtub</p>
          <p class="site-description motion-element" itemprop="description">人文/科学/读书/写作/思考/编程/架构/数据/广交朋友/@SYSU/@CMU</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">710</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">分类</span>
              
            </div>
          

          
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">874</span>
                <span class="site-state-item-name">标签</span>
              
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/wdxtub" target="_blank" title="GitHub">
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/wdxtub" target="_blank" title="微博">
                  
                  微博
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://douban.com/people/wdx" target="_blank" title="豆瓣">
                  
                  豆瓣
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/wdxtub" target="_blank" title="知乎">
                  
                  知乎
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              不妨看看
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://zhchbin.github.io/" title="zhchbin" target="_blank">zhchbin</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.algorithmdog.com/" title="算法狗" target="_blank">算法狗</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.52cs.org/" title="我爱计算机" target="_blank">我爱计算机</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://jackqdyulei.github.io/" title="雷雷" target="_blank">雷雷</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://guojiex.github.io/" title="瓜瓜" target="_blank">瓜瓜</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://wdxtub.lofter.com/" title="我的 Lofter" target="_blank">我的 Lofter</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://wdxtub.com/interview/" title="刷题笔记" target="_blank">刷题笔记</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2013 - 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wdxtub</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

<div class="busuanzi-count">

  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv"><i class="fa fa-user"></i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span></span>
  

  
    <span class="site-pv"><i class="fa fa-eye"></i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span>
  
  
</div>



        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"wdxblog"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementById('footer')
      || document.getElementById('footer')).appendChild(ds);
    })();
  </script>

  
    
      
      <script src="/vendors/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
      <script src="/js/src/hook-duoshuo.js"></script>
    
  





  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
       search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();

    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
    'use strict';
    $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
            // get the contents from search data
            isfetched = true;
            $('.popup').detach().appendTo('.header-inner');
            var datas = $( "entry", xmlResponse ).map(function() {
                return {
                    title: $( "title", this ).text(),
                    content: $("content",this).text(),
                    url: $( "url" , this).text()
                };
            }).get();
            var $input = document.getElementById(search_id);
            var $resultContent = document.getElementById(content_id);
            $input.addEventListener('input', function(){
                var matchcounts = 0;
                var str='<ul class=\"search-result-list\">';                
                var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                $resultContent.innerHTML = "";
                if (this.value.trim().length > 1) {
                // perform local searching
                datas.forEach(function(data) {
                    var isMatch = true;
                    var content_index = [];
                    var data_title = data.title.trim().toLowerCase();
                    var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                    var data_url = data.url;
                    var index_title = -1;
                    var index_content = -1;
                    var first_occur = -1;
                    // only match artiles with not empty titles and contents
                    if(data_title != '' && data_content != '') {
                        keywords.forEach(function(keyword, i) {
                            index_title = data_title.indexOf(keyword);
                            index_content = data_content.indexOf(keyword);
                            if( index_title < 0 && index_content < 0 ){
                                isMatch = false;
                            } else {
                                if (index_content < 0) {
                                    index_content = 0;
                                }
                                if (i == 0) {
                                    first_occur = index_content;
                                }
                            }
                        });
                    }
                    // show search results
                    if (isMatch) {
                        matchcounts += 1;
                        str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                        var content = data.content.trim().replace(/<[^>]+>/g,"");
                        if (first_occur >= 0) {
                            // cut out 100 characters
                            var start = first_occur - 20;
                            var end = first_occur + 80;
                            if(start < 0){
                                start = 0;
                            }
                            if(start == 0){
                                end = 50;
                            }
                            if(end > content.length){
                                end = content.length;
                            }
                            var match_content = content.substring(start, end);
                            // highlight all keywords
                            keywords.forEach(function(keyword){
                                var regS = new RegExp(keyword, "gi");
                                match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                            });
                            
                            str += "<p class=\"search-result\">" + match_content +"...</p>"
                        }
                        str += "</li>";
                    }
                })};
                str += "</ul>";
                if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
                if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
                $resultContent.innerHTML = str;
            });
            proceedsearch();
        }
    });}

    // handle and trigger popup window;
    $('.popup-trigger').mousedown(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };

    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src=""></script>


  

  
<script type="text/javascript" async src="//push.zhanzhang.baidu.com/push.js">
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


</body>
</html>
